
<!-- saved from url=(0051)https://computing.llnl.gov/tutorials/parallel_comp/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Introduction to Parallel Computing</title>

<script language="JavaScript" src="./Introduction to Parallel Computing_files/tutorials.js.Без названия"></script>
<link rel="StyleSheet" href="./Introduction to Parallel Computing_files/tutorials.css" type="text/css">
<link rel="SHORTCUT ICON" href="http://www.llnl.gov/favicon.ico">

<!-- BEGIN META TAGS -->
<meta name="LLNLRandR" content="UCRL-MI-133316">
<meta name="distribution" content="global">
<meta name="description" content="Livermore Computing Training">
<meta name="rating" content="general">
<meta http-equiv="keywords" content="Lawrence Livermore
National Laboratory, LLNL, High Performance Computing, parallel, programming, 
HPC, training, workshops, tutorials, Blaise Barney">
<meta name="copyright" content="This document is copyrighted U.S.
Department of Energy">
<meta name="Author" content="Blaise Barney">
<meta name="email" content="blaiseb@llnl.gov">
<!-- END META TAGS -->
</head>

<body>
<basefont size="3">            <!-- default font size -->
<font face="arial">

<!-- Begin Piwik Tracking Code  -->
<script src="./Introduction to Parallel Computing_files/piwik.js.Без названия" type="text/javascript">
</script>
<script>
var siteName = document.domain;
var pkBaseURL = 'https://analytics.llnl.gov/';
if (typeof jQuery=="undefined") {
    document.write(unescape("%3Cscript src='" + pkBaseURL + "jquery.js' type='text/javascript'%3E%3C/script%3E"));
}
</script><script src="./Introduction to Parallel Computing_files/jquery.js.Без названия" type="text/javascript"></script>
<script>
    try {
        var LLNLTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 1);
        LLNLTracker.trackPageView();
        LLNLTracker.enableLinkTracking();
        var localSiteTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 149);
        localSiteTracker.trackPageView();
        localSiteTracker.enableLinkTracking();
    }
    catch (err) {
        console.log(err);
    }
</script><noscript><p><img src="https://analytics.llnl.gov/piwik.php?idsite=149" style="border:0" alt="" /></p></noscript>
<!-- End Piwik Tracking Code -->

<a name="top">  </a>
<table cellpadding="0" cellspacing="0" width="100%">
<tbody><tr><td colspan="2" bgcolor="#3F5098">
  <table cellpadding="0" cellspacing="0" width="900">
  <tbody><tr><td background="./Introduction to Parallel Computing_files/bg1.gif">
  <a name="top"> </a>
  <script language="JavaScript">addNavigation()</script>   <table border="0"><tbody><tr align="center" valign="center">    <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/training#training_materials">Tutorials</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/exercises/index.html">Exercises</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/abstracts/index.html">Abstracts</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/training#workshops">LC&nbsp;Workshops</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/misc/comments.html">Comments</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/search/index.html">Search</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">   <a href="http://www.llnl.gov/disclaimer.html" target="W2">   Privacy &amp; Legal Notice</a></font></td>   </tr></tbody></table>   
  <p><br>
  </p><h1>Introduction to Parallel Computing</h1>
  <p>
  </p></td></tr></tbody></table>
</td>
</tr><tr valign="top">
<td><i>Author: Blaise Barney, Lawrence Livermore National Laboratory</i></td>
<td align="right"><font size="-1">UCRL-MI-133316</font></td>
</tr></tbody></table>
<p>

<a name="TOC"> </a>
</p><h2>Table of Contents</h2>
<ol>
<li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Abstract">Abstract</a>
</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Overview">Overview</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Whatis">What is Parallel Computing?</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#WhyUse">Why Use Parallel Computing?</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Who">Who is Using Parallel Computing?</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Concepts">Concepts and Terminology</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Neumann">von Neumann Computer Architecture</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Flynn">Flynn's Classical Taxonomy</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Terminology">Some General Parallel Terminology</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#LimitsCosts">Limits and Costs of Parallel Programming</a> 
</li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#MemoryArch">Parallel Computer Memory Architectures</a> 
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#SharedMemory">Shared Memory</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DistributedMemory">Distributed Memory</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#HybridMemory">Hybrid Distributed-Shared Memory</a>
    </li></ol>
 
</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Models">Parallel Programming Models</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsOverview">Overview</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsShared">Shared Memory Model</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsThreads">Threads Model</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsMessage">Distributed Memory / Message Passing Model</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsData">Data Parallel Model</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Hybrid">Hybrid Model</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#SPMD-MPMD">SPMD and MPMP</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Designing">Designing Parallel Programs</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignAutomatic">Automatic vs. Manual Parallelization</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignUnderstand">Understand the Problem and the Program</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignPartitioning">Partitioning</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignCommunications">Communications</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignSynchronization">Synchronization</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignDependencies">Data Dependencies</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignLoadBalance">Load Balancing</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignGranularity">Granularity</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignIO">I/O</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignDebug">Debugging</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignPerformance">Performance Analysis and Tuning</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Examples">Parallel Examples</a> 
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesArray">Array Processing</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI">PI Calculation</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesHeat">Simple Heat Equation</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesWave">1-D Wave Equation</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#References">References and More Information</a>
</li></ol>
 
<!--========================================================================-->
 
<a name="Abstract"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Abstract</span></td>
</tr></tbody></table>
<p><br>
 
This is the first tutorial in the "Livermore Computing Getting Started" workshop. It is intended to provide only a very quick overview of the extensive and broad topic of Parallel Computing, as a lead-in for the tutorials that follow it.  As such, it covers just the very basics of parallel computing, and is intended for someone who is just becoming acquainted with the subject and who is planning to attend one or more of the other tutorials in this workshop. It is not intended to cover Parallel Programming in depth, as this would require significantly more time. The tutorial begins with a discussion on parallel computing - what it is and how it's used, followed by a discussion on concepts and terminology associated with parallel computing. The topics of parallel memory architectures and programming models are then explored. These topics are followed by a series of practical discussions on a number of the complex issues related to designing and running parallel programs. The tutorial concludes with several examples of how to parallelize simple serial programs. 
<br><br>

<!--========================================================================-->

<a name="Overview"> <br><br> </a>
<a name="Whatis"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Overview</span></td>
</tr></tbody></table>
</p><h2>What is Parallel Computing?</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Serial Computing:</span>
<ul>
<p>
</p><li>Traditionally, software has been written for <b><i>serial</i></b>  
    computation:
    <ul>
    <li>A problem is broken into a discrete series of instructions
    </li><li>Instructions are executed sequentially one after another
    </li><li>Executed on a single processor
    </li><li>Only one instruction may execute at any moment in time
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/serialProblem.gif" width="604" height="250" border="1" alt="Serial computing">
</p><p>
<b>For example:</b>
</p><p>
<img src="./Introduction to Parallel Computing_files/serialProblem2.gif" width="604" height="250" border="1" alt="Serial computing">
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Parallel Computing:</span>
</p><ul>
<li>In the simplest sense, <b><i>parallel computing</i></b> is the simultaneous 
    use of multiple compute resources to solve a computational problem:
    <ul>
    <li>A problem is broken into discrete parts that can be solved concurrently
    </li><li>Each part is further broken down to a series of instructions
    </li><li>Instructions from each part execute simultaneously on different processors
    </li><li>An overall control/coordination mechanism is employed 
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/parallelProblem.gif" width="683" height="372" border="1" alt="Parallel computing">
</p><p>
<b>For example:</b>
</p><p>
<img src="./Introduction to Parallel Computing_files/parallelProblem2.gif" width="683" height="372" border="1" alt="Parallel computing">
</p><p>
</p></li><li>The computational problem should be able to: 
    <ul>
    <li>Be broken apart into discrete pieces of work that can be solved 
        simultaneously;
    </li><li>Execute multiple program instructions at any moment in time;
    </li><li>Be solved in less time with multiple compute resources than with a single 
        compute resource.
    </li></ul>
<p>
</p></li><li>The compute resources are typically:
    <ul>
    <li>A single computer with multiple processors/cores
    </li><li>An arbitrary number of such computers connected by a network
    </li></ul>
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Parallel Computers:</span>
</p><ul>
<p>
</p><li>Virtually all stand-alone computers today are parallel from a hardware
    perspective:
    <ul>
    <li>Multiple functional units (L1 cache, L2 cache, branch, prefetch, decode,
        floating-point, graphics processing (GPU), integer, etc.)
    </li><li>Multiple execution units/cores
    </li><li>Multiple hardware threads
    </li></ul>
<p>
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td align="center">
<img src="./Introduction to Parallel Computing_files/bgqComputeChip.jpg" width="450" heigth="453">
<br>IBM BG/Q Compute Chip with 18 cores (PU) and 16 L2 Cache units (L2) </td>
</tr></tbody></table>
</p><p>
</p></li><li>Networks connect multiple stand-alone computers (nodes) to make larger
    parallel computer clusters.
<p>
<img src="./Introduction to Parallel Computing_files/nodesNetwork.gif" width="720" heigth="249">
</p><p>
</p></li><li>For example, the schematic below shows a typical LLNL parallel computer cluster: 
    <ul>
    <li>Each compute node is a multi-processor parallel computer in itself
    </li><li>Multiple compute nodes are networked together with an Infiniband network
    </li><li>Special purpose nodes, also multi-processor, are used for other purposes
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/parallelComputer1.gif" width="781" heigth="402">
</p><p>
</p></li><li>The majority of the world's large parallel computers (supercomputers)
    are clusters of hardware produced by a handful of (mostly) well known vendors.
<p>
<img src="./Introduction to Parallel Computing_files/top500Vendors.png">
<br><font size="-1"><i>Source: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font>
</p></li></ul>

<!--========================================================================-->

<a name="WhyUse"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Overview</span></td>
</tr></tbody></table>
<h2>Why Use Parallel Computing?</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">The Real World is Massively Parallel:</span>
<p>
<table border="0" cellspacing="0" cellpadding="0" width="800">
<tbody><tr valign="top">
<td>
<ul>
<li>In the natural world, many complex, interrelated events are happening at the
    same time, yet within a temporal sequence.
<p>
</p></li><li>Compared to serial computing, parallel computing is much better suited for
    modeling, simulating and understanding complex, real world phenomena.
<p>
</p></li><li>For example, imagine modeling these serially:
<p>
<img src="./Introduction to Parallel Computing_files/realWorldCollage1.jpg" width="760" height="220">
<br><img src="./Introduction to Parallel Computing_files/realWorldCollage2.jpg" width="760" height="230">
<br><img src="./Introduction to Parallel Computing_files/realWorldCollage3.jpg" width="760" height="208">
</p></li></ul>
</td>
</tr></tbody></table>


<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Main Reasons:</span>

</p><p>
<table border="0" cellpadding="0" cellspacing="0" width="700">
<tbody><tr valign="top">
<td><ul>
<li><b>SAVE TIME AND/OR MONEY:</b> 
    <ul>
    <li>In theory, throwing more resources at a task will shorten its time to 
        completion, with potential cost savings. 
    </li><li>Parallel computers can be built from cheap, commodity components.
    <p>
    <img src="./Introduction to Parallel Computing_files/timeMoney2.jpg" width="600" height="182">
    </p></li></ul>
<p>
</p></li><li><b>SOLVE LARGER / MORE COMPLEX PROBLEMS:</b> 
    <ul>
    <li>Many problems are so large and/or complex that it is impractical or
        impossible to solve them on a single computer, especially given limited
        computer memory.
    </li><li>Example: "Grand Challenge Problems" 
        (<a href="http://en.wikipedia.org/wiki/Grand_Challenge" target="_blank">en.wikipedia.org/wiki/Grand_Challenge</a>) requiring
        PetaFLOPS and PetaBytes of computing resources.
    </li><li>Example: Web search engines/databases processing millions of transactions 
        every second
    <p>
    <img src="./Introduction to Parallel Computing_files/biggerProblems.jpg" width="600" height="180">
    </p></li></ul>
<p>
</p></li><li><b>PROVIDE CONCURRENCY:</b> 
    <ul>
    <li>A single compute resource can only do one thing at a time. Multiple  
        compute resources can do many things simultaneously. 
    </li><li>Example: Collaborative Networks
        provide a global venue where people from around the 
        world can meet and conduct work "virtually".
    <p>
    <img src="./Introduction to Parallel Computing_files/collaborativeNetworks.jpg" width="600" height="182">
    </p></li></ul>
<p>
</p></li><li><b>TAKE ADVANTAGE OF NON-LOCAL RESOURCES:</b> 
    <ul>
    <li>Using compute resources on a wide area network, or even the Internet when 
        local compute resources are scarce or insufficient. Two examples below, each
        of which has over 1.7 million contributors globally (May 2018):
    </li><li>Example: SETI@home (<a href="http://setiathome.berkeley.edu/" target="_blank">setiathome.berkeley.edu</a>)        <!------------ Source:
        <A HREF=https://boincstats.com/ TARGET=_blank>
        https://boincstats.com/</A> 
        -------------->
    </li><li>Example: Folding@home (<a href="http://folding.stanford.edu/" target="folding">folding.stanford.edu</a>)
    <p>
    <img src="./Introduction to Parallel Computing_files/SETILogo.jpg" width="600" height="122">
    </p></li></ul>
<p>
</p></li><li><b>MAKE BETTER USE OF UNDERLYING PARALLEL HARDWARE:</b> 
    <ul>
    <li>Modern computers, even laptops, are parallel in architecture with
        multiple processors/cores.
    </li><li>Parallel software is specifically intended for parallel hardware with
        multiple cores, threads, etc.
    </li><li>In most cases, serial programs run on modern computers "waste" 
        potential computing power.
    <p>
    <table border="0" cellspacing="0" cellpadding="0">
    <tbody><tr valign="top">
    <td align="center">
    <img src="./Introduction to Parallel Computing_files/xeon5600processorDie3.jpg" width="600" height="321">
    <br>Intel Xeon processor with 6 cores and 6 L3 cache units</td>
    </tr></tbody></table>
    </p></li></ul>
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">The Future:</span>
</p><ul>
<li>During the past 20+ years, the trends indicated by ever faster
    networks, distributed systems, and multi-processor computer architectures
    (even at the desktop level) clearly show 
    that <b><i>parallelism is the future of computing</i></b>.
<p>
</p></li><li>In this same time period, there has been a greater than 
    <font style="background-color: yellow"><b>500,000x</b></font>
    increase in supercomputer performance, with no end currently in sight.
<p>
</p></li><li><b><i>The race is already on for Exascale Computing!</i></b>
    <ul>
    <li>Exaflop = 10<sup>18</sup> calculations per second
    </li></ul>
</li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/top500.1993-2018.png">
</p><dd><font size="-1"><i>Source: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font>
</dd></td>
</tr></tbody></table>

<!--========================================================================-->

<a name="Who"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Overview</span></td>
</tr></tbody></table>
</p><h2>Who is Using Parallel Computing?</h2>

<p>
<table border="0" cellspacing="0" cellpadding="0" width="800">
<tbody><tr valign="top">
<td>
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Science and Engineering:</span>
<ul>
<li>Historically, parallel computing has been considered to be 
    "the high end of computing", and has been used to model difficult 
    problems in many areas of science and engineering:
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
    <li>Atmosphere, Earth, Environment 
    </li><li>Physics - applied, nuclear, particle, condensed matter, 
        high pressure, fusion, photonics
    </li><li>Bioscience, Biotechnology, Genetics
    </li><li>Chemistry, Molecular Sciences
    </li><li>Geology, Seismology
    </li></ul></td>
<td><ul>
    <li>Mechanical Engineering - from prosthetics to spacecraft
    </li><li>Electrical Engineering, Circuit Design, Microelectronics    
    </li><li>Computer Science, Mathematics
    </li><li>Defense, Weapons
    </li></ul></td>
</tr></tbody></table>
<p>
<img src="./Introduction to Parallel Computing_files/simulations01.jpg" width="781" height="357">
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Industrial and Commercial:</span>
</p><ul>
<li>Today, commercial applications provide an equal or greater driving 
    force in the development of faster computers.
    These applications require the processing of large 
    amounts of data in sophisticated ways. For example:
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
    <li>"Big Data", databases, data mining
    </li><li>Artificial Intelligence (AI)
    </li><li>Web search engines, web based business services
    </li><li>Medical imaging and diagnosis
    </li><li>Pharmaceutical design
    </li></ul></td>
<td><ul>
    <li>Financial and economic modeling
    </li><li>Management of national and multi-national corporations 
    </li><li>Advanced graphics and virtual reality, particularly in the entertainment
        industry
    </li><li>Networked video and multi-media technologies
    </li><li>Oil exploration
    </li></ul></td>
</tr></tbody></table>
<p>
<img src="./Introduction to Parallel Computing_files/simulations03.jpg" width="781" height="360">
</p></li></ul>
</td>
</tr></tbody></table>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Global Applications:</span>
</p><ul>
<li>Parallel computing is now being used extensively around the world, in a
    wide variety of applications.
<p>
<img src="./Introduction to Parallel Computing_files/top500Apps.gif" width="753" height="446"><br><font size="-1"><i>Source: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font><p>
</p><table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">


<td colspan="2">Click on images below for larger version</td>
</tr><tr valign="top">
<td><a href="./Introduction to Parallel Computing_files/top500SegmentsTime.png" target="_blank">
<img src="./Introduction to Parallel Computing_files/top500SegmentsTime.png" width="400"></a></td>
<td><a href="./Introduction to Parallel Computing_files/top500CountriesTime.png">
<img src="./Introduction to Parallel Computing_files/top500CountriesTime.png" target="_blank" width="400"></a></td>
</tr></tbody></table>
<br><font size="-1"><i>Source: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font>
</p></li></ul>

<!--========================================================================-->

<a name="Concepts"> <br><br> </a>
<a name="Neumann"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Concepts and Terminology</span></td>
</tr></tbody></table>
<h2>von Neumann Architecture</h2>

<ul>
<p>
</p><li>Named after the Hungarian mathematician/genius John von Neumann who first
    authored the general requirements for an electronic computer in his 1945 papers. 
<p>
</p></li><li>Also known as "stored-program computer" - both program instructions and data are 
    kept in electronic memory.  Differs from earlier computers which were programmed  
    through "hard wiring".
<p>
</p></li><li>Since then, virtually all computers have followed this basic design: 
   
</li></ul>
<p>
<table cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/vonNeumann1.gif" width="293" height="277"></td>
<td><ul>
<li>Comprised of four main components:
    <ul>
    <li>Memory
    </li><li>Control Unit
    </li><li>Arithmetic Logic Unit
    </li><li>Input/Output
    </li></ul>
<p>
</p></li><li>Read/write, random access memory is used to store both program instructions
    and data
    <ul>
    <li>Program instructions are coded data which tell the computer to do 
        something
    </li><li>Data is simply information to be used by the program
    </li></ul>
<p>
</p></li><li>Control unit fetches instructions/data from memory, decodes 
    the instructions and then <b><i>sequentially</i></b> coordinates operations
    to accomplish the programmed task.
<p>
</p></li><li>Arithmetic Unit performs basic arithmetic operations
<p>
</p></li><li>Input/Output is the interface to the human operator
</li></ul></td>
<td align="center"><img src="./Introduction to Parallel Computing_files/vonNeumann2.jpg" width="221" height="287" hspace="20">
<br><i>John von Neumann circa 1940s <br>(Source: LANL archives)</i></td>
</tr></tbody></table>
</p><p>
</p><ul>
<p>
</p><li>More info on his other remarkable accomplishments: 
    <a href="http://en.wikipedia.org/wiki/John_von_Neumann" target="_blank">
    http://en.wikipedia.org/wiki/John_von_Neumann</a>
<p>
</p></li><li>So what? Who cares?  
    <ul>
    <li>Well, parallel computers still follow this basic design,
        just multiplied in units. The basic, fundamental architecture remains the 
        same.
    </li></ul>
</li></ul>

<!--========================================================================-->
<p>
<a name="Flynn"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Concepts and Terminology</span></td>
</tr></tbody></table>
</p><h2>Flynn's Classical Taxonomy</h2>

<ul>
<li>There are different ways to classify parallel computers. Examples available
    <a href="https://computing.llnl.gov/tutorials/parallel_comp/parallelClassifications.pdf" target="_blank">HERE</a>. 
<p>
</p></li><li>One of the more widely used classifications, in use since 1966, is called 
    Flynn's Taxonomy.
<p>
</p></li><li>Flynn's taxonomy distinguishes multi-processor computer architectures 
    according
    to how they can be classified along the two independent dimensions of
    <b><i>Instruction Stream</i></b> and <b><i>Data Stream</i></b>.  
    Each of these dimensions can have only one of two possible states:
    <b><i>Single</i></b> or <b><i>Multiple</i></b>.
<p>
</p></li><li>The matrix below defines the 4 possible classifications according to Flynn:
<p>
<img src="./Introduction to Parallel Computing_files/flynnsTaxonomy.gif" width="425">
</p></li></ul>

<p><br></p><hr><p><br>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Single Instruction, Single Data (SISD):</span>
</p><ul>
<li>A serial (non-parallel) computer
</li><li><b>Single Instruction:</b> Only one instruction stream is
    being acted on by the CPU during any one clock cycle
</li><li><b>Single Data:</b> Only one data stream is being used as input during any one clock cycle
</li><li>Deterministic execution     
</li><li>This is the oldest type of computer
</li><li>Examples: older generation mainframes, minicomputers, workstations and single
    processor/core PCs.
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td align="left"><img src="./Introduction to Parallel Computing_files/sisd2.gif" height="224" border="1" hspace="30"></td>
<td> </td>
<td><img src="./Introduction to Parallel Computing_files/sisd.gif" width="188" height="224"></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/univac1.LLNL.200pix.jpg" width="262" height="200">
<br><b>UNIVAC1</b></td>
<td><img src="./Introduction to Parallel Computing_files/ibm.360.200pix.jpg" width="300" height="200">
<br><b>IBM 360</b></td>
<td><img src="./Introduction to Parallel Computing_files/cray1.LLNL.200pix.jpg" width="200" height="200">
<br><b>CRAY1</b></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/cdc7600.LLNL.200pix.jpg" width="262" height="200">
<br><b>CDC 7600</b></td>
<td><img src="./Introduction to Parallel Computing_files/pdp1.LLNL.200pix.jpg" width="298" height="200">
<br><b>PDP1</b></td>
<td><img src="./Introduction to Parallel Computing_files/dellLaptop.200pix.jpg" width="201" height="200">
<br><b>Dell Laptop</b></td>
</tr></tbody></table>
</p></li></ul>

<p><br></p><hr><p><br>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Single Instruction, Multiple Data (SIMD):</span>
</p><ul>
<li>A type of parallel computer
</li><li><b>Single Instruction:</b> All processing units execute the same instruction at        any given clock cycle
</li><li><b>Multiple Data:</b> Each processing unit can operate on a different data
    element
</li><li>Best suited for specialized problems characterized by a high degree of 
    regularity, such as graphics/image processing.
</li><li>Synchronous (lockstep) and deterministic execution
</li><li>Two varieties: Processor Arrays and Vector Pipelines
</li><li>Examples: 
    <ul type="circle">
    <li>Processor Arrays: Thinking Machines CM-2, MasPar MP-1 &amp; MP-2, 
        ILLIAC IV 
    </li><li>Vector Pipelines: IBM 9000, Cray X-MP, Y-MP &amp; C90, Fujitsu VP, NEC SX-2, 
        Hitachi S820, ETA10
    </li></ul>
</li><li>Most modern computers, particularly those with graphics processor units 
   (GPUs) employ SIMD instructions and execution units.
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/simd3.gif" height="224" border="1"></td>
<td> </td>
<td><img src="./Introduction to Parallel Computing_files/simd.gif" width="438" height="224"></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/illiacIV.200pix.jpg" width="293" height="200">
<br><b>ILLIAC IV</b></td>
<td><img src="./Introduction to Parallel Computing_files/MasPar.200pix.jpg" width="172" height="200">
<br><b>MasPar</b></td>
<td><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="./Introduction to Parallel Computing_files/simd2.gif" width="400" height="147" border="0"></td>
</tr></tbody></table>
<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/crayXMP.200pix.jpg" width="150" height="200">
<br><b>Cray X-MP</b></td>
<td><img src="./Introduction to Parallel Computing_files/crayYMP.200pix.jpg" width="282" height="200">
<br><b>Cray Y-MP</b></td>
<td><img src="./Introduction to Parallel Computing_files/cm2.200pix.jpg" width="298" height="200">
<br><b>Thinking Machines CM-2</b></td>
<td><img src="./Introduction to Parallel Computing_files/cellProcessor.200pix.jpg" width="179" height="200">
<br><b>Cell Processor (GPU)</b></td>
</tr></tbody></table>
</p></li></ul>

<p><br></p><hr><p><br>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Multiple Instruction, Single Data (MISD):</span>
</p><ul>
<li>A type of parallel computer
</li><li><b>Multiple Instruction:</b> Each processing unit operates on the data 
    independently via separate instruction streams.
</li><li><b>Single Data:</b> A single data stream is fed into multiple processing units. 
</li><li>Few (if any) actual examples of this class of parallel computer have ever
    existed.
</li><li>Some conceivable uses might be:
    <ul>
    <li>multiple frequency filters operating on a single signal stream
    </li><li>multiple cryptography algorithms attempting to crack a single coded 
        message.
    </li></ul>
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="center" align="center">
<td align="left"><img src="./Introduction to Parallel Computing_files/misd4.gif" height="245" hspace="30" border="1"></td>
<td><img src="./Introduction to Parallel Computing_files/misd.gif" width="438" height="207" hspace="20"></td>
</tr></tbody></table>
</p></li></ul>

<p></p><hr><p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Multiple Instruction, Multiple Data (MIMD):</span>
</p><ul>
<li>A type of parallel computer
</li><li><b>Multiple Instruction:</b> Every processor may be executing a different 
    instruction stream
</li><li><b>Multiple Data:</b> Every processor may be working with a different data 
    stream
</li><li>Execution can be synchronous or asynchronous, deterministic or 
    non-deterministic
</li><li>Currently, the most common type of parallel computer - most modern
    supercomputers fall into this category.
</li><li>Examples: most current supercomputers, networked parallel computer 
    clusters and "grids", multi-processor SMP computers, multi-core PCs.
</li><li>Note: many MIMD architectures also include SIMD execution sub-components
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/mimd2.gif" height="245" border="1"></td>
<td colspan="2"><img src="./Introduction to Parallel Computing_files/mimd.gif" width="438" height="245"></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/ibmPower5Cluster.200pix.jpg" width="301" height="200">
<br><b>IBM POWER5</b></td>
<td><img src="./Introduction to Parallel Computing_files/alphaserverCluster.200pix.jpg" width="302" height="200">
<br><b>HP/Compaq Alphaserver</b></td>
<td><img src="./Introduction to Parallel Computing_files/ia32Cluster.200pix.jpg" width="299" height="200">
<br><b>Intel IA32</b></td>
</tr></tbody></table>
<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/opteronCluster.200pix.jpg" width="299" height="200">
<br><b>AMD Opteron</b></td>
<td><img src="./Introduction to Parallel Computing_files/crayXT3Cluster.200pix.jpg" width="300" height="200">
<br><b>Cray XT3</b></td>
<td><img src="./Introduction to Parallel Computing_files/bglCluster.200pix.jpg" width="301" height="200">
<br><b>IBM BG/L</b></td>
</tr></tbody></table>
</p></li></ul>
<p>

<!--========================================================================-->

<a name="Terminology"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Concepts and Terminology</span></td>
</tr></tbody></table>
</p><h2>Some General Parallel Terminology</h2>

<ul>
<li>Like everything else, parallel computing has its own "jargon".  Some of the 
    more commonly used terms associated with parallel computing are listed below.
<p>
</p></li><li>Most of these will be discussed in more detail later.
<p>
</p><dl>
<dt><b>Supercomputing / High Performance Computing (HPC)</b>
</dt><dd>Using the world's fastest and largest computers to solve large problems. 
<p>
</p></dd><dt><b>Node </b>
</dt><dd>A standalone "computer in a box". Usually comprised of multiple CPUs/processors/cores, memory, network interfaces, etc. Nodes are networked 
together to comprise a supercomputer.
<p>
</p></dd><dt><b>CPU / Socket / Processor / Core </b>
</dt><dd>This varies, depending upon who you talk to. In the past, a CPU (Central Processing Unit) was a singular execution component for a computer. Then, multiple CPUs were incorporated into a node. Then, individual CPUs were subdivided into multiple "cores", each being a unique execution unit. CPUs with multiple cores are sometimes called "sockets" - vendor dependent. The result is a node with multiple CPUs, each containing multiple cores. The nomenclature is confused at times. Wonder why?
<p>
<img src="./Introduction to Parallel Computing_files/nodeSocketCores.jpg" width="800" heigth="375">
</p><p>
</p></dd><dt><b>Task </b>
</dt><dd>A logically discrete section of computational work.  A task is typically a 
    program or program-like set of instructions that is executed by a processor.
    A parallel program consists of multiple tasks running on multiple processors.
<p>
</p></dd><dt><b>Pipelining</b>
</dt><dd>Breaking a task into steps performed by different processor units, with inputs streaming through, much like an assembly line; a type of parallel computing.
<p>
</p></dd><dt><b>Shared Memory </b>
</dt><dd>From a strictly hardware point of view, describes a computer architecture
    where all processors have direct (usually bus based) access to common 
    physical memory.  In a programming sense, it describes a model where
    parallel tasks all have the same "picture" of memory and can directly
    address and access the same logical memory locations regardless 
    of where the physical memory actually exists.
<p>
</p></dd><dt><b>Symmetric Multi-Processor (SMP)</b>
</dt><dd>Shared memory hardware architecture where multiple processors share a single address space and have equal access to all resources.
<p>
</p></dd><dt><b>Distributed Memory </b>
</dt><dd>In hardware, refers to network based memory access for physical memory that 
    is not common. As a programming model, tasks can only logically "see" 
    local machine memory and must use communications to access memory on other
    machines where other tasks are executing.
<p>
</p></dd><dt><b>Communications </b>
</dt><dd>Parallel tasks typically need to exchange data.  There are several ways this can be accomplished, such as through a shared memory bus or over a network, however the actual event of data exchange is commonly referred to as communications regardless of the method employed.
<p>
</p></dd><dt><b>Synchronization </b>
</dt><dd>The coordination of parallel tasks in real time, very often associated with
communications. Often implemented by establishing a synchronization point within an application where a task may not proceed further until another task(s) reaches the same or logically equivalent point.
<p>
Synchronization usually involves waiting by at least one task, and can therefore cause a parallel application's wall clock execution time to increase.
</p><p>
</p></dd><dt><b>Granularity </b>
</dt><dd> In parallel computing, granularity is a qualitative measure of the ratio
    of computation to communication. 
    <ul>
    <li><b><i>Coarse: </i></b> relatively large amounts of computational work  
        are done between communication events
    </li><li><b><i>Fine:</i></b> relatively small amounts of computational work are 
        done between communication events
    </li></ul>
<p></p></dd><dt><b>Observed Speedup </b>
</dt><dd>Observed speedup of a code which has been parallelized, defined as:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td align="center">
<pre>wall-clock time of serial execution
-----------------------------------
 wall-clock time of parallel execution</pre></td>
</tr></tbody></table>
</p><p>
One of the simplest and most widely used indicators for a parallel program's performance.
</p><p>
</p></dd><dt><b>Parallel Overhead </b>
</dt><dd>The amount of time required to coordinate parallel tasks, as opposed to     
    doing useful work. Parallel overhead can include factors such as:
    <ul>
    <li>Task start-up time
    </li><li>Synchronizations
    </li><li>Data communications
    </li><li>Software overhead imposed by parallel languages, libraries, 
        operating system, etc.
    </li><li>Task termination time
    </li></ul>
<p>
</p></dd><dt><b>Massively Parallel </b>
</dt><dd>Refers to the hardware that comprises a given parallel system - having many processing elements. The meaning of "many" keeps increasing, but currently, the largest parallel computers are comprised of processing elements numbering in the hundreds of thousands to millions.
<p>
</p></dd><dt><b>Embarrassingly Parallel</b> 
</dt><dd>Solving many similar, but independent tasks
simultaneously; little to no need for coordination between the tasks.
<p>
</p></dd><dt><b>Scalability</b>
</dt><dd>Refers to a parallel system's (hardware and/or software) ability to demonstrate a proportionate increase in parallel speedup with the addition of more resources. Factors that contribute to scalability include:
    <ul>
    <li>Hardware - particularly memory-cpu bandwidths and network communication
        properties
    </li><li>Application algorithm
    </li><li>Parallel overhead related
    </li><li>Characteristics of your specific application
    </li></ul>
</dd></dl>
</li></ul>

<!--========================================================================-->

<a name="LimitsCosts"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Concepts and Terminology</span></td>
</tr></tbody></table>
<h2>Limits and Costs of Parallel Programming</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Amdahl's Law:</span>
<p>
<table cellpadding="0" cellspacing="0" width="100%">
<tbody><tr valign="top">
<td><ul>
<p>
</p><li><b>Amdahl's Law</b> states that potential program
    speedup is defined by the fraction of code (P) that can be parallelized:
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
                     1
    speedup   =   -------- 
                   1  - P

</b></pre></td></tr></tbody></table>
</p><p>
</p></li><li>If none of the code can be parallelized, P = 0 and the speedup = 1 (no
     speedup).  
<p>
</p></li><li>If all of the code is parallelized, P = 1 and the speedup is
     infinite (in theory).
<p>
</p></li><li>If 50% of the code can be parallelized, maximum speedup = 2, meaning
    the code will run twice as fast.
<p>
</p></li><li>Introducing the number of processors performing the parallel fraction of 
    work, the relationship can be modeled by:
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
                       1  
    speedup   =   ------------ 
                    P   +  S
                   ---
                    N

</b></pre></td></tr></tbody></table>
</p><p>
    where P = parallel fraction, N = number of processors and S = serial 
    fraction.
</p></li></ul>
</td>
<td><img src="./Introduction to Parallel Computing_files/amdahl1.gif" width="509" height="390" border="1" hspace="20">
<br>
<img src="./Introduction to Parallel Computing_files/amdahl2.gif" width="509" height="391" border="1" hspace="20"></td>
</tr></tbody></table>
</p><p>
</p><ul>
<li>It soon becomes obvious that there are limits to the scalability of 
     parallelism.  For example: 
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
                       speedup
          -------------------------------------
    N     P = .50   P = .90   P = .95   P = .99
  -----   -------   -------   -------   -------
     10      1.82      5.26      6.89      9.17
    100      1.98      9.17     16.80     50.25     
  1,000      1.99      9.91     19.62     90.99
 10,000      1.99      9.91     19.96     99.02
100,000      1.99      9.99     19.99     99.90

</b></pre></td></tr></tbody></table>
</p><p>
<b>"Famous" qoute:</b><i> You can spend a lifetime getting 95% of your code to be parallel, and never achieve better than 20x speedup no matter how many processors you throw at it!</i>
</p></li></ul>

<ul>
<p>
</p><li>However, certain problems demonstrate increased performance by increasing
     the problem size.  For example:
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
    2D Grid Calculations     85 seconds   85%    
    Serial fraction          15 seconds   15%    

</b></pre></td></tr></tbody></table>
</p><p>
    We can increase the problem size by doubling the grid dimensions and
    halving the time step. This results in four times the number of grid 
    points and twice the number of time steps. The timings then look like:
</p><p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
    2D Grid Calculations     680 seconds   97.84%    
    Serial fraction           15 seconds    2.16%    

</b></pre></td></tr></tbody></table>
</p><p>
</p></li><li>Problems that increase the percentage of parallel time with their size
    are more <b><i>scalable</i></b> than problems with a fixed percentage of
    parallel time.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Complexity:</span>
</p><ul>
<p>
</p><li>In general, parallel applications are much more complex than corresponding
    serial applications, perhaps an order of magnitude.  Not only do you have
    multiple instruction streams executing at the same time, but you also have
    data flowing between them.
<p>
</p></li><li>The costs of complexity are measured in programmer time in virtually every
    aspect of the software development cycle:
    <ul>
    <li>Design
    </li><li>Coding
    </li><li>Debugging
    </li><li>Tuning
    </li><li>Maintenance
    </li></ul>
<p>
</p></li><li>Adhering to "good" software development practices is essential when 
    working with parallel applications - especially if somebody besides
    you will have to work with the software.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Portability:</span>
</p><ul>
<p>
</p><li>Thanks to standardization in several APIs, such as MPI, POSIX threads,
    and OpenMP, portability issues with parallel programs are not as
    serious as in years past.  However...
<p>
</p></li><li>All of the usual portability issues associated with serial programs
    apply to parallel programs.  For example, if you use vendor "enhancements"
    to Fortran, C or C++, portability will be a problem.
<p>
</p></li><li>Even though standards exist for several APIs, implementations will differ
    in a number of details, sometimes to the point of requiring code 
    modifications in order to effect portability.
<p>
</p></li><li>Operating systems can play a key role in code portability issues.  
<p>
</p></li><li>Hardware architectures are characteristically highly variable and can
    affect portability.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Resource Requirements:</span>
</p><ul>
<p>
</p><li>The primary intent of parallel programming is to decrease execution
    wall clock time, however in order to accomplish this, more CPU time
    is required.  For example, a parallel code that runs in 1 hour on 8
    processors actually uses 8 hours of CPU time.
<p>
</p></li><li>The amount of memory required can be greater for parallel codes than
    serial codes, due to the need to replicate data and for overheads
    associated with parallel support libraries and subsystems.
<p>
</p></li><li>For short running parallel programs, there can actually be a decrease
    in performance compared to a similar serial implementation.  The overhead 
    costs associated with setting up the parallel environment, task creation,
    communications and task termination can comprise a significant portion of
    the total execution time for short runs.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Scalability:</span>
</p><ul>
<p>
</p><li>Two types of scaling based on time to solution: strong scaling and weak
    scaling.
<p>

<img src="./Introduction to Parallel Computing_files/strongWeakScaling.gif" align="right" hspace="20" width="300">
</p></li><li><b>Strong scaling:</b> 
<ul>
<li>The total problem size stays fixed as more processors are added.
</li><li>Goal is to run the same problem size faster
</li><li>Perfect scaling means problem is solved in 1/P time (compared to serial)
</li></ul>
<p>
</p></li><li><b>Weak scaling:</b> 
<ul>
<li>The problem size <i>per processor</i> stays fixed as more processors are added.
</li><li>Goal is to run larger problem in same amount of time
</li><li>Perfect scaling means problem Px runs in same time as single processor run
</li></ul>
<p>
</p></li><li>The ability of a parallel program's performance to scale is a result
    of a number of interrelated factors.  Simply adding more processors
    is rarely the answer.
<p>
</p></li><li>The algorithm may have inherent limits to scalability.  At some point,
    adding more resources causes performance to decrease.  This is a common
    situation with many parallel applications.
<p>
</p></li><li>Hardware factors play a significant role in scalability.  Examples:
    <ul>
    <li>Memory-cpu bus bandwidth on an SMP machine
    </li><li>Communications network bandwidth
    </li><li>Amount of memory available on any given machine or set of machines
    </li><li>Processor clock speed
    </li></ul>    
<p>
</p></li><li>Parallel support libraries and subsystems software can limit scalability
    independent of your application.
</li></ul>

<!--========================================================================-->

<a name="MemoryArch"> <br><br> </a>
<a name="SharedMemory"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Computer Memory Architectures</span></td>
</tr></tbody></table>
<h2>Shared Memory</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">General Characteristics:</span>
<p>
<table cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><ul>
<li>Shared memory parallel computers vary widely, but generally have in common 
    the ability for all processors to access all memory as global address space. 
<p>
</p></li><li>Multiple processors can operate independently but share the same memory 
    resources.
<p>
</p></li><li>Changes in a memory location effected by one processor are visible to all
    other processors.
<p>
</p></li><li>Historically, shared memory machines have been classified as  
    <b><i>UMA</i></b> and <b><i>NUMA</i></b>, based upon memory access times.
<p>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Uniform Memory Access (UMA):</span>
    </p><ul>
    <li>Most commonly represented today by <b><i>Symmetric Multiprocessor 
       (SMP)</i></b> machines
    </li><li>Identical processors 
    </li><li>Equal access and access times to memory 
    </li><li>Sometimes called CC-UMA - Cache Coherent UMA.
        Cache coherent means if one processor updates a location in shared 
        memory, all 
        the other processors know about the update.  Cache coherency is 
        accomplished at the hardware level. 
    </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Non-Uniform Memory Access (NUMA):</span>
    </p><ul>
    <li>Often made by physically linking two or more SMPs 
    </li><li>One SMP can directly access memory of another SMP 
    </li><li>Not all processors have equal access time to all memories 
    </li><li>Memory access across link is slower
    </li><li>If cache coherency is maintained, then may also be called CC-NUMA - 
        Cache Coherent NUMA 
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Advantages:</span>
    </p><ul>
    <li>Global address space provides a user-friendly programming perspective 
        to memory
    </li><li>Data sharing between tasks is both fast and uniform due to the proximity 
        of memory to CPUs
    </li></ul>
</td>
<td align="center"><img src="./Introduction to Parallel Computing_files/shared_mem.gif" width="414" height="285">
<br><b>Shared Memory (UMA)</b><br><br><br>
<img src="./Introduction to Parallel Computing_files/numa.gif" width="484" height="196">
<br><b>Shared Memory (NUMA)</b>
</td>
</tr></tbody></table>
</p><p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Disadvantages:</span>
</p><ul>
<li>Primary disadvantage is the lack of scalability between memory and CPUs.
    Adding more CPUs can geometrically increases traffic on the shared
    memory-CPU path, and for cache coherent systems, geometrically increase 
    traffic associated with cache/memory management. 
</li><li>Programmer responsibility for synchronization constructs that ensure
    "correct" access of global memory. 
</li></ul>


<!--========================================================================-->
<p>
<a name="DistributedMemory"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Computer Memory Architectures</span></td>
</tr></tbody></table>
</p><h2>Distributed Memory</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">General Characteristics:</span>
<ul>
<p>
</p><li>Like shared memory systems, distributed memory systems vary widely but 
    share a common characteristic. Distributed memory systems require a 
    communication network to connect inter-processor memory.
<p>
<img src="./Introduction to Parallel Computing_files/distributed_mem.gif" width="484" height="196" hspace="10">
</p><p>
</p></li><li>Processors have their own local memory.  Memory addresses in one 
    processor do not map to another processor, so there is no concept of
    global address space across all processors.
<p>
</p></li><li>Because each processor has its own local memory, it operates 
    independently. Changes it makes to its local memory have no effect
    on the memory of other processors.  Hence, the concept of cache
    coherency does not apply.
<p>
</p></li><li>When a processor needs access to data in another processor, it is 
    usually the task of the programmer to explicitly define how and when
    data is communicated.  Synchronization between tasks is likewise the
    programmer's responsibility.
<p>
</p></li><li>The network "fabric" used for data transfer varies widely, though it can
    be as simple as Ethernet.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Advantages:</span>
    </p><ul>
    <li>Memory is scalable with the number of processors. Increase the number of    
        processors and the size of memory increases proportionately. 
    </li><li>Each processor can rapidly access its own memory without interference
        and without the overhead incurred with trying to maintain global cache
        coherency.
    </li><li>Cost effectiveness: can use commodity, off-the-shelf processors and
        networking.
    </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Disadvantages:</span>
    </p><ul>
    <li>The programmer is responsible for many of the details associated with
        data communication between processors. 
    </li><li>It may be difficult to map existing data structures, based on global
        memory, to this memory organization. 
    </li><li>Non-uniform memory access times - data residing on a remote node
        takes longer to access than node local data.
    </li></ul>



<!--========================================================================-->
<p>
<a name="HybridMemory"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Computer Memory Architectures</span></td>
</tr></tbody></table>
</p><h2>Hybrid Distributed-Shared Memory</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">General Characteristics:</span>
<ul>
<p>
</p><li>The largest and fastest computers in the world today employ both shared
    and distributed memory architectures.
<p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/hybrid_mem.gif" width="484" height="196" hspace="10"></td>
<td><img src="./Introduction to Parallel Computing_files/hybrid_mem2.gif" width="484" height="196" hspace="10"></td>
</tr></tbody></table>
</p><p>
</p></li><li>The shared memory component can be a shared memory machine and/or
    graphics processing units (GPU).
<p>
</p></li><li>The distributed memory component is the networking of multiple shared memory/GPU
    machines, which know only about their own memory - not the memory on another
    machine. Therefore, network communications are required to move data from one
    machine to another.
<p>
</p></li><li>Current trends seem to indicate that this type of memory architecture
    will continue to prevail and increase at the high end of computing for
    the foreseeable future.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Advantages and Disadvantages:</span>
</p><ul>
<li>Whatever is common to both shared and distributed memory architectures. 
</li><li>Increased scalability is an important advantage
</li><li>Increased programmer complexity is an important disadvantage
</li></ul>


<!--========================================================================-->
<p>
<a name="Models"> <br><br> </a>
<a name="ModelsOverview"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
</p><h2>Overview</h2>

<ul>
<p>
</p><li>There are several parallel programming models in common use:
    <ul>
    <li>Shared Memory (without threads)
    </li><li>Threads
    </li><li>Distributed Memory / Message Passing
    </li><li>Data Parallel
    </li><li>Hybrid
    </li><li>Single Program Multiple Data (SPMD)
    </li><li>Multiple Program Multiple Data (MPMD)
    </li></ul>
<p>
</p></li><li><b>Parallel programming models exist as an abstraction above hardware
    and memory architectures.</b>
<p>
</p></li><li>Although it might not seem apparent, these models are <b>NOT</b> specific
    to a particular type of machine or memory architecture.  In fact, any
    of these models can (theoretically) be implemented on any underlying
    hardware.  Two examples from the past are discussed below.
<p>
<table border="0" cellspacing="0" cellpadding="0" width="800">
<tbody><tr valign="top">
<td colspan="2">
<b>SHARED memory model on a DISTRIBUTED memory machine:</b> 
<br>Kendall Square Research (KSR) ALLCACHE approach.  
Machine memory was physically distributed across networked machines, but        appeared to the user as a single shared memory global address space.         Generically, this approach is referred to as "virtual shared memory".
</td></tr><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/modelAbstraction1.gif" height="150" vspace="20"></td>
<td><img src="./Introduction to Parallel Computing_files/ksr1.gif" width="198" height="130" vspace="20"></td>
</tr><tr valign="top">
<td colspan="2">
<b>DISTRIBUTED memory model on a SHARED memory machine:</b> 
<br>Message Passing Interface (MPI) on SGI Origin 2000. The SGI Origin 2000 employed the CC-NUMA type of shared memory architecture, where every task has direct access to global address space spread across all machines.  However, the ability to send and receive messages using MPI, as is commonly done over a network of distributed memory machines, was implemented and commonly used.
</td></tr><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/modelAbstraction2.gif" height="150" vspace="20"></td>
<td><img src="./Introduction to Parallel Computing_files/sgiOrigin2000.jpg" width="198" height="149" vspace="20"></td>
</tr></tbody></table>
</p><p>
</p></li><li><b>Which model to use?</b> 
    This is often a combination of what is available and personal
    choice.  There is no "best" model, although there certainly are better
    implementations of some models over others.
<p>
</p></li><li>The following sections describe each of the models mentioned above, and
    also discuss some of their actual implementations.
</li></ul>


<!--========================================================================-->
<p>
<a name="ModelsShared"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
</p><h2>Shared Memory Model (without threads)</h2>

<ul>
<img src="./Introduction to Parallel Computing_files/sharedMemoryModel.gif" width="350" hspace="20" align="right">
<li>In this programming model, processes/tasks share a common address space,  
    which they read and write to asynchronously. 
<p>
</p></li><li>Various mechanisms such as locks / semaphores are used to control
    access to the shared memory, resolve contentions and to prevent race 
    conditions and deadlocks. 
<p>
</p></li><li>This is perhaps the simplest parallel programming model.
<p>
</p></li><li>An advantage of this model from the programmer's point of view is that the
    notion of data "ownership" is lacking, so there is no need to specify 
    explicitly the communication of data between tasks. All processes see and
    have equal access to shared memory. Program development can often be 
    simplified.
<p>
</p></li><li>An important disadvantage in terms of performance is that it becomes
    more difficult to understand and manage <b><i>data locality</i></b>:
    <ul>
    <li>Keeping data local to the process that works on it conserves memory
        accesses, cache refreshes and bus traffic that occurs when multiple
        processes use the same data.
    </li><li>Unfortunately, controlling data locality is hard to understand and 
        may be beyond the control of the average user.
    </li></ul>
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Implementations:</span>
</p><ul>
    <p>
    </p><li>On stand-alone shared memory machines, native operating systems, 
        compilers and/or hardware provide support for shared memory programming.
        For example, the POSIX standard provides an API for using shared memory,
        and UNIX provides shared memory segments (shmget, shmat, shmctl, etc).
    <p>
    </p></li><li>On distributed memory machines, memory is physically distributed 
        across a network of machines, but made global through specialized hardware 
        and software. A variety of SHMEM implementations are available:
        <a href="http://en.wikipedia.org/wiki/SHMEM" target="_blank">
        http://en.wikipedia.org/wiki/SHMEM</a>.
    </li></ul>


<!--========================================================================-->
<p>
<a name="ModelsThreads"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
</p><h2>Threads Model</h2>

<ul>
<p>
</p><li>This programming model is a type of shared memory programming.
<p>
</p></li><li>In the threads model of parallel programming, a single "heavy weight" 
    process can have multiple "light weight", concurrent execution paths.
<p>
</p></li><li>For example:

<img src="./Introduction to Parallel Computing_files/threadsModel2.gif" align="right" width="350" height="550" hspace="20">
    <ul>
    <p>
    </p><li>The main program <b>a.out</b> is scheduled to run by the
        native operating system. <b>a.out</b> loads and acquires all of the
        necessary system and user resources to run. This is the "heavy weight"
        process.
    <p>
    </p></li><li><b>a.out</b> performs some serial work, and then creates
        a number of tasks (threads) that can be scheduled and run by the
        operating system concurrently.  
    <p>
    </p></li><li>Each thread has local data, but also, shares the entire resources of 
        <b>a.out</b>.  This saves the overhead associated with
        replicating a program's resources for each thread ("light weight").  
        Each thread also benefits from a global memory view because it shares the
        memory space of <b>a.out</b>.      
    <p>
    </p></li><li>A thread's work may best be described as a subroutine within
        the main program.  Any thread can execute any subroutine at the
        same time as other threads.
    <p>
    </p></li><li>Threads communicate with each other through global memory (updating
        address locations).  This requires synchronization constructs to ensure
        that more than one thread is not updating the same global address at
        any time.
    <p>
    </p></li><li>Threads can come and go, but <b>a.out</b> remains present 
        to provide the necessary shared resources until the
        application has completed.
    </li></ul>
    <p>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Implementations:</span>
</p><ul>
<li>From a programming perspective, threads implementations commonly 
    comprise:
    <ul type="circle">
    <li>A library of subroutines that are called from within 
        parallel source code
    </li><li>A set of compiler directives imbedded in either serial 
        or parallel source code
    </li></ul>
<p>
    In both cases, the programmer is responsible for determining the 
    parallelism (although compilers can sometimes help).
</p><p>
</p></li><li>Threaded implementations are not new in computing.  Historically,
    hardware vendors have implemented their own proprietary versions of
    threads. These implementations differed substantially from each other
    making it difficult for programmers to develop portable threaded
    applications. 
<p>
</p></li><li>Unrelated standardization efforts have resulted in 
    two very different implementations of threads:
    <b><i>POSIX Threads</i></b> and <b><i>OpenMP</i></b>.
<p>
</p></li><li><b>POSIX Threads</b> 
    <ul>
    <li>Specified by the IEEE POSIX 1003.1c standard (1995). C Language only. 
    </li><li>Part of Unix/Linux operating systems
    </li><li>Library based     
    </li><li>Commonly referred to as Pthreads.  
    </li><li>Very explicit parallelism; requires significant programmer attention 
        to detail.
    </li></ul>
<p>
</p></li><li><b>OpenMP</b>  
    <ul>
    <li>Industry standard, jointly defined and endorsed by a group of major 
        computer hardware and software vendors, organizations and individuals. 
    </li><li>Compiler directive based
    </li><li>Portable / multi-platform, including Unix and Windows platforms
    </li><li>Available in C/C++ and Fortran implementations
    </li><li>Can be very easy and simple to use - provides for "incremental 
        parallelism". Can begin with serial code.
    </li></ul>
<p>
</p></li><li>Other threaded implementations are common, but not discussed here:
<ul>
<li>Microsoft threads
</li><li>Java, Python threads
</li><li>CUDA threads for GPUs
</li></ul>

</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">More Information:</span>
</p><ul>
<li>POSIX Threads tutorial: 
<a href="https://computing.llnl.gov/tutorials/pthreads/" target="pthreads">
computing.llnl.gov/tutorials/pthreads</a>
</li><li>OpenMP tutorial: 
<a href="https://computing.llnl.gov/tutorials/openMP/" target="openMP">
computing.llnl.gov/tutorials/openMP</a>
</li></ul>

<!--========================================================================-->

<a name="ModelsMessage"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
<h2>Розподілена пам'ять / Модель обмін повідомленнями</h2>

<ul>
<p>
</p><li>Ця модель демонструє наступні характеристики:
<img src="./Introduction to Parallel Computing_files/msg_pass_model.gif" align="right" width="446" height="310" hspace="10" vspace="10">
    <ul>
    <p>
    </p><li>Набір завдань, які використовують власну 
    	локальну пам'ять під час обчислень. Кілька 
    	завдань можуть знаходитись на тій же фізичній
    	машині та / або на довільній кількості машин.
    <p>
    </p></li><li>Завдання обмінюються даними через 
    	повідомлення, відправляючи та отримуючи 
    	повідомлення.
    <p>
    </p></li><liПередача даних зазвичай вимагає кооперативних
    	операцій для кожного процесу. Наприклад, 
    	операція відправки повинна мати відповідну 
    	операцію прийому.
    </li></ul> 
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Реалізації:</span>
</p><ul>
<li>З точки зору програмування, реалізація передачі 
	повідомлень зазвичай складається з бібліотеки підпрограм. Виклики на ці підпрограми вкладені в вихідний код. Програміст несе відповідальність за визначення всіх паралелізму.
<p>
</p></li><li>Історично склалося, що з 80-х років доступні різні бібліотеки повідомлень. Ці реалізації суттєво 	відрізнялися один від одного, що ускладнює програмістам розробляти портативні програми. 
<p>
</p></li><li>У 1992 році був створений Форум MPI з основною метою створення стандартного інтерфейсу для 		реалізації послань до повідомлень. 
<p>
</p></li><li>Частина 1 <b>Message Passing Interface (MPI)</b> була випущена в 1994 році. Частина 2 
	(MPI-2) була випущена в 1996 році і MPI-3 в 2012 році. Всі специфікації MPI доступні в Інтернеті за адресою
    <a href="http://www.mpi-forum.org/docs/" target="_blank">http://www.mpi-forum.org/docs/</a>.
<p>
</p></li><li>MPI - це "фактичний" галузевий стандарт для передачі повідомлень, який замінює практично всі 		інші реалізовані повідомлення, що використовуються для виробничої роботи. Реалізації MPI існують 			практично для всіх популярних паралельних обчислювальних платформ. Не всі реалізації включають все в 		MPI-1, MPI-2 або MPI-3.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Більше інформації:</span>
</p><ul>
<li>Підручник з MPI: 
<a href="https://computing.llnl.gov/tutorials/mpi/" target="mpi">
computing.llnl.gov/tutorials/mpi</a>
</li></ul>

<!--========================================================================-->

<a name="ModelsData"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
<h2>Паралельна модель даних</h2>

<ul>
<p>
</p><li>Може також називатися <b>Partitioned Global Address Space (PGAS)</b>.
<p>
</p></li><li>Дана паралельна модель демонструє такі характеристики: 

<img src="./Introduction to Parallel Computing_files/data_parallel_model.gif" align="right" width="409" height="362" hspace="10" vspace="10">
    <ul>
    <p>
    </p><li>Адресний простір розглядається глобально
    <p>
    </p></li><li>Велика частина паралельної роботи зосереджена на виконанні операцій на 
    	наборі даних. Набір даних, як правило, організований у 
    	загальну структуру, таку як масив або куб.
    <p>
    </p></li><li>Набір завдань спільно працює на тій же структурі 
    	даних, однак кожне завдання працює на іншому розділі тієї ж структури даних.
    <p>
    </p></li><li>Завдання виконують одну й ту саму операцію при їх розподілі роботи, 
    	наприклад, "додати 4 до кожного елементу масиву".
    </li></ul>
<p>
</p></li><li>На загальних архітектурах пам'яті всі завдання можуть мати 
	доступ до структури даних через глобальну пам'ять.
<p>
</p></li><li>У розподілених архітектурах пам'яті глобальна структура 
	даних може бути логічно та / або фізично розподілена між завданнями.
</li></ul>
<br clear="">
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3"> Реалізації:</span>
</p><ul>
<p>
</p><li>В даний час існує декілька відносно популярних, а інколи і розроблювальних, 
	паралельних програмних рішень на основі моделі Data Parallel / PGAS.
<p>
</p></li><li><b>Coarray Fortran:</b> невеликий набір розширень для Fortran 95 для парамного програмування		 SPMD. Компілятор залежить. Додаткова інформація: 
    <a href="https://en.wikipedia.org/wiki/Coarray_Fortran" target="_blank">https://en.wikipedia.org/wiki/Coarray_Fortran</a>
<p>
</p></li><li><b>Unified Parallel C (UPC):</b> розширення на мову програмування C для паралельного 				програмування SPMD. Компілятор залежить. Додаткова інформація: 
    <a href="http://upc.lbl.gov/" target="_blank">http://upc.lbl.gov/</a>
<p>
</p></li><li><b>Global Arrays:</b> забезпечує спільне середовище програмування стилю пам'яті в контексті структури даних розподіленого масиву. Бібліотека публічного домену із закріпленнями C і Fortran77. Додаткова інформація:
    <a href="https://en.wikipedia.org/wiki/Global_Arrays" target="_blank">
    https://en.wikipedia.org/wiki/Global_Arrays</a>
<p>
</p></li><li><b>X10:</b> мова паралельного програмування на базі PGAS, розроблена компанією IBM в дослідницькому центрі ім. Томаса Дж. Уотсона. Більш детальна інформація:
    <a href="http://x10-lang.org/" target="_blank">http://x10-lang.org/</a>
<p>
</p></li><li><b>Chapel:</b> проект з паралельним програмуванням з відкритим кодом під керівництвом Cray. Додаткова інформація: 
    <a href="http://chapel.cray.com/" target="_blank">http://chapel.cray.com/</a>
</li></ul>

<!--========================================================================-->

<a name="Hybrid"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
<h2>Гібридна модель</h2>

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td>
<ul>
<li>Гібридна модель поєднує в собі більше ніж одну з описаних раніше моделей програмування.
<p>
</p></li><li>В даний час загальним прикладом гібридної моделі є поєднання моделі 
	передачі повідомлень (MPI) з моделлю потоків (OpenMP).
    <ul>
    <li>Потоки виконують інтенсивні обчислювальння ядра, використовуючи локальні,
        дані на вузлі
    </li><li>Зв'язок між процесами на різних вузлах відбувається по мережі за допомогою MPI
    </li></ul> 
<p>
</p></li><li>Ця гібридна модель добре поширюється на найпопулярнішу (в даний час) апаратне середовище кластеризованих мульти / багатоядерних машин. 
<p>
</p></li><li>Іншим подібним і все більш популярним прикладом гібридної моделі є використання MPI з процесором-графічним процесором (Graphics Processing Unit - програма обробки графіки).
    <ul>
    <li>Завдання MPI працюють на процесорах із використанням локальної пам'яті та спілкування між собою через мережу.
    </li><li>Обчислювальні інтенсивні ядра вивантажуються на вузли графічних процесорів.
    </li><li>Обмін даними між вузлом-локальною пам'яттю та графічними процесорами використовує CUDA (або щось еквівалентне).
    </li></ul>
<p>
</p></li><li>Інші гібридні моделі є загальними:
    <ul>
    <li>MPI with Pthreads
    </li><li>MPI з не-GPU прискорювачами
    </li><li>...
    </li></ul>
</li></ul>
</td>
<td align="right">
<img src="./Introduction to Parallel Computing_files/hybrid_model.gif" width="485" height="241" hspace="20">
<br>
<img src="./Introduction to Parallel Computing_files/hybrid_model2.gif" width="485" height="209" hspace="20" vspace="20">
</td></tr></tbody></table>


<!--========================================================================-->

<a name="SPMD-MPMD"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
<h2>SPMD and MPMD</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Одноразові дані декількох програм (SPMD):</span>
<ul>
<p>
</p><li>SPMD - це насправді модель програмування "високого рівня", яка може бути побудована на будь-якій комбінації раніше згаданих моделей паралельного програмування.
<img src="./Introduction to Parallel Computing_files/spmd_model.gif" align="right" width="395" height="110" hspace="10" vspace="10">

<p>
</p></li><li>SINGLE PROGRAM:  всі завдання виконують свою копію однієї і тієї ж програми одночасно. Ця програма може бути потоками, передаванням повідомлень, паралельними або гібридними даними.
<p>
</p></li><li>MULTIPLE DATA: Всі завдання можуть використовувати різні дані
<p>
</p></li><li>Програми SPMD зазвичай мають необхідну логіку, запрограмовану в них, щоб дозволити різним завданням відгалужуватися або умовно виконувати лише ті частини програми, які вони призначені для виконання. Тобто завдання не обов'язково повинні виконувати всю програму - можливо, лише її частину.
<p>
</p></li><li>Модель SPMD, що використовує передавання повідомлень або гібридне програмування, є, 	мабуть, найбільш часто використовуваною моделлю паралельного програмування для багатонасельних 	кластерів.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Кілька даних декількох програм (MPMD):</span>
</p><ul>
<p>
</p><li>Як і SPMD, MPMD насправді є моделлю програмування "високого рівня", яка може бути 			побудована на будь-якій комбінації раніше згаданих моделей паралельного програмування.

<img src="./Introduction to Parallel Computing_files/mpmd_model.gif" align="right" width="395" height="110" hspace="10" vspace="10">

<p>
</p></li><li>MULTIPLE PROGRAM: Завдання можуть виконувати різні програми одночасно. Програмами можуть бути потоки, передача повідомлень, дані паралельні або гібридні.
<p>
</p></li><li>MULTIPLE DATA: Всі завдання можуть використовувати різні дані
<p>
</p></li><li>MPMD-програми не настільки поширені, як програми SPMD, але можуть бути краще підходять для певних типів проблем, особливо тих, які краще піддаються функціональному розкладу, ніж розбиттям домену (обговорене пізніше під розділом <a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignPartitioning">
    Partioning</a>).
</li></ul>

<!--========================================================================-->
<p>
<a name="Designing"> <br><br> </a>
<a name="DesignAutomatic"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2Автоматична або ручна паралелізація </h2>

<ul>
<p>
</p><li>Проектування та розробка паралельних програм характерно було дуже ручним процесом. Програміст, як правило, несе відповідальність за ідентифікацію та фактичне здійснення паралелізму.
<p>
</p></li><li>Дуже часто ручні розробки паралельних кодів - це багато часу, складний, схильний до помилок та <i><b>ітераційний</b></i> процес.
<p>
</p></li><li>Протягом кількох років існують різні інструменти, які допомагають програмісту перетворити серійні програми на паралельні програми. Найпоширеніший тип інструмента, який використовується для автоматичного розпаралелювання серійної програми, - це компілятор, що розпаралелює, або попередній процесор.
<p>
</p></li><li>Компонент, що розпаралелює, зазвичай працює двома способами:
<p>
<b>Повністю автоматичний</b>
</p><ul>
<li>Компілятор аналізує вихідний код та визначає можливості паралелізму.  
</li><li>Аналіз включає в себе ідентифікацію інгібіторів паралелізму та, можливо, розрахунок витрат на те, чи паралелізм дійсно покращить продуктивність.
</li><li>Цикли (do, for) - найчастіша мета для автоматичної розпаралелювання.
</li></ul>
<p>
<b>Вручну</b>
</p><ul>
<li>Використовуючи "директиви компілятора" або, можливо, прапори компілятора, програміст явним чином повідомляє компілятору, як паралелізувати код.
</li><li>Може бути в змозі використовувати разом з деякою мірою автоматичної розпаралелювання також.
</li></ul>
<p>
</p></li><li>Найпоширеніша паралілізація, сформована компілятором, виконується за допомогою спільної пам'яті на вузлі та потоків (наприклад, OpenMP).
<p>
</p></li><li>Якщо ви починаєте з існуючого серійного коду та маєте обмеження часу або бюджету, то відповідь може стати автоматичним розпаралелюванням. Проте існує кілька важливих застережень, які застосовуються до автоматичної розпаралелювання:
    <ul>
    <li>Неправильні результати можуть бути отримані
    </li><li>Продуктивність може деградувати
    </li><li>Набагато менш гнучкий, ніж ручне розпаралелювання
    </li><li>Обмежено до підмножини (в основному петель) коду
    </li><li>Може фактично не розпаралелювати код, якщо аналіз компілятора передбачає наявність інгібіторів або код занадто складний
    </li></ul><p>
</p></li><li>Решта цього розділу стосується ручного методу розробки паралельних кодів.
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignUnderstand"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>Зрозумійте проблему та програму</h2>

<ul>
<p>
</p><li>Безумовно, першим кроком у розробці паралельного програмного забезпечення є спочатку зрозуміти проблему, яку ви хочете вирішити паралельно. Якщо ви починаєте з серійної програми, це вимагає розуміння існуючого коду.
 
<p>
</p></li><li>Перш ніж витрачати час на спробу розробити паралельне рішення задачі, визначте, чи є така проблема дійсно паралізованою. 
<ul>
<p>
</p><li>Приклад простої для розпаралелювання проблеми:
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr><td><b>
    Обчислити потенційну енергію для кожної з декількох тисяч незалежних конформацій молекули. Коли закінчите, знайдіть мінімальну конформацію енергії.
</b></td></tr></tbody></table>
</p><p>
    Цю проблему можна вирішити паралельно. Кожна молекулярна конформація самостійно визначається. Розрахунок мінімальної конформації енергії також є паралізованою проблемою.
</p><p>
</p></li><li>Приклад проблеми з незмінним паралелізмом: 
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr><td><b>
    Розрахунок серії Фібоначчі (0,1,1,2,3,5,8,13,21, ...) за допомогою формули:
</b><p><b>F(n) = F(n-1) + F(n-2)
</b><br>
</p></td></tr></tbody></table>
</p><p> Розрахунок значення F (n) використовує такі значення як F (n-1), так і F (n-2), які необхідно спочатку обчислити.  
</p></li></ul> 
</li></ul>
<p>

<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td>
<ul>
<li>Визначте  
    <font style="background-color: yellow"><i><b>точки доступу</b></i></font>програми :
    <ul>
    <li>Знайте, де виконується більша частина справжньої роботи. Більшість науково-технічних програм зазвичай виконують більшість своїх робіт у кількох місцях. 
    </li><li>Засоби профілювання та інструменти аналізу продуктивності можуть допомогти тут
    </li><li>Зосередьтеся на розпаралелювання точок доступу та ігноруйте ті розділи програми, що призводить до невеликого використання процесора. 
    </li></ul>
<p>
</p></li><li>Визначте <font style="background-color: yellow"><i><b>вузькі місця</b></i></font>
    в програмі:
    <ul>
    <li>Чи існують ділянки, які непропорційно сповільнюють роботу або призводять до призупинення 	чи відкладені роботи, що паралелізуються? Наприклад, введення / виведення - це, як 			правило, щось, що сповільнює роботу програми.
    </li><li>Може бути можливо перебудувати програму або використовувати інший алгоритм для зменшення або усунення непотрібних повільних областей
    </li></ul>
<p>
</p></li><li>Визначте інгібітори паралелізму. Одним із загальних класів інгібіторів є
    is <i>залежність даних</i>, про що свідчить послідовність Фібоначчі вище.  
<p>
</p></li><li>Вивчіть інші алгоритми, якщо це можливо. Це може бути найважливішим аспектом при розробці паралельної програми.
<p>
</p></li><li>Скористайтеся перевагами оптимізованого паралельного програмного забезпечення сторонніх розробників та високо оптимізованих математичних бібліотек, доступних провідними постачальниками (ESSL IBM, MKL Intel, AMCL AMD та ін.).
</li></ul></td>
<td><img src="./Introduction to Parallel Computing_files/hotspotBottleneck2.jpg" hspace="20"></td>
</tr></tbody></table>


<!--========================================================================-->

<a name="DesignPartitioning"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>Розбиття</h2>

<ul>
<p>
</p><li>Одним з перших кроків при проектуванні паралельної програми є розбиття цієї проблеми на дискретні "шматки" роботи, які можуть бути розподілені на декілька завдань. Це відоме як розкладання або розбиття.
<p>
</p></li><li>Існує два основних способи розподілу обчислювальної роботи серед паралельних завдань: <b><i>розбиття на ділянки</i></b> і 
    <b><i>функціональне розбиття</i></b>. 
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Розподіл ділянок:</span>
    </p><ul>
    <p>
    </p><li>У цьому типі розбиття дані, пов'язані з проблемою, розкладені. Кожна паралельна задача потім працює на частині даних.
    <p><img src="./Introduction to Parallel Computing_files/domain_decomp.gif" width="388" height="216">
    </p><p>
<a name="distributions"> </a>
    </p></li><li>Існують різні способи розподілу даних:
    <p><img src="./Introduction to Parallel Computing_files/distributions.gif" width="502" height="386">
    </p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Функціональне розкладання:</span>
    </p><ul>
    <p>
    </p><li>У цьому підході основна увага приділяється виконанню обчислень, а не даним, яким керується обчислення. Проблема розкладається відповідно до роботи, яка повинна бути виконана. Потім кожне завдання виконує частину загальної роботи.
    <p><img src="./Introduction to Parallel Computing_files/functional_decomp.gif" width="587" height="353">
    </p><p>
    </p></li><li>Функціональна декомпозиція добре підходить до проблем, які можна розділити на різні завдання. Наприклад:
        <dl>
        <p>
        </p><dt><b>Моделювання екосистем</b>
        <br>Кожна програма розраховує кількість населення певної групи, де ріст кожної групи залежить від кількості його сусідів. З розвитком часу кожен процес обчислює його поточний стан, потім обмінюється інформацією з сусідніми групами. Всі завдання виконуються, щоб обчислити стан на наступному кроці.
        <p>
        <img src="./Introduction to Parallel Computing_files/functional_ex1.gif" width="567" height="221">
        </p><p>
        </p></dt><dt><b>Обробка</b>
        <br>Набір даних аудіосигналу проходить через чотири різні обчислювальні фільтри. Кожен фільтр є окремим процесом. Перший сегмент даних повинен пройти через перший фільтр, перш ніж перейти до другого. Коли це відбувається, другий сегмент даних проходить через перший фільтр. На той час, коли четвертий сегмент даних знаходиться в першому фільтрі, всі чотири завдання є зайняті.
        <p>
        <img src="./Introduction to Parallel Computing_files/functional_ex2.gif" width="703" height="272">
        </p><p>
        </p></dt><dt><b>Кліматичне моделювання</b>
        <br>Кожна модель може розглядатися як окреме завдання. Стрілки являють собою обмін даними між компонентами під час обчислень: модель атмосфери формує дані швидкості вітру, які використовуються океанською моделлю, модель океану дає дані про температуру поверхні моря, які використовуються моделлю атмосфери тощо.
        <p>
        <table border="0" cellpadding="0" cellspacing="0">
        <tbody><tr valign="top">
        <td><img src="./Introduction to Parallel Computing_files/functional_ex3.gif" width="372" height="257"></td>
        <td><a href="./Introduction to Parallel Computing_files/climateModelling.png" target="_blank">
            <img src="./Introduction to Parallel Computing_files/climateModelling.png" height="257" hspace="50"></a></td>
        </tr></tbody></table>
        </p></dt></dl>
<p>
</p></li><li>Поєднання цих двох типів розкладання задач є загальним і природним.
<p>
</p></li></ul>

<!--========================================================================-->

<a name="DesignCommunications"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
<h2>Зв'язок</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Кому потрібні комунікації?</span>
<ul>
<p>Необхідність зв'язку між завданнями залежить від вашої проблеми:
</p><p>
<table border="0" cellspacing="0" cellpadding="0" width="90%">
<tbody><tr valign="top">
<td><b>Вам не потрібні комунікації:</b>
<ul>
<li>Деякі типи проблем можуть бути розкладені та виконані паралельно з практично відсутністю необхідності у спільних даних для передачі даних. Ці типи проблем часто називаються <b><i>незрозуміло паралельними</i></b> - повідомлення або відсутність повідомлень потрібні. 
</li><li>Наприклад, уявіть собі операцію обробки зображення, коли кожен піксель у чорно-білому 		малюнку повинен мати свій колір навпаки. Дані зображення можна легко розподілити на декілька 	 завдань, які потім діють незалежно один від одного, щоб виконувати свою частину роботи.  
</li></ul></td>
<td><b>Вам потрібні комунікації:</b>
<ul>
<li>Більшість паралельних програм не зовсім настільки прості, і вони вимагають завдань для обміну даними один з одним.  
</li><li>Наприклад, проблема з дифузією 2D вимагає задачі знати температури, розраховані завдань, які мають сусідні дані. Зміни сусідніх даних безпосередньо впливають на дані цього завдання.
</li></ul></td>
</tr><tr valign="top">
<td align="center"><br><img src="./Introduction to Parallel Computing_files/black2white.gif"></td>
<td align="center"><br><img src="./Introduction to Parallel Computing_files/heat_partitioned.gif"></td>
</tr></tbody></table>
</p></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Фактори, які слід враховувати:</span>
</p><ul>
<p>
При розробці міжзадачних комунікацій вашої програми необхідно враховувати ряд важливих факторів:
</p><p>
<img src="./Introduction to Parallel Computing_files/commOverhead.jpg" align="right" hspace="20" width="250">
</p><li><b>Комунікаційні накладні витрати</b>
    <ul>
    <li>Міжзазначне спілкування практично завжди означає накладні витрати.
    </li><li>Циклі та ресурси машини, які можуть використовуватися для обчислень, замість цього використовуються для пакетування та передачі даних.
    </li><li>Зв'язок часто вимагає певного типу синхронізації між завданнями, що може призвести до того, що завдання виконуються часом "очікування", а не робота.
    </li><li>Конкуруючий трафік зв'язку може наситити доступну смугу пропускання мережі, що ще більше погіршує проблеми з продуктивністю.
    </li></ul>
<p>
</p></li><li><b>Затримка проти пропускної здатності</b>
    <ul>
    <li><b><i>Затримка</i></b>- це час, який потрібно для відправки мінімального (0 байт) повідомлення з точки А в пункт B. Загально виражений як мікросекунди.
    </li><li><b><i>пропускна здатність</i></b>- це кількість даних, які можна передавати за одиницю часу. Зазвичай виражається як мегабайт / с або гігабайт / с.
    </li><li>Відправка багатьох невеликих повідомлень може призвести до затримки домінування накладних витрат на зв'язок. Часто ефективніше пакуйте невеликі повідомлення у великі повідомлення, тим самим збільшуючи ефективну пропускну здатність зв'язку.
    </li></ul>
<p>
</p></li><li><b>Видимість комунікацій</b>
    <ul>
    <li>З моделлю передачі повідомлень зв'язок є явним і загалом цілком видимим і під контролем програміста. 
    </li><li>За допомогою паралельної моделі даних, комунікації часто прозорі у програмному забезпеченні, особливо в розподілених архітектурах пам'яті. Програміст може навіть не вміти точно знати, як відбувається взаємодія між повідомленнями.
    </li></ul>
<p>
</p></li><li><b>Синхронні або асинхронні комунікації</b>
    <ul>
    <li>Синхронні комунікації вимагають певного типу "рукостискання" між завданнями, які використовують дані. Програміст може чітко структуруватися в коді, або це може статися на більш низькому рівні, невідомий програмісту.
    </li><li>Синхронні комунікації часто називають
        <b><i>блокування</i></b> комунікацій, оскільки інша робота повинна зачекати до завершення зв'язку.
    </li><li>Асинхронні зв'язки дозволяють завданням передавати дані незалежно один від одного. Наприклад, завдання 1 може підготувати і відправити повідомлення до завдання 2, а потім негайно почати робити іншу роботу. Коли завдання 2 фактично отримує дані, це не має значення. 
    </li><li>Асинхронні комунікації часто називають
        <b><i>неблокуючими</i></b> комунікаціями, оскільки інша робота може виконуватися під час спілкування.
    </li><li>Обчислення між класами при спілкуванні є єдиною найбільшою перевагою для використання асинхронних повідомлень.
    </li></ul>
<p>
</p></li><li><b>Обсяг комунікацій</b>
    <ul>
    <li>Знання, які завдання повинні взаємодіяти один з одним, є критичним під час розробки паралельного коду. Обидва описані нижче обчислення можуть бути реалізовані синхронно або асинхронно.
    </li><li><b><i>Point-to-point</i></b> - включає в себе два завдання з одним завданням, що виступає як відправник / виробник даних, а інший - як одержувач / споживач.
        the receiver/consumer.
    </li><li><b><i>Collective</i></b> - включає обмін даними між більш ніж двома завданнями, які часто вказуються як члени спільної групи чи колективу. Деякі загальні варіанти (їх більше):
    <p>
    <img src="./Introduction to Parallel Computing_files/collective_comm.gif"> 
    </p></li></ul>
<p>
</p></li><li><b>Ефективність комунікацій</b>
    <ul>
    <li>Часто, програміст має вибір, який може вплинути на продуктивність зв'язку. Тут згадуються лише деякі.        
    </li><li>Яка реалізація для даної моделі повинна бути використана? Як приклад, як приклад, Модель передачі повідомлень, одна реалізація MPI може бути швидшою на даній апаратній платформі, ніж інша.
    </li><li>Який тип комунікаційних операцій слід використовувати? Як згадувалося раніше, асинхронні операції зв'язку можуть поліпшити загальну продуктивність програми.
    </li><li>Мережева тканина - деякі платформи можуть пропонувати більше однієї мережі для зв'язку. Який з них найкращий?
    </li></ul>
<p>
</p><p>
</p></li><li><b>Накладні та складні</b>
<p>    
    <img src="./Introduction to Parallel Computing_files/helloWorldParallelCallgraph.gif" width="914" height="497">
</p><p>
</p></li><li>Нарешті, зрозумійте, що це лише частковий перелік речей, які слід розглянути !!!
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignSynchronization"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>Синхронізація</h2>

<img src="./Introduction to Parallel Computing_files/sychronization2.jpg" width="300" align="right" hspace="20">
<ul>
<li>Управління послідовністю роботи та виконання завдань є критичним дизайном для більшості паралельних програм.
<p>
</p></li><li>Може бути важливим чинником ефективності програми (або його відсутності)
<p>
</p></li><li>Часто вимагає "серіалізація" сегментів програми
</li></ul>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Типи синхронізації:</span>
<ul>
<p>
</p><li><b>Бар'єр</b>
    <ul>
    <li>Звичайно, випливає, що всі завдання беруть участь
    </li><li>Кожне завдання виконує свою роботу до досягнення бар'єру. Потім він зупиняється або "блокується".        
    </li><li>Коли останнє завдання досягає бар'єру, всі завдання синхронізуються.
    </li><li>Те, що відбувається від цього, змінюється. Часто робиться серійний розділ роботи. В інших випадках завдання автоматично відпускаються для продовження роботи.  
    </li></ul>
<p>
</p></li><li><b>Замок / семафор</b>
    <ul>
    <li>Можна задіяти будь-яку кількість завдань
    </li><li>Зазвичай використовується для серіалізації (захисту) доступу до глобальних даних або розділу коду. Тільки одне завдання одночасно може використовувати (власний) замок / семафор / прапор.
    </li><li>Перше завдання придбання блокування "встановлює" його. Це завдання може безпечно (серійно) отримати доступ до захищених даних або коду.
    </li><li>Інші завдання можуть намагатися набувати замок, але слід зачекати, поки завдання, яке належить блоку, не виключає його.
    </li><li>Можна блокувати або не блокувати
    </li></ul>
<p>
</p></li><li><b>Синхронні операції зв'язку</b>
    <ul>
    <li>Включає лише ті завдання, що виконують операцію зв'язку
    </li><li>Коли завдання виконує операцію зв'язку, потрібна певна форма координації з іншим завданням, що беруть участь у спілкуванні. Наприклад, перед тим, як завдання може виконати операцію відправки, вона спочатку повинна отримати підтвердження від приймаючої задачі, що його потрібно надіслати.
    </li><li>Обговорювалося раніше в розділі "Повідомлення".
    </li></ul>
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignDependencies"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>Залежності даних</h2>

<img src="./Introduction to Parallel Computing_files/dependencies1.jpg" align="right" hspace="20">
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Визначення:</span>
<ul>
<p>
</p><li>A <b><i>Залежність</i></b> існує між операторами програми , коли порядок виконання інструкції впливає на результати програми. 
<p>
</p></li><li>A <b><i>Залежність даних</i></b> виникає в результаті багаторазового використання одного і того ж місця (місць) в сховище з допомогою різних завдань.
</p></li><li>Залежність важлива для паралельного програмування, оскільки вони є одним з первинних інгібіторів паралелізму.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Приклади:</span>
</p><p>
</p><ul>
<li><b>Петля здійснює залежність даних</b>
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top">
<td><pre><b>
DO J = MYSTART,MYEND
   A(J) = A(J-1) * 2.0
END DO
</b></pre></td>
</tr></tbody></table>
</p><p>
    Значення A (J-1) повинно бути обчислено до значення A (J), тому A (J) виявляє залежність даних від A (J-1). Паралелізм гальмується.
</p><p> Якщо завдання 2 має A (J) і завдання 1 має A (J-1), обчислення правильного значення A (J) вимагає: 
    </p><ul type="circle">
    <li>Архітектура розподіленої пам'яті - задача 2 повинна отримати значення A (J-1) від завдання 1 після виконання завдання 1 для його обчислення
    </li><li>Архітектура спільної пам'яті. Завдання 2 має прочитати A (J-1) після завдання 1 оновлює його
    </li></ul>
<p>
</p></li><li><b>Незалежна залежність даних від циклу</b>
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top">
<td><pre><b>
Завдання 1        завдання 2
------        	    ------

X = 2               X = 4
  .                   .
  .                   .
Y = X**2           Y = X**3
</b></pre></td>
</tr></tbody></table>
</p><p>
    Як і в попередньому прикладі, паралелізм гальмується. Значення Y залежить від:
    </p><ul type="circle">
    <li>Архітектура розподіленої пам'яті - якщо або коли значення X передається між завданнями. 
    </li><li>Архітектура спільної пам'яті - останнє завдання, яке зберігає значення X.
    </li></ul>
<p>
</p></li><li>Хоча всі залежнощі даних важливі для ідентифікації під час розробки паралельних програм, залежно від виконання циклів є особливо важливими, оскільки цикли є, можливо, найпоширенішою метою процесу розпаралелювання.

</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Як керувати залежностями даних:</span>
</p><ul>
<p>
</p><li>Розподілені архітектури пам'яті - передача необхідних даних в точках синхронізації.
<p>
</p></li><li>Архітектура спільної пам'яті - синхронізація операцій читання / запису між завданнями. 
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignLoadBalance"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>Балансування навантаження</h2>

<ul>
<p>
</p><li>Балансування навантаження стосується практики розподілу приблизно однакових обсягів робіт серед завдань, щоб  <b><i>всі</i></b> завдання були постійно зайняті <b><i>весь</i></b> час. Це можна вважати мінімізацією часу простою завдання.
<p>
</p></li><li>Балансування навантаження важливе для паралельних програм з міркувань продуктивності. Наприклад, якщо для всіх завдань підлягає точка синхронізації бар'єру, повільне завдання визначатиме загальну продуктивність.
<p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/load_bal1.gif" width="403" height="188"></td>
<td><img src="./Introduction to Parallel Computing_files/loadImbalance2.jpg" height="188" hspace="50"></td>
</tr></tbody></table>
</p><p>
</p></li></ul>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Як досягти балансу завантаження</span>
<ul>
<p>
</p><li><b>Кожне завдання одержує рівну частину роботи</b>
    <ul>
    <li>Для операцій масиву / матриці, де кожне завдання виконує подібну роботу, рівномірно розподіляють набір даних серед завдань.
    </li><li>Для ітерацій циклу, де робота, виконана у кожній ітерації, аналогічна, рівномірно розподіляють ітерації по задачам.
    </li><li>Якщо використовується неоднорідна суміш машин з різними характеристиками продуктивності, обов'язково використовуйте який-небудь тип інструмента аналізу продуктивності для виявлення будь-якого дисбалансу навантаження. Відрегулюйте роботу відповідно.
    </li></ul><p>
</p></li><li><b>Використовуйте динамічне робоче завдання</b>
    <ul>
    <li>Деякі класи проблем призводять до дисбалансу навантаження, навіть якщо дані рівномірно розподілені між завданнями:
<p>
<table border="0" cellspacing="0" cellpadding="5" width="90%">
<tbody><tr valign="top">
<td width="33%"><img src="./Introduction to Parallel Computing_files/sparseMatrix.gif" height="250"></td>
<td width="33%"><img src="./Introduction to Parallel Computing_files/adaptiveGrid.jpg" height="250"></td>
<td width="33%"><img src="./Introduction to Parallel Computing_files/n-body.jpg" height="230" vspace="10"></td>
</tr><tr valign="top">
<td>Розгалужені масиви - деякі завдання матимуть фактичні дані для роботи, тоді як інші мають в основному «нулі».</td>
<td>Методи адаптивної сітки - для деяких завдань може знадобитися поліпшити їх сітку, а інших - ні.</td>
<td><i>N</i>-body simulations - частинки можуть мігрувати між доменами задач, що потребують більшої роботи для виконання деяких завдань.
</td>
</tr></tbody></table>
</p><p>
    </p></li><li>Коли обсяг роботи кожної задачі виконуватиметься навмисно змінено або неможливо передбачити, може бути корисно застосувати підхід до набору <b><i>планувальника</i></b>. Коли кожне завдання закінчує свою роботу, він отримує нову частину з черги роботи.
<p>
<img src="./Introduction to Parallel Computing_files/schedulerTaskPool.gif">
</p><p>
    </p></li><li>Зрештою, може стати необхідним розробляти алгоритм, який визначає і управляє дисбалансом навантаження, оскільки він відбувається динамічно в межах коду.
    </li></ul>
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignGranularity"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>Гранулярність</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Обчислення / коефіцієнт зв'язку:</span>
<ul>
<p>
</p><li>При паралельних обчисленнях деталізація є якісною мірою співвідношення обчислень до комунікації.
<p>
</p></li><li>Періоди обчислення, як правило, відокремлюються від періодів спілкування по подій синхронізації.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Дрібнозернисті паралелізм:</span>
</p><p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td> <ul> 
     <p>
     </p><li>Порівняно невеликі обсяги обчислювальних робіт виконуються між подій зв'язку 
     <p>
     </p></li><li>Низькі обчислення до співвідношення зв'язку 
     <p>
     </p></li><li>Сприяє збалансуванню навантаження
     <p>
     </p></li><li>Мається на увазі високі комунікаційні витрати та менша можливість для підвищення продуктивності
     <p>
     </p></li><li>Якщо деталізація занадто добре, можливо, що накладні витрати, необхідні для зв'язку та синхронізації між завданнями, тривають довше, ніж обчислення. 
     </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Паралелізм грубозернистості:</span>
     </p><ul>
     <p>
     </p><li>Відносно великі обсяги обчислювальних робіт виконуються між подіями зв'язку / синхронізації
     <p>
     </p></li><li>Високі обчислення до співвідношення зв'язку
     <p>
     </p></li><li>Мається на увазі більша можливість для підвищення продуктивності
     <p>
     </p></li><li>Важче ефективно завантажувати баланс
     </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Який найкращий?</span>
</p><ul>
<p>
</p><li>Найбільш ефективна грануляція залежить від алгоритму та апаратного середовища, в якому він працює.
<p>
</p></li><li>У більшості випадків накладні витрати, пов'язані з комунікацією та синхронізацією, є високими відносно швидкості виконання, тому перевага полягає в тому, щоб груба гранулярність була.
<p>
</p></li><li>Точний паралелізм може допомогти зменшити накладні витрати через незбалансованість навантаження.
</li></ul></td>
<td><img src="./Introduction to Parallel Computing_files/granularity2.gif" align="right" hspace="20">
<img src="./Introduction to Parallel Computing_files/granularity3.gif" align="right" hspace="20" vspace="30"></td>
</tr></tbody></table>

<!--========================================================================-->

<a name="DesignIO"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
</p><h2>I/O</h2>

<img src="./Introduction to Parallel Computing_files/memoryAccessTimes.gif" width="555" height="258" align="right" hspace="25">
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Погані новини:</span>
<p>
</p><ul>
<li>Операції введення-виведення, як правило, розглядаються як інгібітори паралелізму.
<p>
</p></li><li>Операції введення-виведення потребують набагато більше часу, ніж операції пам'яті.
<p>
</p></li><li>Паралельні системи вводу-виводу можуть бути незрілими або недоступними для всіх платформ.
<p>
</p></li><li>У середовищі, де всі завдання мають однакове розміщення файлів, операції запису можуть призвести до перезапису файлів.
<p>
</p></li><li>Функція читання може впливати на спроможність файлового сервера обробляти декілька запитів для читання одночасно.
<p>
</p></li><li>Введення / виводу, яке повинно проводитись через мережу (NFS, не локальне), може спричинити серйозні вузькі місця та навіть збій файлових серверів.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Добрі новини:</span>
</p><ul>
<p>
</p><li>Паралельні файлові системи доступні. Наприклад:
    <ul>
    <li>GPFS: загальна паралельна файлова система (IBM). Тепер називається спектромова шкала IBM.
    </li><li>Lustre: для кластерів Linux (Intel)
    </li><li>HDFS: розподілена файлова система Hadoop (Apache)
    </li><li>PanFS: Panasas ActiveScale файлова система для кластерів Linux (Panasas, Inc.)
    </li><li>І ще - див.      
<a href="http://en.wikipedia.org/wiki/List_of_file_systems#Distributed_parallel_file_systems" target="_blank">http://en.wikipedia.org/wiki/List_of_file_systems#Distributed_parallel_file_systems</a>
    </li></ul>
<p>
</p></li><li>Специфікація інтерфейсу програмування для паралельного введення / виводу для MPI була доступна з 1996 року як частина MPI-2. Постачальники та "вільні" реалізації в даний час широко доступні.
<p>
</p></li><li>Кілька вказівників:
    <ul>
    <p>
    </p><li>Правило №1: максимально зменшити загальне введення / виведення
    <p>
    </p></li><li>Якщо у вас є доступ до паралельної файлової системи, використовуйте її.
    <p>
    </p></li><li>Запис великих шматків даних, а не невеликих шматочків зазвичай значно ефективніший.
    <p>
    </p></li><li>Менше, більші файли виконують краще багатьох малих файлів.
    <p>
    </p></li><li>Обмежте введення / виводу певні послідовні частини завдання, а потім використовуйте паралельне з'єднання для розповсюдження даних для паралельних завдань. Наприклад, Завдання 1 може читати вхідний файл, а потім передавати необхідні дані до інших завдань. Крім того, Завдання 1 могла виконувати операцію запису після отримання необхідних даних з усіх інших завдань.
    <p>
    </p></li><li>Сукупні операції вводу-виводу в різних задачах - замість того, щоб виконувати багато завдань введення / виводу, вони виконують декілька завдань.
     </li></ul>
</li></ul>

<!--========================================================================-->

<a name="DesignDebug"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
<h2>Налагодження</h2>

<ul>
<p>
</p><li>Налагодження паралельних кодів може бути надзвичайно важким, особливо, коли коди масштабуються вгору.
<p>
</p></li><li>Хороша новина полягає в тому, що є кілька чудових відладчиків, які допомагають:
    <ul>
    <li>Threaded -  pthreads та OpenMP
    </li><li>MPI
    </li><li>GPU / акселератор
    </li><li>Hybrid
    </li></ul>
<p>
</p></li><li>Користувачі Livermore Computing мають доступ до кількох паралельних налагоджувальних засобів, встановлених на кластері LC:
    <ul>
    <li>TotalView від RogueWave Software
    </li><li>DDT від Allinea
    </li><li>Інспектор від Intel
    </li><li>Інструмент аналізу трасування стеку (STAT) - локально розроблений
    </li></ul>
<p>
</p></li><li>Всі ці інструменти мають навчальну криву, пов'язану з ними - трохи більше, ніж інші.
<p>
</p></li><li>Докладнішу інформацію та інформацію про початок роботи див.
    <ul>
    <li>Веб-сторінки LC за адресою <a href="https://hpc.llnl.gov/software/development-environment-software" target="_blank">
https://hpc.llnl.gov/software/development-environment-software</a>
    </li><li>Підручник з TotalView:  <a href="https://computing.llnl.gov/tutorials/totalview/" target="_blank">https://computing.llnl.gov/tutorials/totalview/</a>
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/debug1.gif" width="1000" height="509">
</p></li></ul>

<!--========================================================================-->

<a name="DesignPerformance"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Розробка паралельних програм</span></td>
</tr></tbody></table>
<h2>Аналіз продуктивності та настройка</h2>

<ul>
<p>
</p><li>Як і при налагодженні, аналіз і настройка роботи паралельної програми може бути набагато складнішим, ніж для серійних програм.
<p>
</p></li><li>На щастя, існує цілий ряд відмінних інструментів для паралельного аналізу продуктивності та налаштування.
<p>
</p></li><li>Користувачі Livermore Computing мають доступ до кількох таких інструментів, більшість з яких доступні для всіх виробничих кластерів.
<p>
</p></li><li>Деякі вихідні точки для інструментів, встановлених на систем LC:
    <ul>
    <li>Веб-сторінки LC за адресою <a href="https://hpc.llnl.gov/software/development-environment-software" target="_blank">
https://hpc.llnl.gov/software/development-environment-software</a>
    </li><li>TAU:
        <a href="http://www.cs.uoregon.edu/research/tau/docs.php" target="_blank">http://www.cs.uoregon.edu/research/tau/docs.php</a>
    </li><li>HPCToolkit:
        <a href="http://hpctoolkit.org/documentation.html" target="_blank">http://hpctoolkit.org/documentation.html</a>
    </li><li>Open|Speedshop:
        <a href="http://www.openspeedshop.org/" target="_blank">http://www.openspeedshop.org/</a>
    </li><li>Vampir / Vampirtrace:
        <a href="http://vampir.eu/" target="_blank">http://vampir.eu/</a>
    </li><li>Valgrind:
        <a href="http://valgrind.org/" target="_blank">http://valgrind.org/</a>
    </li><li>PAPI:
        <a href="http://icl.cs.utk.edu/papi/" target="_blank">http://icl.cs.utk.edu/papi/</a>
    </li><li>mpitrace
        <a href="https://computing.llnl.gov/tutorials/bgq/index.html#mpitrace" target="_blank">
        https://computing.llnl.gov/tutorials/bgq/index.html#mpitrace</a>
    </li><li>mpiP:
        <a href="http://mpip.sourceforge.net/" target="_blank">http://mpip.sourceforge.net/</a>
    </li><li>memP:
        <a href="http://memp.sourceforge.net/" target="_blank">http://memp.sourceforge.net/</a>
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/perfAnalysis.jpg" width="981" height="627">
</p></li></ul>

<!---------------------------- Removed 5/7/14 -------------------------------------
    <LI>A dated, but potentially useful LC whitepaper on the subject of "High Performance Tools and Technologies" describes a large number of tools, and a number of performance related topics applicable to code developers. Find it at:
<A HREF=../performance_tools/HighPerformanceToolsTechnologiesLC.pdf TARGET=toolspaper>computing.llnl.gov/tutorials/performance_tools/HighPerformanceToolsTechnologiesLC.pdf</A>.
    <LI><A HREF=../performance_tools TARGET=W2>Performance 
    Analysis Tools Tutorial</A>
------------------------------ Removed 5/7/14 ------------------------------------->


<!--========================================================================-->

<a name="Examples"> <br><br> </a><a name="ExamplesArray"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні приклади</span></td>
</tr></tbody></table>
<h2>Обробка масивів</h2>

<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td>
<ul>
<p>
</p><li>Цей приклад демонструє розрахунки на 2-мірних елементах масиву; функція оцінюється на кожен елемент масиву.
<p>
</p></li><li>Обчислення на кожен елемент масиву незалежно від інших елементів масиву.
<p>
</p></li><li>Проблема є обчислювальною інтенсивністю.
<p>
</p></li><li>Серійна програма розраховує один елемент одночасно в послідовному порядку.
<p>
</p></li><li>Серійний код може мати форму:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
do j = 1,n
  do i = 1,n
    a(i,j) = fcn(i,j)
  end do
end do

</b></pre>
</td></tr></tbody></table>
</p><p>
</p></li><li>Питання для запитання:
<ul>
<li>Чи можна розпаралелювати цю проблему?
</li><li>Як буде розбита проблема?
</li><li>Чи потрібні комунікації?
</li><li>Чи існують які-небудь дані залежностей?
</li><li>Чи потрібна синхронізація?
</li><li>Чи буде балансування навантаження бути проблемою?
</li></ul>
</li></ul>
</td>
<td>
<img src="./Introduction to Parallel Computing_files/array_proc1.gif" width="297" height="369" hspace="20">
</td>
</tr></tbody></table>


<p></p><hr><p>
</p><h2>Паралельне рішення <br>обробки масивів 1</h2>

<ul>
<p>
<img src="./Introduction to Parallel Computing_files/array_proc2.gif" width="297" height="247" hspace="20" align="right">
</p><li>Розрахунок елементів незалежно один від одного - призводить до незрозуміло паралельного рішення.
<p>
</p></li><li>Елементи масиву рівномірно розподілені таким чином, що кожен процес володіє частиною масиву (subarray).
<ul>
<p>
</p><li>Схема розподілу вибирається для ефективного доступу до пам'яті; наприклад, одиничний кроком (кроком 1) через сусіди. Stride одиниці максимізує використання кеша / пам'яті.
<p>
</p></li><li>Оскільки бажано пройти одиницю через субари, вибір схеми розподілу залежить від мови програмування. Див. <a href="https://computing.llnl.gov/tutorials/parallel_comp/#distributions"> Діаграму блок-циклічних розподілів</a>
    для параметрів.
</li></ul>
<p>
</p></li><li>Незалежний розрахунок елементів масиву забезпечує відсутність необхідності спілкування або синхронізації завдань.
<p>
</p></li><li>Оскільки обсяг робіт рівномірно розподіляється між процесами, не повинно бути проблем з навантаженням.
<p>
</p></li><li>Після розповсюдження масиву кожне завдання виконує частину циклу, відповідну до даних, якими вона володіє. 
Наприклад, показано розподіл блоків як Fortran (column-major), так і C (row-major).
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<td width="50%"><pre><b>
do j = <font color="red">mystart, myend</font>
  do i = 1, n
    a(i,j) = fcn(i,j)
  end do
end do

</b></pre></td>
<td width="50%"><pre><b>
for i (i = <font color="red">mystart</font>; i &lt; <font color="red">myend</font>; i++) {
  for j (j = 0; j &lt; n; j++) {
    a(i,j) = fcn(i,j);
    }
  }

</b></pre></td>
</tr></tbody></table>
</p><p>
</p></li><li>Зверніть увагу, що лише змінні зовнішнього циклу відрізняються від серійного рішення.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Одне з можливих рішень:</span>
</p><ul>
<p>
</p><li>Впровадити в якості моделі єдиної програми кількох даних (SPMD) - кожне завдання виконує ту ж програму.
<p>
</p></li><li>Майстер-процес ініціалізує масив, передає інформацію робочим процесам і отримує результати.
<p>
</p></li><li>Робочий процес отримує інформацію, виконує свою частку обчислень і відправляє результати для оволодіння.
<p>
</p></li><li>Використовуючи схему зберігання Fortran, виконуйте блоковий розподіл масиву.
<p>
</p></li><li>Псевдокодове рішення:
    <b><font color="red">червоний</font></b> виділяє зміни для паралелізму.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out if I am MASTER or WORKER
   
if I am MASTER
   
  initialize the array
  send each WORKER info on part of array it owns
  send each WORKER its portion of initial array
   
  receive from each WORKER results 
   
else if I am WORKER
  receive from MASTER info on part of array I own
  receive from MASTER my portion of initial array
</font>
  # calculate my portion of array
  do j = <font color="red">my first column,my last column </font>
    do i = 1,n
      a(i,j) = fcn(i,j)
    end do
  end do
<font color="red">
  send MASTER results 

endif
</font></b></pre>
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Приклади програм:</span>
</p><ul>
<li>Програма MPI в C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_array.c" onclick="popUp(&#39;../mpi/samples/C/mpi_array.c&#39;)"></b></font>
<p>
</p></li><li>Програма MPI в Фортран: &nbsp;
<font size="-1"><b><input type="button" value="mpi_array.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_array.f&#39;)"></b></font>
</li></ul>

<p></p><hr><p>
</p><h2>Parallel Solution 2:<br>обробка масивів завдань</h2>

<ul>
<p>
</p><li>У попередньому рішенні масиву показано статичне балансування навантаження: 
    <ul>
    <li>Кожне завдання має виконувати фіксовану кількість робіт 
    </li><li>Може бути значним часом простою для більш швидких або більш легко завантажених процесорів - найповільніші завдання визначають загальну продуктивність.
    </li></ul>
<p>
</p></li><li>Статичне балансування навантаження зазвичай не є основним завданням, якщо всі завдання виконують однакову кількість роботи на однакових машинах. 
<p>
</p></li><li>Якщо у вас є проблема балансу навантаження (деякі завдання працюють швидше, ніж інші), ви можете скористатися схемою "пул завдань".
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Схема пулу завдань:</span>
</p><p>
</p><ul>
<p>
</p><li>Використовуються два процеси
<p>
Магістерський процес: 
    </p><ul type="circle">
    <li>Тримає пул завдань для працівників процесів зробити
    </li><li>Відправляє працівника завдання за запитом
    </li><li>Збирає результати працівників
    </li></ul>
<p>
Робочий процес: багаторазово робить наступне
    </p><ul type="circle">
    <li>Отримує завдання від майстер-процесу
    </li><li>Виконає обчислення
    </li><li>Відправляє результати майстеру
    </li></ul>
<p>
</p></li><li>Робочі процеси не знають перед виконанням, яку частину масиву вони оброблятимуть, або скільки завдань вони виконають.
<p>
</p></li><li>Динамічне завантаження балансу відбувається під час виконання: швидше завданням буде потрібно більше роботи.    
<p>
</p></li><li>Псевдокодове рішення:
    <font color="red"><b>червоний</b></font> виділяє зміни для паралелізму.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out if I am MASTER or WORKER

if I am MASTER

  do until no more jobs
    if request send to WORKER next job
    else receive results from WORKER
  end do

else if I am WORKER

  do until no more jobs
    request job from MASTER
    receive from MASTER next job
</font>
    calculate array element: a(i,j) = fcn(i,j)
<font color="red">
    send results to MASTER
  end do

endif
</font>
</b></pre>
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Обговорення:</span>
</p><ul>
<p>
</p><li>Наприклад, у наведеному вище пулі завдань кожне завдання розрахувало окремий елемент масиву як завдання. Обчислення до коефіцієнта зв'язку є дрібно гранульованим.
<p>
</p></li><li>Дрібно гранульовані рішення потребують додаткових витрат на зв'язок, щоб зменшити час простою роботи.
<p>
</p></li><li>Більш оптимальним рішенням може бути поширення більше роботи з кожною роботою. "Правильний" обсяг роботи залежить від проблеми.
</li></ul>


<!--========================================================================-->

<a name="ExamplesPI"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні приклади</span></td>
</tr></tbody></table>
<h2>Розрахунок PI</h2> 

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td>
<ul>
<li>Значення PI може бути розраховане різними способами. Розглянемо метод Монте-Карло для наближення PI:
<ul> 
<li>Вставте колі з радіусом <b>r</b> в квадрат із довжиною сторони <b>2<i>p</i></b> 
</li><li>Площа окружності становить <b>Πr<sup>2</sup></b> площа квадрата <b>4r<sup>2</sup></b> 
</li><li>Співвідношення площі кола до площі квадрата становить: <br><b>Πr<sup>2</sup> / 4r<sup>2</sup> = Π / 4</b> 
</li><li>Якщо ви довільно генеруєте <b>N</b> балів усередині квадрата, приблизно <br><b>N * Π / 4</b> цих точок (<b>M</b>) повинно потрапляти всередину кола. </li><li><b>Π</b> потім аппроксиміруется як:  
<br><b>N * Π / 4 = M 
<br>Π / 4 = M / N 
<br>Π = 4 * M / N</b> 
</li><li>Зверніть увагу, що збільшення кількості отриманих балів покращує наближення.
</li></ul>
<p>
</p></li><li>Послідовний псевдокод для цієї процедури:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
npoints = 10000
circle_count = 0

do j = 1,npoints
  generate 2 random numbers between 0 and 1
  xcoordinate = random1
  ycoordinate = random2
  if (xcoordinate, ycoordinate) inside circle
  then circle_count = circle_count + 1
end do

PI = 4.0*circle_count/npoints

</b></pre>
</td></tr></tbody></table>


</p><p>
</p></li><li>Проблема є обчислювальною інтенсивністю - більша частина часу витрачається на виконання циклу
<p>
</p></li><li>Питання для запитання:
<ul>
<li>Чи можна розпаралелювати цю проблему?
</li><li>Як буде розбита проблема?
</li><li>Чи потрібні комунікації?
</li><li>Чи існують які-небудь дані залежностей?
</li><li>Чи потрібна синхронізація?
</li><li>Чи буде балансування навантаження бути проблемою?
</li></ul>
</li></ul>
</td>
<td>
<img src="./Introduction to Parallel Computing_files/pi1.gif" width="400" height="517" hspace="20">
</td></tr></tbody></table>


<p></p><hr><p>
</p><h2>PI розрахунок<br>паралельних рішень</h2>

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td>
<ul>
<p>
</p><li>Інша проблема, яку легко розпаралелювати:
<ul>
<li>Всі точні розрахунки незалежні; немає залежностей даних
</li><li>Робота може бути розподілена рівномірно; немає проблем із завантаженням вантажу
</li><li>Немає необхідності спілкування або синхронізації між завданнями
</li></ul>
<p>
</p></li><li>Паралельна стратегія:
<ul>
<li>Розділіть цикл на рівних частинах, які можуть бути виконані пулом завдань
</li><li>Кожне завдання самостійно виконує свою роботу
</li><li>Використовується модель SPMD
</li><li>Одне завдання виконує роль майстра для збору результатів та обчислення значення PI
</li></ul>
<p>
</p></li><li>Псевдокодове рішення:
    <font color="red"><b>червоний</b></font> виділяє зміни для паралелізму.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
npoints = 10000
circle_count = 0
<font color="red">
p = number of tasks
num = npoints/p

find out if I am MASTER or WORKER </font>

do j = 1,<font color="red">num </font>
  generate 2 random numbers between 0 and 1
  xcoordinate = random1
  ycoordinate = random2
  if (xcoordinate, ycoordinate) inside circle
  then circle_count = circle_count + 1
end do
<font color="red">
if I am MASTER

  receive from WORKERS their circle_counts
  compute PI (use MASTER and WORKER calculations)

else if I am WORKER

  send to MASTER circle_count

endif
</font>
</b></pre>
</td></tr></tbody></table>
</p></li></ul>
</td>
<td>
<img src="./Introduction to Parallel Computing_files/pi2.gif" width="400" height="475" hspace="20">
</td></tr></tbody></table>

<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Приклади програм:</span>
</p><ul>
<li>Програма MPI в C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_pi_reduce.c" onclick="popUp(&#39;../mpi/samples/C/mpi_pi_reduce.c&#39;)"></b></font>
<p>
</p></li><li>Програма MPI в Фортран: &nbsp;
<font size="-1"><b><input type="button" value="mpi_pi_reduce.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_pi_reduce.f&#39;)"></b></font>
</li></ul>

<!--========================================================================-->

<a name="ExamplesHeat"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні приклади</span></td>
</tr></tbody></table>
<h2>Simple Heat Equation</h2>

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
<p>
</p><li>Більшість проблем паралельних обчислень вимагають спілкування між завданнями. Ряд спільних проблем вимагає спілкування з "сусідами" завдань.
<p>
</p></li><li>2-D рівняння теплопровідності описує зміну температури з плином часу, з урахуванням початкового розподілу температури та граничних умов.
<p>
</p></li><li>Для розв'язання рівняння теплопути на квадратній ділянці застосовується скінченна диференційована схема.
<ul>
<li>Елементи 2-мірного масиву представляють температуру в точках на квадраті.
</li><li>Початкова температура дорівнює нулю на кордонах і висока в середині.
</li><li>Гранична температура зберігається в нулі.
</li><li>Використовується алгоритм з часом.
</li></ul>
<p>
</p></li><li>Розрахунок елемента <b><i>залежить</i></b> від значень сусідніх елементів:
    values:
<p>
<img src="./Introduction to Parallel Computing_files/heat_equation2.gif" width="276" height="114">
</p><p>
</p></li><li>Серійна програма міститиме код, такий як:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
do iy = 2, ny - 1
  do ix = 2, nx - 1
    u2(ix, iy) =  u1(ix, iy)  +
        cx * (u1(ix+1,iy) + u1(ix-1,iy) - 2.*u1(ix,iy)) +
        cy * (u1(ix,iy+1) + u1(ix,iy-1) - 2.*u1(ix,iy))
  end do
end do
</b></pre>
</td></tr></tbody></table>
</p><p>
</p></li><li>Питання для запитання:
<ul>
<li>Чи можна розпаралелювати цю проблему?
</li><li>Як буде розбита проблема?
</li><li>Чи потрібні комунікації?
</li><li>Чи існують які-небудь дані залежностей?
</li><li>Чи потрібна синхронізація?
</li><li>Чи буде балансування навантаження бути проблемою? 
</li></ul>
</li></ul></td>

<td><img src="./Introduction to Parallel Computing_files/heat_initial.gif" width="300" height="301" hspace="20">

<img src="./Introduction to Parallel Computing_files/heat_equation.gif" width="261" height="258" border="0" hspace="20" vspace="20" alt="Heat equation">
</td></tr></tbody></table>

<p></p><hr><p>
</p><h2>Просте паралельне рішення<br>рівняння тепла</h2>
<p>

<img src="./Introduction to Parallel Computing_files/heat_partitioned.gif" width="300" height="301" align="right" hspace="20">
</p><ul>
<p>
</p><li>Ця проблема є складнішою, оскільки існує залежність даних, яка вимагає зв'язку та синхронізації.
<p>
</p></li><li>Весь масив розподіляється і розподіляється як сумарний для всіх завдань. Кожне завдання володіє рівною частиною всього масиву.
<p>
</p></li><li>Оскільки обсяг роботи однаковий, балансування навантаження не повинно бути проблемою
<p>
</p></li><li>Визначення залежностей даних:
    <ul>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/images/heat_interior.gif" target="W6">елементи інтер'єру</a> що належать до завдання, не залежать від інших завдань
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/images/heat_edge.gif" target="W7">прикордонні елементи</a> залежать від даних сусіда, що вимагають зв'язку.
    </li></ul>
<p>
</p></li><li>Впровадити як модель SPMD:
<ul>
<li>Майстерський процес надсилає первинну інформацію працівникам, а потім чекає, щоб збирати результати від усіх працівників
</li><li>Робочі процеси розраховують рішення в задану кількість етапів часу, спілкуючись, коли це необхідно, з сусідніми процесами
</li></ul>
<p>
</p></li><li>Псевдокодове рішення:
    <font color="red"><b>червоний</b></font> виділяє зміни для паралелізму.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out if I am MASTER or WORKER

if I am MASTER
  initialize array
  send each WORKER starting info and subarray
  receive results from each WORKER

else if I am WORKER
  receive from MASTER starting info and subarray
</font>
  # Perform time steps
  do t = 1, nsteps
    update time <font color="red">
    send neighbors my border info
    receive from neighbors their border info </font>
    update my portion of solution array
    
  end do
  <font color="red">
  send MASTER results
      
endif
</font>
</b></pre>
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Приклади програм:</span>
</p><ul>
<li>Програма MPI в C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_heat2D.c" onclick="popUp(&#39;../mpi/samples/C/mpi_heat2D.c&#39;)"></b></font>
<p>
</p></li><li>Програма MPI в Фортран: &nbsp;
<font size="-1"><b><input type="button" value="mpi_heat2D.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_heat2D.f&#39;)"></b></font>
</li></ul>

<!-----------------------------------------------------------------------

<P><HR><P> 
<H2>Simple Heat Equation<BR>
Parallel Solution 2: Overlapping Communication and Computation</H2>

<UL>
<P>
<LI>In the previous solution, it was assumed that blocking communications
    were used by the worker tasks.  Blocking communications wait for the 
    communication process to complete before continuing to the next 
    program instruction.
<P>
<LI>In the previous solution, neighbor tasks communicated border
    data, then each process updated its portion of the array.
<P>
<LI>Computing times can often be reduced by using non-blocking
    communication.  Non-blocking communications allow work to be performed 
    while communication is in progress.
<P>
<LI>Each task could update the interior of its part of the solution
    array while the communication of border data is occurring, and
    update its border after communication has completed.
<P>
<LI>Pseudo code for the second solution:
    <FONT COLOR=red><B>red</B></FONT COLOR> highlights changes for 
    non-blocking communications.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0>
<TR><TD><PRE><B>
find out if I am MASTER or WORKER
 
if I am MASTER
  initialize array
  send each WORKER starting info and subarray
    
  do until all WORKERS converge
    gather from all WORKERS convergence data
    broadcast to all WORKERS convergence signal
  end do
 
  receive results from each WORKER
 
else if I am WORKER
  receive from MASTER starting info and subarray
 
  do until solution converged
    update time
    <FONT COLOR=red>
    non-blocking send neighbors my border info
    non-blocking receive neighbors border info

    update interior of my portion of solution array
    wait for non-blocking communication complete
    update border of my portion of solution array
    </FONT>
    determine if my solution has converged
      send MASTER convergence data
      receive from MASTER convergence signal
  end do
  
  send MASTER results
       
endif

</B></PRE>
</TD></TR></TABLE>
</UL>
------------------------------------------------------------------------>

<!--========================================================================-->

<a name="ExamplesWave"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні приклади</span></td>
</tr></tbody></table>
<h2>1-D хвильове рівняння</h2>

<ul>
<p>
</p><li>У цьому прикладі амплітуда вздовж рівномірної вібраційної струни обчислюється після того, як пройшло певну кількість часу.
<p>
</p></li><li>Розрахунок передбачає:
    <ul>
    <li>амплітуда на осі y
    </li><li>i як індекс позиції вздовж осі x
    </li><li>вузлові точки, накладені вздовж струни
    </li><li>оновлення амплітуди на дискретних етапах часу.
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/wave3.gif">
</p><p>
</p></li><li>Рівняння, яке потрібно вирішити, - одномірне хвильове рівняння:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<td><pre><span style="white-space: nowrap"><b>
    A(i,t+1) = (2.0 * A(i,t)) - A(i,t-1) 
        + (c * (A(i-1,t) - (2.0 * A(i,t)) + A(i+1,t))) 
</b></span></pre></td>
</tr></tbody></table>
</p><p>
де c - константа
</p><p>
</p></li><li>Зверніть увагу, що амплітуда буде залежати від попередніх кроків (t, t-1) та сусідніх точок (i-1, i + 1).  
<p>
</p></li><li>Питання для запитання:
<ul>
<li>Чи можна розпаралелювати цю проблему?
</li><li>Як буде розбита проблема?
</li><li>Чи потрібні комунікації?
</li><li>Чи існують які-небудь дані залежностей?
</li><li>Чи потрібна синхронізація?
</li><li>Чи буде балансування навантаження бути проблемою?
</li></ul>
</li></ul>

<p></p><hr><p> 
</p><h2>Паралельне рішення<br>1-D хвильового рівняння</h2>

<ul>
<p>
</p><li>Це ще один приклад проблеми із залежністю даних. Паралельне рішення передбачає зв'язок та синхронізацію.
<p>
</p></li><li>Весь амплітудний масив розподіляється і розподіляється як субарні для всіх завдань. Кожне завдання володіє рівною частиною всього масиву.
<p>
</p></li><li>Балансування навантаження: всі пункти вимагають рівної роботи, тому точки повинні бути розділені рівномірно
<p>
</p></li><li>Розбиття на блок буде розбивати роботу на кількість завдань у вигляді фрагментів, що дозволяє кожному завданням володіти переважно сусідніми точками даних.
<p>
</p></li><li>Зв'язок має відбуватися тільки на межі даних. Чим більше розмір блоку, тим менше спілкування. 
<p>
<img src="./Introduction to Parallel Computing_files/wave4.gif">

</p><p>
</p></li><li>Впровадити як модель SPMD:
<ul>
<li>Майстерський процес надсилає первинну інформацію працівникам, а потім чекає, щоб збирати результати від усіх працівників
</li><li>Робочі процеси розраховують рішення в задану кількість етапів часу, спілкуючись, коли це необхідно, з сусідніми процесами
</li></ul>
<p>
</p></li><li>Псевдокодове рішення:
    <font color="red"><b>червоний</b></font> виділяє зміни для паралелізму.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out number of tasks and task identities

#Identify left and right neighbors
left_neighbor = mytaskid - 1
right_neighbor = mytaskid +1
if mytaskid = first then left_neigbor = last
if mytaskid = last then right_neighbor = first

find out if I am MASTER or WORKER
if I am MASTER
  initialize array
  send each WORKER starting info and subarray
else if I am WORKER`
  receive starting info and subarray from MASTER
endif
</font>
#Perform time steps <font color="red">
#In this example the master participates in calculations</font>
do t = 1, nsteps <font color="red">
  send left endpoint to left neighbor
  receive left endpoint from right neighbor
  send right endpoint to right neighbor
  receive right endpoint from left neighbor
</font>
  #Update points along line
  do i = 1, npoints
    newval(i) = (2.0 * values(i)) - oldval(i) 
    + (sqtau * (values(i-1) - (2.0 * values(i)) + values(i+1))) 
  end do

end do
<font color="red">
#Collect results and write to file
if I am MASTER
  receive results from each WORKER
  write results to file
else if I am WORKER
  send results to MASTER
endif </font>

</b></pre></td></tr></tbody></table>
</p><p>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Приклади програм:</span>
</p><ul>
<li>Програма MPI в C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_wave.c" onclick="popUp(&#39;../mpi/samples/C/mpi_wave.c&#39;)"></b></font>
<p>
</p></li><li>Програма MPI в Фортран: &nbsp;
<font size="-1"><b><input type="button" value="mpi_wave.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_wave.f&#39;)"></b></font>
</li></ul>


<br><br>
<p></p><hr><p>

<b>Це завершує навчальний посібник.</b>
</p><p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><a href="https://computing.llnl.gov/tutorials/evaluation/index.html" target="evalForm">
    <img src="./Introduction to Parallel Computing_files/evaluationForm.gif"></a> &nbsp; &nbsp; &nbsp;</td>
<td>Будь ласка, заповніть онлайн-форму для оцінки. </td>
</tr>
</tbody></table>
</p><p>
<b>Куди ти хочеш піти зараз?</b>
</p><ul>
<li><a href="https://computing.llnl.gov/tutorials/agenda/index.html">Порядок денний</a>
</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#top">Перейти до початку сторінки</a>
</li></ul>

<!--========================================================================-->

<a name="References"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Довідники та додаткова інформація</span></td>
</tr></tbody></table>

<ul>
<li>Автор: <a href="mailto:blaiseb@llnl.gov">Блейз Барні</a>,  Лівермор Комп'ютинг.
<p>
</p></li><li>Пошук у WWW для "паралельного програмування" або "паралельні обчислення" дасть широкий спектр інформації.
<p>
</p></li><li>Рекомендоване читання:
    <ul>
    <li>Проектування та побудова паралельних програм". Ян Фостер. 
    <br><a href="http://www.mcs.anl.gov/~itf/dbpp/" target="_blank">
        http://www.mcs.anl.gov/~itf/dbpp/</a>
    </li><li>"Вступ до паралельних обчислень". Анант Грэма, Аншюль Гупта, Джордж Карипіс, Віпін Кумар. 
    <br><a href="http://www-users.cs.umn.edu/~karypis/parbook/" target="_blank">
        http://www-users.cs.umn.edu/~karypis/parbook/</a>
    </li><li>"Огляд недавніх суперкомп'ютерів". А. Дж. Ван дер Стин, Джек Донгара. 
    <br><a href="https://computing.llnl.gov/tutorials/parallel_comp/OverviewRecentSupercomputers.2008.pdf" target="_blank">
        OverviewRecentSupercomputers.2008.pdf</a>
    </li></ul>
<p>
</p></li><li>Фото / графіка створено автором, створеною іншими співробітниками LLNL, отриманими з джерел, не захищених авторським правом, державними або загальнодоступними джерелами (наприклад, http://commons.wikimedia.org/), або використовуються з дозволу авторів з інші презентації та веб-сторінки.
<p>
</p></li><li>Історія: ці матеріали складаються з наступних джерел, деякі з яких більше не підтримуються або недоступні:
    <ul>
    <li>Навчальні посібники розроблені для "Майстер-класу паралельного програмування SP" у Мауї високопродуктивних обчислювальних центрах.
    </li><li>Навчальні посібники, розроблені Корнельським університетським центром передових обчислень (CAC), тепер доступні в якості віртуальних семінарів Cornell за адресою:
        <a href="https://cvw.cac.cornell.edu/topics" target="_blank">https://cvw.cac.cornell.edu/topics</a>.
    </li></ul>
</li></ul>

<!------------------------------------------------------------------------>

<script language="JavaScript">PrintFooter("UCRL-MI-133316")</script><p></p><hr><span class="footer">https://computing.llnl.gov/tutorials/parallel_comp/<br>Last Modified: 05/05/2018 01:54:23 <a href="mailto:blaiseb@llnl.gov">blaiseb@llnl.gov</a><br>UCRL-MI-133316<p>This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27345.

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>




</p></span></font><div class="dd-sttbtn dd-sttbtn--visible" data-stt-pos="br"></div></body></html>
