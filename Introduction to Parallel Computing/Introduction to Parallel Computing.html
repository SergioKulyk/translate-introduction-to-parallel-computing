
<!-- saved from url=(0051)https://computing.llnl.gov/tutorials/parallel_comp/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Вступ до паралельних обчислень</title>

<script language="JavaScript" src="./Introduction to Parallel Computing_files/tutorials.js.Без названия"></script>
<link rel="StyleSheet" href="./Introduction to Parallel Computing_files/tutorials.css" type="text/css">
<link rel="SHORTCUT ICON" href="http://www.llnl.gov/favicon.ico">

<!-- BEGIN META TAGS -->
<meta name="LLNLRandR" content="UCRL-MI-133316">
<meta name="distribution" content="global">
<meta name="description" content="Livermore Computing Training">
<meta name="rating" content="general">
<meta http-equiv="keywords" content="Lawrence Livermore
National Laboratory, LLNL, High Performance Computing, parallel, programming, 
HPC, training, workshops, tutorials, Blaise Barney">
<meta name="copyright" content="This document is copyrighted U.S.
Department of Energy">
<meta name="Author" content="Blaise Barney">
<meta name="email" content="blaiseb@llnl.gov">
<!-- END META TAGS -->
</head>

<body>
<basefont size="3">            <!-- default font size -->
<font face="arial">

<!-- Begin Piwik Tracking Code  -->
<script src="./Introduction to Parallel Computing_files/piwik.js.Без названия" type="text/javascript">
</script>
<script>
var siteName = 1-.domain;
var pkBaseURL = 'https://analytics.llnl.gov/';
if (typeof jQuery=="undefined") {
    document.write(unescape("%3Cscript src='" + pkBaseURL + "jquery.js' type='text/javascript'%3E%3C/script%3E"));
}
</script><script src="./Introduction to Parallel Computing_files/jquery.js.Без названия" type="text/javascript"></script>
<script>
    try {
        var LLNLTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 1);
        LLNLTracker.trackPageView();
        LLNLTracker.enableLinkTracking();
        var localSiteTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 149);
        localSiteTracker.trackPageView();
        localSiteTracker.enableLinkTracking();
    }
    catch (err) {
        console.log(err);
    }
</script><noscript><p><img src="https://analytics.llnl.gov/piwik.php?idsite=149" style="border:0" alt="" /></p></noscript>
<!-- End Piwik Tracking Code -->

<a name="top">  </a>
<table cellpadding="0" cellspacing="0" width="100%">
<tbody><tr><td colspan="2" bgcolor="#3F5098">
  <table cellpadding="0" cellspacing="0" width="900">
  <tbody><tr><td background="./Introduction to Parallel Computing_files/bg1.gif">
  <a name="top"> </a>
  <script language="JavaScript">addNavigation()</script>   <table border="0"><tbody><tr align="center" valign="center">    <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/training#training_materials">Навчальні посібники</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/exercises/index.html">Вправи</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/abstracts/index.html">Реферати</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/training#workshops">Магазин робіт</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/misc/comments.html">Коментарі</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/search/index.html">Пошук</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">   <a href="http://www.llnl.gov/disclaimer.html" target="W2">   Конфіденційність</a></font></td>   </tr></tbody></table>   
  <p><br>
  </p><h1>Вступ до паралельних обчислень</h1>
  <p>
  </p></td></tr></tbody></table>
</td>
</tr><tr valign="top">
<td><i>Автори: Блейз Барні, Національна лабораторія ім. Лоуренса Лівермора</i></td>
<td align="right"><font size="-1">UCRL-MI-133316</font></td>
</tr></tbody></table>
<p>

<a name="TOC"> </a>
</p><h2>Зміст</h2>
<ol>
<li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Abstract">Навчальні посібники</a>
</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Overview">Огляд</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Whatis">Що таке паралельні обчислення?</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#WhyUse">Навіщо використовувати паралельні обчислення?</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Who">Хто використовує паралельні обчислення?</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Concepts">Поняття та термінології</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Neumann">Архітектурою комп'ютерної системи Неймана</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Flynn">Класична таксономія Флінна</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Terminology">Деяка загальна паралельна термінологія</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#LimitsCosts">Межі та витрати паралельного програмування</a> 
</li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#MemoryArch">Паралельні архітектури пам'яті комп'ютера</a> 
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#SharedMemory">Спільна пам'ять</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DistributedMemory">Розподілена пам'ять</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#HybridMemory">Гібридна розподілена загальна пам'ять</a>
    </li></ol>
 
</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Models">Моделі паралельного програмування</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsOverview">Огляд</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsShared">Модель спільної пам'яті</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsThreads">Моделі ниток</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsMessage">Модель пропущеної пам'яті / повідомлення</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ModelsData">Паралельна модель даних</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Hybrid">Гібридна модель</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#SPMD-MPMD">SPMD та MPMP</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Designing">Розробка паралельних програм</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignAutomatic">Автоматична або паралелізація вручну</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignUnderstand">Зрозуміння проблем та програм</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignPartitioning">Розбиття</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignCommunications">Зв'язок</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignSynchronization">Синхронізація</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignDependencies">Залежності даних</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignLoadBalance">Балансування навантаження</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignGranularity">Гранулярність</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignIO">I/O</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignDebug">Налагодження</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignPerformance">Аналіз продуктивності та настройка</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Examples">Паралельні приклади</a> 
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesArray">Обробка масивів</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI">Розрахунок PI</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesHeat">Просте рівняння тепла</a> 
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesWave">1-D хвильове рівняння</a>
    </li></ol>

</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#References">Довідники та додаткова інформація</a>
</li></ol>
 
<!--========================================================================-->
 
<a name="Abstract"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Навчальні посібники</span></td>
</tr></tbody></table>
<p><br>
 
Це перший навчальний посібник у семінарі "Лівермор Комп'ютер". Він має на меті забезпечити лише дуже короткий огляд широкої та широкої теми паралельних обчислень, як провідний для підручників, які слідують за нею. Таким чином, він охоплює лише основи паралельних обчислень і призначений для того, хто просто хочу ознайомиться з предметом і хто планує відвідувати один або декілька інших підручників у цьому семінарі. Він не має на меті поширювати глибоке паралельне програмування, оскільки це вимагатиме значно більше часу. Навчальний посібник починається з обговорення паралельних обчислень - що це таке, як він використовується, а потім обговорення концепцій та термінів, пов'язаних з паралельними обчисленнями. Потім досліджуються теми архітектур паралельної пам'яті та моделей програмування. За цими темами йде низка практичних обговорень з ряду складних питань, пов'язаних із розробкою та виконанням паралельних програм. Підручник завершується декількома прикладами паралелізації простих серійних програм.
<br><br>

<!--========================================================================-->

<a name="Overview"> <br><br> </a>
<a name="Whatis"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Огляд</span></td>
</tr></tbody></table>
</p><h2>Що таке паралельні обчислення?</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Серійний обчислення:</span>
<ul>
<p>
</p><li>Традиційно для  <b><i>серійного</i></b>  обчислення було написано програмне забезпечення:
    <ul>
    <li>Проблема розбита на дискретну серію інструкцій
    </li><li>Інструкції виконуються послідовно один за одним
    </li><li>Виконано на одному процесорі
    </li><li>Тільки одна команда може виконуватись в будь-який момент часу
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/serialProblem.gif" width="604" height="250" border="1" alt="Serial computing">
</p><p>
<b>Наприклад:</b>
</p><p>
<img src="./Introduction to Parallel Computing_files/serialProblem2.gif" width="604" height="250" border="1" alt="Serial computing">
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Паралельні обчислення:</span>
</p><ul>
<li>У найпростішому сенсі, <b><i>паралельні обчислення</i></b> - це одночасне використання кількох обчислювальних ресурсів для вирішення обчислювальної задачі:
    <ul>
    <li>Проблема розбита на окремі частини, які можна вирішити одночасно
    </li><li>Кожна частина далі розбита на ряд інструкцій
    </li><li>Інструкції з кожної частини виконуються одночасно на різних процесорах
    </li><li>Використовується загальний механізм контролю / координації
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/parallelProblem.gif" width="683" height="372" border="1" alt="Parallel computing">
</p><p>
<b>Наприклад:</b>
</p><p>
<img src="./Introduction to Parallel Computing_files/parallelProblem2.gif" width="683" height="372" border="1" alt="Parallel computing">
</p><p>
</p></li><li>Обчислювальна проблема повинна бути в змозі:
    <ul>
    <li>Розбиті на окремі частини задачі, які можна вирішити одночасно;
    </li><li>Виконати декілька інструкцій програми в будь-який момент часу;
    </li><li>Вирішуватиметься в менший проміжок часу за допомогою кількох обчислювальних ресурсів, ніж з єдиним обчислювальним ресурсом.
    </li></ul>
<p>
</p></li><li>Обчислювальні ресурси зазвичай:
    <ul>
    <li>Єдиний комп'ютер з декількома процесорами / ядрами
    </li><li>Довільне число таких комп'ютерів, з'єднаних мережею
    </li></ul>
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Паралельні комп'ютери:</span>
</p><ul>
<p>
</p><li>Практично всі автономні комп'ютери сьогодні паралельні з апаратної перспективи:
    <ul>
    <li>Кілька функціональних одиниць (кеш L1, кеш-пам'ять L2, гілка, попередня вибірка, декодування, плаваюча кома, обробка графіки (графічне зображення), ціле число тощо).
    </li><li>Кілька одиниць виконання / ядер
    </li><li>Кілька апаратних ниток
    </li></ul>
<p>
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td align="center">
<img src="./Introduction to Parallel Computing_files/bgqComputeChip.jpg" width="450" heigth="453">
<br>IBM BG / Q Compute Chip з 18 ядер (PU) і 16 одиниць кеш-пам'яті L2 (L2)</td>
</tr></tbody></table>
</p><p>
</p></li><li>Мережі підключають декілька автономних комп'ютерів (вузлів) для створення більших паралельних комп'ютерних кластерів.
<p>
<img src="./Introduction to Parallel Computing_files/nodesNetwork.gif" width="720" heigth="249">
</p><p>
</p></li><li>Наприклад, на схемі нижче показано типовий паралельний кластер LLNL
    <ul>
    <li>Кожен обчислювальний вузол сам по собі є багатопроцесорним паралельним комп'ютером
    </li><li>Кілька обчислювальних вузлів мережа разом з мережею Infiniband
    </li><li>Вузли спеціального призначення, також багатопроцесорні, використовуються для інших цілей
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/parallelComputer1.gif" width="781" heigth="402">
</p><p>
</p></li><li>Більшість великих паралельних комп'ютерів у світі (суперкомп'ютерів) - це кластери апаратного забезпечення, створені декількома (в основному) відомими постачальниками.
<p>
<img src="./Introduction to Parallel Computing_files/top500Vendors.png">
<br><font size="-1"><i>Джерело: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font>
</p></li></ul>

<!--========================================================================-->

<a name="WhyUse"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Огляд</span></td>
</tr></tbody></table>
<h2>Навіщо використовувати паралельні обчислення?</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Реальний світ масово паралельно:</span>
<p>
<table border="0" cellspacing="0" cellpadding="0" width="800">
<tbody><tr valign="top">
<td>
<ul>
<li>У природному світі багато складних, взаємопов'язаних подій відбуваються одночасно, але всередині тимчасової послідовності.
<p>
</p></li><li>Порівняно з послідовним обчисленням, паралельні обчислення набагато краще підходять для моделювання, моделювання та розуміння складних, реальних явищ.
<p>
</p></li><li>Наприклад, уявімо собі, як моделювати їх послідовно:
<p>
<img src="./Introduction to Parallel Computing_files/realWorldCollage1.jpg" width="760" height="220">
<br><img src="./Introduction to Parallel Computing_files/realWorldCollage2.jpg" width="760" height="230">
<br><img src="./Introduction to Parallel Computing_files/realWorldCollage3.jpg" width="760" height="208">
</p></li></ul>
</td>
</tr></tbody></table>


<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Основні причини:</span>

</p><p>
<table border="0" cellpadding="0" cellspacing="0" width="700">
<tbody><tr valign="top">
<td><ul>
<li><b>Зберегти час і гроші:</b> 
    <ul>
    <li>Теоретично, викидаючи більше ресурсів на завдання, скорочується час до завершення, з можливістю економії коштів.
    </li><li>Паралельні комп'ютери можуть бути побудовані з дешевих, товарних компонентів.
    <p>
    <img src="./Introduction to Parallel Computing_files/timeMoney2.jpg" width="600" height="182">
    </p></li></ul>
<p>
</p></li><li><b>РОЗШИРЕННЯ ВЕЛИКИХ /  КОМПЛЕКСНИХ ПРОБЛЕМ:</b> 
    <ul>
    <li>Багато проблем настільки великі та / або складні, що непрактично або неможливо їх вирішити на одному комп'ютері, особливо з обмеженою комп'ютерною пам'яттю.
    </li><li>Приклад: "Проблеми великих викликів 
        (<a href="http://en.wikipedia.org/wiki/Grand_Challenge" target="_blank">en.wikipedia.org/wiki/Grand_Challenge</a>), що вимагають використання PetaFLOPS та PetaBytes обчислювальних ресурсів.
    </li><li>Приклад: веб-пошукові машини / бази даних, що обробляють мільйони транзакцій кожну секунду.
    <p>
    <img src="./Introduction to Parallel Computing_files/biggerProblems.jpg" width="600" height="180">
    </p></li></ul>
<p>
</p></li><li><b>Впровадження паралельності:</b> 
    <ul>
    <li>Один обчислювальний ресурс може робити лише одну річ за раз. Кілька обчислювальних ресурсів можуть робити багато речей одночасно.
    </li><li>Приклад: спільні мережі забезпечують глобальне місце, де люди з усього світу можуть зустрічатися та працювати "практично".
    <p>
    <img src="./Introduction to Parallel Computing_files/collaborativeNetworks.jpg" width="600" height="182">
    </p></li></ul>
<p>
</p></li><li><b>ЗАСТОСОВУВАТИ ПЕРЕВАГУ НЕЛОКАЛЬНИХ РЕСУРСІВ:</b> 
    <ul>
    <li>Використання обчислювальних ресурсів у широкосмуговій мережі або навіть в Інтернеті, коли локальні обчислювальні ресурси є дефіцитними або недостатніми. Нижче наведено два приклади, у кожному з яких понад 1,7 мільйонів учасників у глобальному масштабі (травень 2018 року):
    </li><li>Приклад: SETI @ home (<a href="http://setiathome.berkeley.edu/" target="_blank">setiathome.berkeley.edu</a>)        <!------------ Source:
        <A HREF=https://boincstats.com/ TARGET=_blank>
        https://boincstats.com/</A> 
        -------------->
    </li><li>Приклад: Folding @ home (<a href="http://folding.stanford.edu/" target="folding">folding.stanford.edu</a>)
    <p>
    <img src="./Introduction to Parallel Computing_files/SETILogo.jpg" width="600" height="122">
    </p></li></ul>
<p>
</p></li><li><b>ЗРОБИТИ ВИКОРИСТАННЯ ПАРАМЕТРОВОГО ОБЛАДНАННЯ:</b> 
    <ul>
    <li>Сучасні комп'ютери, навіть ноутбуки, є паралельними в архітектурі з декількома процесорами / ядрами.
    </li><li>Паралельне програмне забезпечення спеціально призначене для паралельних апаратних засобів з декількома ядрами, нитками тощо.
    </li><li>У більшості випадків серійні програми запускають на сучасних комп'ютерах "відходи" потенціалу обчислювальної потужності.
    <p>
    <table border="0" cellspacing="0" cellpadding="0">
    <tbody><tr valign="top">
    <td align="center">
    <img src="./Introduction to Parallel Computing_files/xeon5600processorDie3.jpg" width="600" height="321">
    <br>Процесор Intel Xeon з 6 ядер та 6 одиниць кеш-пам'яті L3</td>
    </tr></tbody></table>
    </p></li></ul>
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Майбутнє:</span>
</p><ul>
<li>Протягом останніх 20 років тенденції, що свідчать про все більш швидкі мережі, розподілені системи та багатопроцесорні комп'ютерні архітектури (навіть на рівні робочого столу), ясно показують, що <b><i>паралелізм - це майбутнє обчислень.</i></b>.
<p>
</p></li><li>У цей же період часу спостерігалося збільшення продуктивності суперкомп'ютера більш ніж на
    <font style="background-color: yellow"><b>500 000 хв</b></font>, 
   і в даний час не існує кінця.
<p>
</p></li><li><b><i>Гонка вже працює на Exascale Computing!</i></b>
    <ul>
    <li>Exaflop = 10<sup>18</sup> розрахунків за секунду
    </li></ul>
</li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/top500.1993-2018.png">
</p><dd><font size="-1"><i>Джерело: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font>
</dd></td>
</tr></tbody></table>

<!--========================================================================-->

<a name="Who"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Огляд</span></td>
</tr></tbody></table>
</p><h2>Хто використовує паралельні обчислення?</h2>

<p>
<table border="0" cellspacing="0" cellpadding="0" width="800">
<tbody><tr valign="top">
<td>
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Наука та техніка:</span>
<ul>
<li>Історично склалося так, що паралельні обчислення вважаються "високим кінцем обчислень", і були використані для моделювання складних проблем у багатьох галузях науки та техніки:
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
    <li>Атмосфера, Земля, навколишнє середовище
    </li><li>ФФізика - прикладна, ядерна, частинка, конденсована речовина, високий тиск, синтез, фотоніка
    </li><li>Біологія, Біотехнологія, Генетика
    </li><li>Хімія, молекулярні науки
    </li><li>Геологія, сейсмологія
    </li></ul></td>
<td><ul>
    <li>Машинобудування - від протезування до космічних апаратів
    </li><li>Електротехніка, схемотехніка, мікроелектроніка
    </li><li>Комп'ютерні науки, математика
    </li><li>Оборона, зброя
    </li></ul></td>
</tr></tbody></table>
<p>
<img src="./Introduction to Parallel Computing_files/simulations01.jpg" width="781" height="357">
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Промислові та комерційні</span>
</p><ul>
<li>Сьогодні комерційні додатки забезпечують однакову або більшу рушійну силу при розробці більш швидких комп'ютерів. Ці програми вимагають обробки великої кількості даних за допомогою складних способів. Наприклад:
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
    <li>"Big Data", бази даних, виведення даних
    </li><li>Штучний інтелект (AI)
    </li><li>Веб-пошукові системи, веб-сервіси
    </li><li>Медична візуалізація та діагностика
    </li><li>Фармацевтичний дизайн
    </li></ul></td>
<td><ul>
    <li>Фінансове та економічне моделювання
    </li><li>Управління національними та мультинаціональними корпораціями 
    </li><li>Розвинена графіка та віртуальна реальність, особливо в індустрії розваг
    </li><li>Мережеві відео та мультимедійні технології
    </li><li>Пошук нафти
    </li></ul></td>
</tr></tbody></table>
<p>
<img src="./Introduction to Parallel Computing_files/simulations03.jpg" width="781" height="360">
</p></li></ul>
</td>
</tr></tbody></table>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Глобальні програми:</span>
</p><ul>
<li>Паралельні обчислення в даний час широко використовуються у всьому світі, у різних областях застосування.
<p>
<img src="./Introduction to Parallel Computing_files/top500Apps.gif" width="753" height="446"><br><font size="-1"><i>Джерело: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font><p>
</p><table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">


<td colspan="2">
Натисніть на зображення нижче для більшої версії</td>
</tr><tr valign="top">
<td><a href="./Introduction to Parallel Computing_files/top500SegmentsTime.png" target="_blank">
<img src="./Introduction to Parallel Computing_files/top500SegmentsTime.png" width="400"></a></td>
<td><a href="./Introduction to Parallel Computing_files/top500CountriesTime.png">
<img src="./Introduction to Parallel Computing_files/top500CountriesTime.png" target="_blank" width="400"></a></td>
</tr></tbody></table>
<br><font size="-1"><i>Джерело: <a href="http://top500.org/" target="_blank">Top500.org</a></i></font>
</p></li></ul>

<!--========================================================================-->

<a name="Concepts"> <br><br> </a>
<a name="Neumann"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Поняття та термінологія</span></td>
</tr></tbody></table>
<h2>за архітектурою Неймана</h2>

<ul>
<p>
</p><li>Названий за угорським математиком / генієм Джоном фон Нейманом, який вперше створив загальні вимоги до електронного комп'ютера в своїх 1945-х роботах.
<p>
</p></li><li>Також відомий як "Накопичувач програма" - як інструкції програми, так і дані зберігаються в електронній пам'яті.
<p>
</p></li><li>З тих пір практично всі комп'ютери дотримуються цього базового дизайну:
   
</li></ul>
<p>
<table cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/vonNeumann1.gif" width="293" height="277"></td>
<td><ul>
<li>Складається з чотирьох основних компонентів:
    <ul>
    <li>Пам'ять
    </li><li>Пристрій управління
    </li><li>Арифметичний логічний блок
    </li><li>Введення-виведення
    </li></ul>
<p>
</p></li><li>Читання / запис, пам'ять випадкового доступу використовується для зберігання як програмних інструкцій, так і даних
    <ul>
    <li>Програмні інструкції - це закодовані дані, які повідомляють комп'ютеру щось зробити
    </li><li>Дані - це просто інформація, яка буде використовуватися програмою
    </li></ul>
<p>
</p></li><li>Блок керування виводить інструкції / дані з пам'яті, декодує інструкції, а потім <b><i>послідовно</i></b> координує операції для виконання запрограмованого завдання.
<p>
</p></li><li>Арифметичний підрозділ виконує базові арифметичні операції
<p>
</p></li><li>Вхід / вихід є інтерфейсом для оператора людини
</li></ul></td>
<td align="center"><img src="./Introduction to Parallel Computing_files/vonNeumann2.jpg" width="221" height="287" hspace="20">
<br><i>Джон фон Нейман близько 1940-х років<br>(Джерело: архіви LANL)</i></td>
</tr></tbody></table>
</p><p>
</p><ul>
<p>
</p><li>Більше інформації про його інші чудові досягнення:
    <a href="http://en.wikipedia.org/wiki/John_von_Neumann" target="_blank">
    http://en.wikipedia.org/wiki/John_von_Neumann</a>
<p>
</p></li><li>І що? Хто хвилює? 
    <ul>
    <li>Що ж, паралельні комп'ютери все ще дотримуються цього базового дизайну, просто помножуючись на одиниці. Основна, фундаментальна архітектура залишається незмінною.
    </li></ul>
</li></ul>

<!--========================================================================-->
<p>
<a name="Flynn"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Поняття та термінологія</span></td>
</tr></tbody></table>
</p><h2>Класична таксономія Флінна</h2>

<ul>
<li>Існують різні способи класифікації паралельних комп'ютерів. Приклади доступні
    <a href="https://computing.llnl.gov/tutorials/parallel_comp/parallelClassifications.pdf" target="_blank">ТУТ</a>. 
<p>
</p></li><li>Один з найбільш широко використовуваних класифікацій, що використовуються з 1966 року, називається таксономією Філліна.
<p>
</p></li><li>Таксономія Флінна розрізняє багатопроцесорні комп'ютерні архітектури відповідно до того, як їх можна класифікувати за двома незалежними вимірами <b><i>потоку інструкцій</i></b> та <b><i>потоку даних</i></b>.  
   Кожен з цих параметрів може мати лише один із двох можливих станів
    <b><i>Одиночний </i></b> or <b><i>Кілька</i></b>.
<p>
</p></li><li>Матриця нижче визначає 4 можливих класифікацій відповідно до Flynn:
<p>
<img src="./Introduction to Parallel Computing_files/flynnsTaxonomy.gif" width="425">
</p></li></ul>

<p><br></p><hr><p><br>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Єдина інструкція, єдині дані (SISD):</span>
</p><ul>
<li>Серійний (не паралельний) комп'ютер
</li><li><b>Єдина інструкція:</b> ЦП використовує лише один потік інструкцій протягом будь-якого циклу годин
</li><li><b>Окремі дані:</b> лише один потік даних використовується як вхід протягом будь-якого циклу годинника
</li><li>Детерміноване виконання   
</li><li>Це найстаріший тип комп'ютера
</li><li>Приклади: мейнфрейми старшого покоління, мікрокомп'ютери, робочі станції та ПК з одним процесором / ядром.
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td align="left"><img src="./Introduction to Parallel Computing_files/sisd2.gif" height="224" border="1" hspace="30"></td>
<td> </td>
<td><img src="./Introduction to Parallel Computing_files/sisd.gif" width="188" height="224"></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/univac1.LLNL.200pix.jpg" width="262" height="200">
<br><b>UNIVAC1	</b></td>
<td><img src="./Introduction to Parallel Computing_files/ibm.360.200pix.jpg" width="300" height="200">
<br><b>IBM 360</b></td>
<td><img src="./Introduction to Parallel Computing_files/cray1.LLNL.200pix.jpg" width="200" height="200">
<br><b>CRAY1</b></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/cdc7600.LLNL.200pix.jpg" width="262" height="200">
<br><b>CDC 7600</b></td>
<td><img src="./Introduction to Parallel Computing_files/pdp1.LLNL.200pix.jpg" width="298" height="200">
<br><b>PDP1</b></td>
<td><img src="./Introduction to Parallel Computing_files/dellLaptop.200pix.jpg" width="201" height="200">
<br><b>Dell Laptop</b></td>
</tr></tbody></table>
</p></li></ul>

<p><br></p><hr><p><br>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Одна інструкція, кілька видів даних (SIMD):</span>
</p><ul>
<li>Тип паралельного комп'ютера
</li><li><b>Одинична інструкція:</b>  всі процесори виконують одну й ту ж інструкцію в будь-якому заданому циклі годин
</li><li><b>Кілька даних:</b> кожен процесор може працювати на іншому елементі даних.
</li><li>Найкраще підходять для спеціалізованих проблем, що характеризуються високим рівнем регулярності, такими як обробка графіки / зображення.
</li><li>Синхронний (локальний) і детермінований виконання.
</li><li>Два різновиди: масив процесора та векторні трубопроводи
</li><li>Приклади: 
    <ul type="circle">
    <li>Масив процесора: мишачі машини CM-2, MasPar MP-1 і MP-2, ILLIAC IV
    </li><li>Векторні трубопроводи: IBM 9000, Cray X-MP, Y-MP і C90, Fujitsu VP, NEC SX-2, Hitachi S820, ETA10
    </li></ul>
</li><li>Більшість сучасних комп'ютерів, особливо ті, що мають графічні процесорні блоки (графічні процесори), використовують команди SIMD та виконавчі пристрої.
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/simd3.gif" height="224" border="1"></td>
<td> </td>
<td><img src="./Introduction to Parallel Computing_files/simd.gif" width="438" height="224"></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/illiacIV.200pix.jpg" width="293" height="200">
<br><b>ILLIAC IV</b></td>
<td><img src="./Introduction to Parallel Computing_files/MasPar.200pix.jpg" width="172" height="200">
<br><b>MasPar</b></td>
<td><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="./Introduction to Parallel Computing_files/simd2.gif" width="400" height="147" border="0"></td>
</tr></tbody></table>
<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/crayXMP.200pix.jpg" width="150" height="200">
<br><b>Cray X-MP</b></td>
<td><img src="./Introduction to Parallel Computing_files/crayYMP.200pix.jpg" width="282" height="200">
<br><b>Cray Y-MP</b></td>
<td><img src="./Introduction to Parallel Computing_files/cm2.200pix.jpg" width="298" height="200">
<br><b>Thinking Machines CM-2</b></td>
<td><img src="./Introduction to Parallel Computing_files/cellProcessor.200pix.jpg" width="179" height="200">
<br><b>Cell Processor (GPU)</b></td>
</tr></tbody></table>
</p></li></ul>

<p><br></p><hr><p><br>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Кілька інструкцій, одиночні дані (MISD):</span>
</p><ul>
<li>Тип паралельного комп'ютера
</li><li><b>Кілька інструкцій:</b> кожен процесор працює на даних самостійно за допомогою окремих потоків інструкцій.
</li><li><b>Окремі дані:</b> єдиний потік даних подається на кілька модулів обробки.
</li><li>Мало (якщо такі є) фактичні приклади цього класу паралельних комп'ютерів коли-небудь існували.
</li><li>Деякі мислимі способи використання можуть бути:
    <ul>
    <li>кілька фільтрів частоти, що працюють на одному сигналі потоку
    </li><li>кілька алгоритмів криптографії, що намагаються зламати єдине кодоване повідомлення.
    </li></ul>
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="center" align="center">
<td align="left"><img src="./Introduction to Parallel Computing_files/misd4.gif" height="245" hspace="30" border="1"></td>
<td><img src="./Introduction to Parallel Computing_files/misd.gif" width="438" height="207" hspace="20"></td>
</tr></tbody></table>
</p></li></ul>

<p></p><hr><p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Кілька інструкцій, кілька даних (MIMD):</span>
</p><ul>
<li>Тип паралельного комп'ютера.
</li><li><b>Кілька інструкцій:</b> кожен процесор може виконувати інший потік інструкцій.
</li><li><b>Кілька даних:</b> кожен процесор може працювати з різним потоком даних.
</li><li>Виконання може бути синхронним або асинхронним, детермінованим або недетермінованим.
</li><li>В даний час найпоширеніший тип паралельного комп'ютера - найсучасніші суперкомп'ютери потрапляють до цієї категорії.
</li><li>Приклади: найбільш сучасні суперкомп'ютери, мережеві паралельні комп'ютерні кластери та "сітки", багатопроцесорні комп'ютери SMP, багатоядерні ПК.
</li><li>Примітка. Багато MIMD-архітектури також включають підкомпоненти виконання SIMD.
<p>

<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/mimd2.gif" height="245" border="1"></td>
<td colspan="2"><img src="./Introduction to Parallel Computing_files/mimd.gif" width="438" height="245"></td>
</tr><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/ibmPower5Cluster.200pix.jpg" width="301" height="200">
<br><b>IBM POWER5</b></td>
<td><img src="./Introduction to Parallel Computing_files/alphaserverCluster.200pix.jpg" width="302" height="200">
<br><b>HP/Compaq Alphaserver</b></td>
<td><img src="./Introduction to Parallel Computing_files/ia32Cluster.200pix.jpg" width="299" height="200">
<br><b>Intel IA32</b></td>
</tr></tbody></table>
<table border="0" cellpadding="5" cellspacing="3" bgcolor="#EEEEEE" width="975"> 
<tbody><tr valign="top" align="center">
<td><img src="./Introduction to Parallel Computing_files/opteronCluster.200pix.jpg" width="299" height="200">
<br><b>AMD Opteron</b></td>
<td><img src="./Introduction to Parallel Computing_files/crayXT3Cluster.200pix.jpg" width="300" height="200">
<br><b>Cray XT3</b></td>
<td><img src="./Introduction to Parallel Computing_files/bglCluster.200pix.jpg" width="301" height="200">
<br><b>IBM BG/L</b></td>
</tr></tbody></table>
</p></li></ul>
<p>

<!--========================================================================-->

<a name="Terminology"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Поняття та термінологія</span></td>
</tr></tbody></table>
</p><h2>Деяка загальна паралельна термінологія</h2>

<ul>
<li>Як і все інше, паралельні обчислення мають свій "жаргон". Нижче наведено деякі з найпоширеніших термінів, пов'язаних з паралельними обчисленнями.
<p>
</p></li><li>Більшість з них буде обговорюватися більш детально пізніше.<p>
</p><dl>
<dt><b>Суперкомп'ютерне / високопродуктивне обчислення (HPC)</b>
</dt><dd>Використання найшвидших і найбільших комп'ютерів у світі для вирішення великих проблем.
<p>
</p></dd><dt><b>Вузол </b>
</dt><dd>Автономний "комп'ютер в коробці". Зазвичай складається з декількох процесорів / процесорів / ядер, пам'яті, мережевих інтерфейсів тощо. Вузли об'єднуються в мережу для створення суперкомп'ютера.
<p>
</p></dd><dt><b>CPU / Socket / Процесор / Core </b>
</dt><dd>Це залежить від того, з чим ви працюєте. У минулому процесор (центральний процесор) був єдиним компонентом виконання для комп'ютера. Потім декілька процесорів були включені в вузол. Потім, окремі процесори були  розділені на кілька "ядер", кожен з яких є унікальним блоком виконання. Процесори з декількома ядрами іноді називають "сокетами" - залежними від постачальників. Результатом є вузол з декількома процесорами, кожен із яких містить декілька ядер. Номенклатура заплутана часом. Чудово чому?
<p>
<img src="./Introduction to Parallel Computing_files/nodeSocketCores.jpg" width="800" heigth="375">
</p><p>
</p></dd><dt><b>Завдання </b>
</dt><dd>Логічно дискретний розділ обчислювальних робіт. Завдання, як правило, являє собою програмний або програмний набір інструкцій, який виконується процесором. Паралельна програма складається з декількох завдань, що виконуються на декількох процесорах.
<p>
</p></dd><dt><b>Трубопроводи</b>
</dt><dd>Розбиваючи завдання на кроки, виконувані різними блоками процесора, з вхідними потоками через, як схожу лінію; тип паралельних обчислень.
<p>
</p></dd><dt><b>Спільна пам'ять</b>
</dt><dd>З суто апаратної точки зору, описується архітектура комп'ютера, де всі процесори мають прямий (звичайний) доступ до загальної фізичної пам'яті. У програмуванні він описує модель, в якій паралельні завдання мають однакову "картинку" пам'яті і можуть безпосередньо звертатися до одних і тих же логічних місць пам'яті незалежно від того, де фактично існує фізична пам'ять.
<p>
</p></dd><dt><b>Симметричний багатопроцесор (SMP)</b>
</dt><dd>Архітектура апаратного забезпечення спільної пам'яті, де кілька процесорів мають єдиний адресний простір і мають рівний доступ до всіх ресурсів.
<p>
</p></dd><dt><b>Розподілена пам'ять</b>
</dt><dd>У апаратному забезпеченні розуміється доступ до пам'яті на мережевому рівні для фізичної пам'яті, який не є загальним. Як модель програмування завдання можуть логічно "бачити" локальну пам'ять комп'ютера і використовувати комунікації для доступу до пам'яті на інших машинах, де виконуються інші завдання.
<p>
</p></dd><dt><b>Зв'язок</b>
</dt><dd>Паралельні завдання зазвичай потребують обміну даними. Це може бути досягнуто кількома способами, наприклад, через спільну пам'ять або через мережу, проте фактична подія обміну даними часто називається зв'язком, незалежно від використовуваного методу.
<p>
</p></dd><dt><b>Синхронізація</b>
</dt><dd>Координація паралельних завдань у режимі реального часу, дуже часто пов'язана з комунікаціями. Часто реалізується шляхом встановлення точки синхронізації в додатку, де завдання може не продовжуватися, доки інші завдання (и) не досягнуть тієї ж або логічно еквівалентної точки.
<p>
Синхронізація зазвичай передбачає очікування як мінімум одного завдання, і тому може призвести до збільшення часу виконання часу настінного годинника паралельної програми.
</p><p>
</p></dd><dt><b>Гранулярність</b>
</dt><dd>При паралельних обчисленнях деталізація є якісною мірою співвідношення обчислень до комунікації.
    <ul>
    <li><b><i>Грубі:</i></b> відносно великі обсяги обчислювальних робіт виконуються між комунікаційними подіями.
    </li><li><b><i>Добре:</i></b> відносно невеликі обсяги обчислювальних робіт виконуються між подій зв'язку.
    </li></ul>
<p></p></dd><dt><b>Спостерігається прискорення</b>
</dt><dd>Спостерігається прискорення розпаралелювання коду, який визначається як:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td align="center">
<pre>настінний час послідовного виконання

-----------------------------------
 настінний час паралельного виконання</pre></td>
</tr></tbody></table>
</p><p>
Один з найпростіших і найбільш широко використовуваних показників для виконання паралельної програми.
</p><p>
</p></dd><dt><b>Паралельні накладні витрати</b>
</dt><dd>Кількість часу, необхідного для координації паралельних завдань, на відміну від здійснення корисної роботи. Паралельні накладні витрати можуть включати наступні фактори:
    <ul>
    <li>Час запуску завдання
    </li><li>Синхронізація
    </li><li>Передача даних
    </li><li>Накладне програмне забезпечення, накладене паралельними мовами, бібліотеками, операційною системою тощо.
    </li><li>Час завершення завдання
    </li></ul>
<p>
</p></dd><dt><b>Масовано паралельно</b>
</dt><dd>Належить до апаратного забезпечення, що складається з заданої паралельної системи, що має багато оброблюваних елементів. Значення "багатьох" постійно зростає, але в даний час найбільші паралельні комп'ютери складаються з оброблюваних елементів, що нумерують від сотень тисяч до мільйонів.
<p>
</p></dd><dt><b>Смутно паралельно</b> 
</dt><dd>Рішення багатьох аналогічних, але незалежних завдань одночасно; трохи не потребує координації між завданнями.
<p>
</p></dd><dt><b>Масштабованість</b>
</dt><dd>Посилається на здатність паралельної системи (обладнання та / або програмне забезпечення) демонструвати пропорційне збільшення паралельного прискорення з додаванням більшої кількості ресурсів. Фактори, що сприяють масштабованості, включають:
    <ul>
    <li>Апаратне забезпечення - особливо пропускна спроможність пам'яті-процесор та властивості мережевого зв'язку
    </li><li>Алгоритм застосування
    </li><li>Паралельні накладні витрати пов'язані
    </li><li>Характеристики вашої конкретної програми
    </li></ul>
</dd></dl>
</li></ul>

<!--========================================================================-->

<a name="LimitsCosts"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Поняття та термінологія</span></td>
</tr></tbody></table>
<h2>Межі та витрати паралельного програмування</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Закон Амдаля:</span>
<p>
<table cellpadding="0" cellspacing="0" width="100%">
<tbody><tr valign="top">
<td><ul>
<p>
</p><li><b>Закон Амдаля</b> говорить про те, що потенціал прискорення програми визначається часткою коду (P), яку можна розпаралелювати:
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
                     1
    прискорення = -------- 
                   1  - P

</b></pre></td></tr></tbody></table>
</p><p>
</p></li><li>Якщо жоден з блоків коду не може бути розпаралелізований, P = 0 і прискорення = 1 (без прискорення).
<p>
</p></li><li>Якщо весь код розпаралелюється, P = 1 і прискорення є нескінченним (теоретично).
<p>
</p></li><li>Якщо 50% коду можна розпаралелювати, максимальна швидкість = 2, тобто код буде працювати вдвічі швидше.
<p>
</p></li><li>Представляючи кількість процесорів, що виконують паралельну частку роботи, співвідношення можна моделювати за допомогою:
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
                       1  
    прискорення =  ------------ 
                    P   +  S
                   ---
                    N

</b></pre></td></tr></tbody></table>
</p><p>
   де P = паралельна фракція, N = кількість процесорів і S = серійна фракція.
</p></li></ul>
</td>
<td><img src="./Introduction to Parallel Computing_files/amdahl1.gif" width="509" height="390" border="1" hspace="20">
<br>
<img src="./Introduction to Parallel Computing_files/amdahl2.gif" width="509" height="391" border="1" hspace="20"></td>
</tr></tbody></table>
</p><p>
</p><ul>
<li>Незабаром стає очевидним, що існують межі масштабованості паралелізму. Наприклад: 
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
                       прискоритись
          -------------------------------------
    N     P = .50   P = .90   P = .95   P = .99
  -----   -------   -------   -------   -------
     10      1.82      5.26      6.89      9.17
    100      1.98      9.17     16.80     50.25     
  1,000      1.99      9.91     19.62     90.99
 10,000      1.99      9.91     19.96     99.02
100,000      1.99      9.99     19.99     99.90

</b></pre></td></tr></tbody></table>
</p><p>
<b>"Знаменитий" вираз:</b><i> Ви можете витратити на все життя 95% вашого коду, щоб бути паралельним, і ніколи не досягти прискорення краще, ніж 20x незалежно від того, скільки процесорів ви кидаєте на це!

</i>
</p></li></ul>

<ul>
<p>
</p><li>Проте деякі проблеми демонструють підвищену продуктивність, збільшуючи розмір проблеми. Наприклад
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
    2D Grid Calculations     85 seconds   85%    
    Serial fraction          15 seconds   15%    

</b></pre></td></tr></tbody></table>
</p><p>
   Ми можемо збільшити розмір проблеми, подвоївши розміри сітки та зменшивши наполовину часовий крок. Це призводить до чотирикратного збільшення кількості точок сітки та вдвічі більшої кількості етапів часу. Тоді танфіни виглядають так:
</p><p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top"><td><pre><b>
    2D Grid Calculations     680 seconds   97.84%    
    Serial fraction           15 seconds    2.16%    

</b></pre></td></tr></tbody></table>
</p><p>
</p></li><li>Проблеми, що збільшують відсоток паралельного часу з їх розміром, більш <b><i>масштабні</i></b> ніж проблеми з фіксованою процентною часткою паралельного часу.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Складність:</span>
</p><ul>
<p>
</p><li>Загалом, паралельні програми набагато складніші, ніж відповідні послідовні програми, можливо, на порядок. Ви не тільки маєте декілька потоків інструкцій, що виконуються одночасно, але також є дані між ними.
<p>
</p></li><li>Вартість складності вимірюється в програміста практично у всіх аспектах циклу розробки програмного забезпечення:
    <ul>
    <li>Дизайн
    </li><li>Кодування
    </li><li>Налагодження
    </li><li>Тюнінг
    </li><li>Технічне обслуговування
    </li></ul>
<p>
</p></li><li>При роботі з паралельними програмами важливо дотримуватися «правильних» правил розробки програмного забезпечення, особливо якщо хтось, крім вас, повинен буде працювати з програмним забезпеченням.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Портативність:</span>
</p><ul>
<p>
</p><li>Завдяки стандартизації в декількох інтерфейсах API, таких як MPI, потоки POSIX та OpenMP, проблеми з переносністю паралельних програм не настільки серйозні, як у минулому. Однак ...
<p>
</p></li><li>Усі звичайні проблеми портативності, пов'язані з серійними програмами, застосовуються до паралельних програм. Наприклад, якщо ви використовуєте "удосконалення" постачальників для Fortran, C або C ++, портативність стане проблемою.
<p>
</p></li><li>Незважаючи на те, що стандарти існують для кількох API, реалізація буде відрізнятися в ряді деталей, іноді до необхідності внесення змін до коду, щоб забезпечити перенесення коду.
<p>
</p></li><li>Операційні системи можуть відігравати ключову роль у питаннях перенесення коду.
<p>
</p></li><li>Апаратні архітектури характеризуються високою мінливістю і можуть вплинути на перенесення коду.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Вимоги до ресурсів:</span>
</p><ul>
<p>
</p><li>Основним наміром паралельного програмування є зменшення часу виконання завдання, однак для цього потрібно більше часу для процесора. Наприклад, паралельний код, який працює протягом 1 години на 8 процесорах, насправді використовує 8 годин процесора.
<p>
</p></li><li>Обсяг необхідної пам'яті може бути більшим для параленього коду, ніж серійний код, у зв'язку з необхідністю реплікації даних та накладних витрат, пов'язаних з паралельними бібліотеками підтримки.
<p>
</p></li><li>Для короткого запуску паралельних програм фактично може бути зниження продуктивності в порівнянні з аналогічною серійною реалізацією. Накладні витрати, пов'язані з налаштуванням паралельного середовища, створення завдання, зв'язку та завершення завдання, можуть скласти значну частину загального часу виконання коротких циклів.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Масштабованість:</span>
</p><ul>
<p>
</p><li>Існують два типи масштабування, кожне залежать від часу: сильне масштабування та слабке масштабування.
<p>

<img src="./Introduction to Parallel Computing_files/strongWeakScaling.gif" align="right" hspace="20" width="300">
</p></li><li><b>Сильне масштабування:</b> 
<ul>
<li>Загальний розмір проблеми залишається фіксованим, оскільки додається більше процесорів.
</li><li>Мета - розвязати один і той же розмір проблеми швидше.
</li><li>Ідеальне масштабування означає, що проблема вирішується в 1 / P час (у порівнянні з серійним).
</li></ul>
<p>
</p></li><li><b>Слабке масштабування:</b> 
<ul>
<li>Розмір задач <i>на одному процесорі</i> залишається фіксованим, оскільки додається більше процесорів.
</li><li>Мета - розвязати більшу проблему в той самий час.
</li><li>Відмінне масштабування означає, що проблема Px запускається в той самий час, що і запуск одного процесора.
</li></ul>
<p>
</p></li><li>Здатність виконання паралельної програми до масштабу є результатом ряду взаємопов'язаних факторів. Просто додавання інших процесорів рідко відповідає вирішенню проблем паралельного програмування.
<p>
</p></li><li>Алгоритм може мати внутрішні межі масштабованості. У якийсь момент додавання додаткових ресурсів призводить до зниження продуктивності. Це загальна ситуація з багатьма паралельними додатками.
<p>
</p></li><li>Апаратні фактори відіграють значну роль у маштабуванні. Приклади:
    <ul>
    <li>Пропускна здатність шини Memory-cpu на машині SMP
    </li><li>Пропускна здатність зв'язку в мережі
    </li><li>Кількість пам'яті, доступної на будь-якій машині чи наборі машин
    </li><li>Тактова частота процесора
    </li></ul>    
<p>
</p></li><li>Програмне забезпечення паралельних бібліотек та підсистем може обмежувати масштабованість, незалежно від вашої програми.
</li></ul>

<!--========================================================================-->

<a name="MemoryArch"> <br><br> </a>
<a name="SharedMemory"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні архітектури пам'яті комп'ютера</span></td>
</tr></tbody></table>
<h2>Спільна пам'ять</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Загальна характеристика:</span>
<p>
<table cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><ul>
<li>Паралельні комп'ютери із загальною пам'яттю знаходяться в широкому діапазоні, але загалом мають загальну здатність всіх процесорів отримати доступ до всієї пам'яті як до глобального адресного простору.
<p>
</p></li><li>Кілька процесорів можуть працювати незалежно, але розділяють ті ж ресурси пам'яті.
<p>
</p></li><li>Зміни в пам'яті, здійснені одним процесором, видні для всіх інших процесорів.
<p>
</p></li><li>Історично загальні пристрої пам'яті були класифіковані як
    <b><i>UMA</i></b> та <b><i>NUMA</i></b>,  на основі часу доступу до пам'яті.
<p>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Єдиний доступ до пам'яті (UMA):</span>
    </p><ul>
    <li>Найбільш часто представлені сьогодні <b><i>симетричними багатопроцесорними (SMP)</i></b> машинами
    </li><li>Ідентичні процесори
    </li><li>Рівний доступ та час доступу до пам'яті 
    </li><li>Іноді називають CC-UMA - Cache Coherent UMA. Кеш-когерентний означає, якщо один процесор оновлює місцеположення в спільній пам'яті, всі інші процесори знають про оновлення. Когерентність кешів здійснюється на апаратному рівні.
    </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Неоднорідний доступ до пам'яті (NUMA):</span>
    </p><ul>
    <li>Часто здійснюється шляхом фізичного з'єднання двох або більше SMP 
    </li><li>Один SMP може безпосередньо отримати доступ до пам'яті іншого SMP
    </li><li>Не всі процесори мають рівний час доступу до всієї пам'яті
    </li><li>Доступ до пам'яті через посилання повільний
    </li><li>Якщо підтримується когерентність кеш-пам'яті, то її також можна назвати CC-NUMA - Cache Coherent NUMA

    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Переваги:</span>
    </p><ul>
    <li>Глобальний адресний простір забезпечує зручну для користувача програмування пам'ять
    </li><li>Обмін даними між завданнями є швидким та однорідним завдяки близькості пам'яті до процесорів
    </li></ul>
</td>
<td align="center"><img src="./Introduction to Parallel Computing_files/shared_mem.gif" width="414" height="285">
<br><b>Спільна пам'ять (UMA)</b><br><br><br>
<img src="./Introduction to Parallel Computing_files/numa.gif" width="484" height="196">
<br><b>Спільна пам'ять (NUMA)</b>
</td>
</tr></tbody></table>
</p><p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Недоліки:</span>
</p><ul>
<li>Primary disadvantage is the lack of scalability between memory and CPUs.
    Adding more CPUs can geometrically increases traffic on the shared
    memory-CPU path, and for cache coherent systems, geometrically increase 
    traffic associated with cache/memory management. 
</li><li>Programmer responsibility for synchronization constructs that ensure
    "correct" access of global memory. 
</li></ul>


<!--========================================================================-->
<p>
<a name="DistributedMemory"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні архітектури пам'яті комп'ютера</span></td>
</tr></tbody></table>
</p><h2>Розподілена пам'ять</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Загальні характеристики:</span>
<ul>
<p>
</p><li>Як і в системах спільної пам'яті, системи розподіленої пам'яті дуже різні, але мають спільну характеристику. Для розподілених систем пам'яті потрібна мережа зв'язку для підключення міжпроцесорної пам'яті.
<p>
<img src="./Introduction to Parallel Computing_files/distributed_mem.gif" width="484" height="196" hspace="10">
</p><p>
</p></li><li>Процесори мають свою локальну пам'ять. Адреси пам'яті в одному процесорі не відображаються на іншому процесорі, тому немає поняття про глобальний адресний простір у всіх процесорах.
<p>
</p></li><li>Оскільки кожен процесор має свою локальну пам'ять, вона працює самостійно. Зміни, внесені в її локальну пам'ять, не впливають на пам'ять інших процесорів. Отже, поняття когерентності кеш-пам'яті не застосовується.
<p>
</p></li><li>Коли процесору потрібен доступ до даних в іншому процесорі, як правило, це завдання для програміста явно визначити, як і коли дані передаються. Синхронізація завдань також є відповідальністю програміста.
<p>
</p></li><li>Мережа "fabric" (тканина), що використовується для передачі даних, широко відрізняється, хоча це може бути настільки ж просто, як Ethernet.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Переваги:</span>
    </p><ul>
    <li>Пам'ять масштабується з кількістю процесорів. Збільшення кількості процесорів і розмір пам'яті збільшуються пропорційно. 
    </li><li>Кожен процесор може швидко отримати доступ до власної пам'яті без перешкод і без накладних витрат, намагаючись зберегти узгодженість глобальної кеш-пам'яті.
    </li><li>Ефективність витрат: можна використовувати товарний, незалежний процесор та мережу.
    </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Недоліки:</span>
    </p><ul>
    <li>Програміст несе відповідальність за багато деталей, пов'язаних з передачею даних між процесорами.
    </li><li>Можливо, буде важко структурувати існуючі структури даних на основі глобальної пам'яті для цієї організації пам'яті.
    </li><li>Неоднорідний час доступу до пам'яті - дані, що знаходяться на віддаленому вузлі, мають більший час доступу, ніж локальні дані вузла.
    </li></ul>



<!--========================================================================-->
<p>
<a name="HybridMemory"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Паралельні архітектури пам'яті комп'ютера</span></td>
</tr></tbody></table>
</p><h2>Гібридна розподілена загальна пам'ять</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Загальні характеристики:</span>
<ul>
<p>
</p><li>Найбільші та найшвидкісніші комп'ютери у світі сьогодні використовують як загальні, так і розподілені архітектури пам'яті.
<p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/hybrid_mem.gif" width="484" height="196" hspace="10"></td>
<td><img src="./Introduction to Parallel Computing_files/hybrid_mem2.gif" width="484" height="196" hspace="10"></td>
</tr></tbody></table>
</p><p>
</p></li><li>Компонентом спільної пам'яті може бути загальна пам'ять та / або графічні модулі (GPU).
<p>
</p></li><li>Компонент розподіленої пам'яті являє собою мережу з декількома машинами іх спільною пам'яттю / графічним процесором, які знають лише про власну пам'ять, а не про пам'ять на іншій машині. Тому мережеві зв'язки необхідні для переміщення даних з однієї машини в іншу.
<p>
</p></li><li>Поточні тенденції, схоже, вказують на те, що такий тип архітектури пам'яті продовжуватиме переважати і збільшуватись на найвищому рівні обчислень у найближчому майбутньому.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3"> Переваги та недоліки:</span>
</p><ul>
<li>Що б не було загальним як для загальної, так і для розподіленої архітектури пам'яті. 
</li><li>Підвищене маштабування - важлива перевага.
</li><li>Збільшення складності програміста є важливим недоліком.
</li></ul>


<!--========================================================================-->
<p>
<a name="Models"> <br><br> </a>
<a name="ModelsOverview"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
</p><h2>Огляд</h2>

<ul>
<p>
</p><li>Існує кілька моделей паралельного програмування, що використовуються у спільному використанні:
    <ul>
    <li>Спільна пам'ять (без потоків)
    </li><li>Нитки
    </li><li>Розподілена пам'ять / передача повідомлень
    </li><li>Паралельні дані
    </li><li>Гібридний
    </li><li>Один тип даних декількох програм (SPMD)
    </li><li>Декілька видів даних декількох програм (MPMD)
    </li></ul>
<p>
</p></li><li><b>Паралельні моделі програмування існують як абстракція над архітектурою апаратного забезпечення та пам'яті.</b>
<p>
</p></li><li>Хоча це може здатися незрозумілим, ці моделі <b>НЕ</b> специфічні для певного типу архітектури машини або пам'яті. Фактично, будь-яка з цих моделей може (теоретично) бути реалізована на будь-якому базовому апаратному забезпеченні. Два приклади з минулого обговорюються нижче.
<p>
<table border="0" cellspacing="0" cellpadding="0" width="800">
<tbody><tr valign="top">
<td colspan="2">
<b>СПІЛЬНА модель пам'яті на ДИСКОВІЙ пам'яті:</b> 
<br>Kendall Square Research (KSR) ALLCACHE підхід. Машинна пам'ять фізично поширювалася на мережевих машинах, але з'являлася користувачеві як єдиний глобальний адресний простір загальної пам'яті. Як правило, цей підхід називається "віртуальною спільною пам'яттю".
</td></tr><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/modelAbstraction1.gif" height="150" vspace="20"></td>
<td><img src="./Introduction to Parallel Computing_files/ksr1.gif" width="198" height="130" vspace="20"></td>
</tr><tr valign="top">
<td colspan="2">
<b>Розподілена модель пам'яті на комп'ютері із СПІЛЬНОЇ пам'ятю:</b> 
<br>Інтерфейс передачі повідомлень (MPI) на SGI Origin 2000. SGI Origin 2000 використовував архітектуру спільної пам'яті типу CC-NUMA, де кожне завдання має прямий доступ до глобального адресного простору, що поширюється по всіх машинах. Проте реалізація і широко використовувана можливість надіслати та отримувати повідомлення за допомогою MPI, як правило, здійснюється через мережу розподілених комп'ютерів.
</td></tr><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/modelAbstraction2.gif" height="150" vspace="20"></td>
<td><img src="./Introduction to Parallel Computing_files/sgiOrigin2000.jpg" width="198" height="149" vspace="20"></td>
</tr></tbody></table>
</p><p>
</p></li><li><b>Яку модель використовувати?</b> 
   Це часто є поєднанням того, що є і особистий вибір. Немає "найкращої" моделі, хоча, звичайно, є кращі реалії деяких моделей над іншими.
<p>
</p></li><li>У наступних розділах описано кожну з моделей, згаданих вище, а також обговорюються деякі їх фактичні реалізації.
</li></ul>


<!--========================================================================-->
<p>
<a name="ModelsShared"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
</p><h2>Модель спільної пам'яті (без потоків)</h2>

<ul>
<img src="./Introduction to Parallel Computing_files/sharedMemoryModel.gif" width="350" hspace="20" align="right">
<li>У цій моделі програмування процеси / завдання поділяють на загальний адресний простір, в який вони читають та записують асинхронно.
<p>
</p></li><li>Різні механізми, такі як замки / семафори, використовуються для контролю доступу до спільної пам'яті, вирішення спорів та запобігання перегонах з пам'яттю та тупиковим умовам.
<p>
</p></li><li>Це, мабуть, найпростіша модель паралельного програмування.
<p>
</p></li><li>Перевага цієї моделі з точки зору програміста полягає в тому, що поняття "власність" даних не вистачає, тому немає потреби явно вказати обмін даними між завданнями. Всі процеси бачать і мають рівний доступ до спільної пам'яті. Завдяки цьому, розробку програми часто можна спростити.
<p>
</p></li><li>Важливим недоліком з точки зору продуктивності є те, що стає все важче зрозуміти та керувати <b><i>локацією даних</i></b>:
    <ul>
    <li>Ведення локальних даних у процесі, який працює з ними, зберігає доступ до пам'яті, оновлення кеша та трафік, який виникає, коли декілька процесів використовують однакові дані.
    </li><li>На жаль, керування локальними даними важко зрозуміти і може перебувати під контролем середнього користувача.
    </li></ul>
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Реалізації:</span>
</p><ul>
    <p>
    </p><li>На автономних машинах спільної пам'яті, нативна операційна система, компілятори та / або апаратні засоби забезпечують підтримку програмування спільної пам'яті. Наприклад, стандарт POSIX забезпечує API для використання спільної пам'яті, а UNIX забезпечує спільні сегменти пам'яті (shmget, shmat, shmctl тощо).
    <p>
    </p></li><li>На розподілених машинах пам'яті фізично розподіляється пам'ять через мережу машин, але робиться глобальною за допомогою спеціалізованого апаратного та програмного забезпечення. Доступні різні реалізації SHMEM:
        <a href="http://en.wikipedia.org/wiki/SHMEM" target="_blank">
        http://en.wikipedia.org/wiki/SHMEM</a>.
    </li></ul>


<!--========================================================================-->
<p>
<a name="ModelsThreads"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Моделі паралельного програмування</span></td>
</tr></tbody></table>
</p><h2>Моделі ниток</h2>

<ul>
<p>
</p><li>Ця модель програмування є типом спільного програмного забезпечення пам'яті.
</p></li><li>У поточній моделі паралельного програмування, один "важкий" процес може мати кілька "легких", одночасних шляхів виконання.
<p>
</p></li><li>Наприклад:

<img src="./Introduction to Parallel Computing_files/threadsModel2.gif" align="right" width="350" height="550" hspace="20">
    <ul>
    <p>
    </p><li>Основна програма <b>a.out</b> планується бути запущена в нативну операційну систему <b>a.out</b> авантажує та отримує всі необхідні системні та користувацькі ресурси для запуску. Це "важка вага" процесу.
    <p>
    </p></li><li><b>a.out</b> виконує деяку послідовну роботу, а потім створює ряд завдань (потоків), які можуть бути заплановані та запущені операційною системою одночасно.
    <p>
    </p></li><li>Кожна нитка має локальні дані, але також ділиться всіма ресурсами
        <b>a.out</b>. Це заощаджує накладні витрати, пов'язані з тиражуванням ресурсів програми для кожного потоку ("легка вага"). Кожна нитка також користується глобальною пам'яттю, оскільки вона поділяє простір пам'яті  <b>a.out</b>.      
    <p>
    </p></li><li>Роботу нитки найкраще можна описати як підпрограму в рамках основної програми. Будь-який поток може виконувати будь-яку підпрограму одночасно з іншими потоками.
    <p>
    </p></li><li>Нитки взаємодіють між собою через глобальну пам'ять (оновлення адреси). Для цього потрібні конфігурації синхронізації, щоб переконатися, що більше ніж одне поточне оновлення не підтримує ту ж глобальну адресу.
    <p>
    </p></li><li>Нитки можуть прийти і йти, але <b>a.out</b> залишається присутнім, щоб забезпечити необхідні спільні ресурси до закінчення програми.
    </li></ul>
    <p>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Реалізації:</span>
</p><ul>
<li>З точки зору програмування, реалізація потоків зазвичай включає:
    <ul type="circle">
    <li>Бібліотека підпрограм, яка викликається з паралельного вихідного коду
    </li><li>Набір директив компіляторів, вбудований як в серійний, так і в паралельний вихідний код
    </li></ul>
<p>
    В обох випадках програміст несе відповідальність за визначення паралелізму (хоча компілятори іноді можуть допомогти).
</p><p>
</p></li><li>Реалізації ниток не є новими в обчисленні. Історично постачальники обладнання реалізували власні версії потоків. Ці реалізації істотно відрізнялися один від одного, що ускладнює програмістам розробляти портативні програми.
<p>
</p></li><li>Незв'язані зусилля зі стандартизації привели до двох дуже різних реалізацій ниток:
    <b><i>POSIX Threads</i></b> та <b><i>OpenMP</i></b>.
<p>
</p></li><li><b>POSIX нитки</b> 
    <ul>
    <li>Зазначений стандартом IEEE POSIX 1003.1c (1995). C тільки мова.
    </li><li>Частина операційних систем Unix / Linux.
    </li><li>Бібліотека на базі.
    </li><li>Зазвичай називають Pthreads.
    </li><li>Дуже явний паралелізм; вимагає значної уваги програміста до деталей.
    </li></ul>
<p>
</p></li><li><b>OpenMP</b>  
    <ul>
    <li>Промисловий стандарт, спільно визначений і схвалений групою основних постачальників комп'ютерних апаратних і програмних продуктів, організацій та окремих осіб.
    </li><li>Директор компіляції заснований
    </li><li>Портативний / багатоплатформнний, включаючи платформи Unix і Windows
    </li><li>Доступно в реалізаціях C / C ++ та Fortran
    </li><li>Може бути дуже простим у використанні - забезпечує "додатковий паралелізм". Можна починати з серійного коду.
    </li></ul>
<p>
</p></li><li>Інші поточні реалізації є загальними, але тут не обговорюються:
<ul>
<li>Нитки Microsoft
</li><li>Java, Python нитки
</li><li>CUDA нитки для графічних процесорів
</li></ul>

</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Більше інформації:</span>
</p><ul>
<li>POSIX Threads уроки: 
<a href="https://computing.llnl.gov/tutorials/pthreads/" target="pthreads">
computing.llnl.gov/tutorials/pthreads</a>
</li><li>OpenMP уроки: 
<a href="https://computing.llnl.gov/tutorials/openMP/" target="openMP">
computing.llnl.gov/tutorials/openMP</a>
</li></ul>

<!--========================================================================-->

<a name="ModelsMessage"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
<h2>Distributed Memory / Message Passing Model</h2>

<ul>
<p>
</p><li>This model demonstrates the following characteristics:
<img src="./Introduction to Parallel Computing_files/msg_pass_model.gif" align="right" width="446" height="310" hspace="10" vspace="10">
    <ul>
    <p>
    </p><li>A set of tasks that use their own local memory during computation.
        Multiple tasks can reside on the same physical machine and/or 
        across an arbitrary number of machines.
    <p>
    </p></li><li>Tasks exchange data through communications by sending and 
        receiving messages.
    <p>
    </p></li><li>Data transfer usually requires cooperative operations to be performed 
        by each process. For example, a send operation must have a matching 
        receive operation.
    </li></ul> 
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Implementations:</span>
</p><ul>
<li>From a programming perspective, message passing implementations usually 
    comprise a library of subroutines. Calls to these subroutines
    are imbedded in source code. The programmer is responsible for determining 
    all parallelism.
<p>
</p></li><li>Historically, a variety of message passing libraries have been 
    available since the 1980s. These implementations differed substantially      
    from each other making it difficult for programmers to develop portable
    applications. 
<p>
</p></li><li>In 1992, the MPI Forum was formed with the primary goal of establishing
    a standard interface for message passing implementations.  
<p>
</p></li><li>Part 1 of the <b>Message Passing Interface (MPI)</b> was released in
    1994. Part 2 (MPI-2) was released in 1996 and MPI-3 in 2012.
    All MPI specifications are available on the web at
    <a href="http://www.mpi-forum.org/docs/" target="_blank">http://www.mpi-forum.org/docs/</a>.
<p>
</p></li><li>MPI is the "de facto" industry 
    standard for message passing, replacing virtually all other 
    message passing implementations used for production work.
    MPI implementations exist for virtually all popular parallel computing 
    platforms. Not all implementations include everything in MPI-1, MPI-2 
    or MPI-3.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">More Information:</span>
</p><ul>
<li>MPI tutorial: 
<a href="https://computing.llnl.gov/tutorials/mpi/" target="mpi">
computing.llnl.gov/tutorials/mpi</a>
</li></ul>

<!--========================================================================-->

<a name="ModelsData"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
<h2>Data Parallel Model</h2>

<ul>
<p>
</p><li>May also be referred to as the <b>Partitioned Global Address Space (PGAS)</b>
    model.
<p>
</p></li><li>The data parallel model demonstrates the following characteristics: 

<img src="./Introduction to Parallel Computing_files/data_parallel_model.gif" align="right" width="409" height="362" hspace="10" vspace="10">
    <ul>
    <p>
    </p><li>Address space is treated globally
    <p>
    </p></li><li>Most of the parallel work focuses on performing operations on a
        data set.  The data set is typically organized into a common
        structure, such as an array or cube.
    <p>
    </p></li><li>A set of tasks work collectively on the same data structure, however,
        each task works on a different partition of the same data structure.
    <p>
    </p></li><li>Tasks perform the same operation on their partition of work, for
        example, "add 4 to every array element".
    </li></ul>
<p>
</p></li><li>On shared memory architectures, all tasks may have access to the data
    structure through global memory.  
<p>
</p></li><li>On distributed memory architectures, the global data structure can be 
    split up logically and/or physically  across tasks.
</li></ul>
<br clear="">
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Implementations:</span>
</p><ul>
<p>
</p><li>Currently, there are several relatively popular, and sometimes developmental,
    parallel programming implementations based on the Data Parallel / PGAS model.
<p>
</p></li><li><b>Coarray Fortran:</b> a small set of extensions to Fortran 95 for
    SPMD parallel programming. Compiler dependent. More information: 
    <a href="https://en.wikipedia.org/wiki/Coarray_Fortran" target="_blank">https://en.wikipedia.org/wiki/Coarray_Fortran</a>
<p>
</p></li><li><b>Unified Parallel C (UPC):</b> an extension to the C programming language
    for SPMD parallel programming. Compiler dependent. More information: 
    <a href="http://upc.lbl.gov/" target="_blank">http://upc.lbl.gov/</a>
<p>
</p></li><li><b>Global Arrays:</b> provides a shared memory style programming environment in
    the context of distributed array data structures. Public domain library with
    C and Fortran77 bindings. More information: 
    <a href="https://en.wikipedia.org/wiki/Global_Arrays" target="_blank">
    https://en.wikipedia.org/wiki/Global_Arrays</a>
<p>
</p></li><li><b>X10:</b> a PGAS based parallel programming language being developed by 
    IBM at the Thomas J. Watson Research Center. More information: 
    <a href="http://x10-lang.org/" target="_blank">http://x10-lang.org/</a>
<p>
</p></li><li><b>Chapel:</b> an open source parallel programming language project
    being led by Cray. More information: 
    <a href="http://chapel.cray.com/" target="_blank">http://chapel.cray.com/</a>
</li></ul>

<!--========================================================================-->

<a name="Hybrid"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
<h2>Hybrid Model</h2>

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td>
<ul>
<li>A hybrid model combines more than one of the previously described
    programming models.
<p>
</p></li><li>Currently, a common example of a hybrid model is the combination
    of the message passing model (MPI) with the threads model (OpenMP).
    <ul>
    <li>Threads perform computationally intensive kernels using local,
        on-node data
    </li><li>Communications between processes on different nodes occurs
        over the network using MPI
    </li></ul> 
<p>
</p></li><li>This hybrid model lends itself well to the most popular (currently)
    hardware environment of clustered multi/many-core machines. 
<p>
</p></li><li>Another similar and increasingly popular example of a hybrid model is 
    using MPI with CPU-GPU (Graphics Processing Unit) programming.
    <ul>
    <li>MPI tasks run on CPUs using local memory and communicating with
        each other over a network.
    </li><li>Computationally intensive kernels are off-loaded to GPUs on-node.
    </li><li>Data exchange between node-local memory and GPUs uses CUDA (or something
        equivalent).
    </li></ul>
<p>
</p></li><li>Other hybrid models are common:
    <ul>
    <li>MPI with Pthreads
    </li><li>MPI with non-GPU accelerators
    </li><li>...
    </li></ul>
</li></ul>
</td>
<td align="right">
<img src="./Introduction to Parallel Computing_files/hybrid_model.gif" width="485" height="241" hspace="20">
<br>
<img src="./Introduction to Parallel Computing_files/hybrid_model2.gif" width="485" height="209" hspace="20" vspace="20">
</td></tr></tbody></table>


<!--========================================================================-->

<a name="SPMD-MPMD"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Programming Models</span></td>
</tr></tbody></table>
<h2>SPMD and MPMD</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Single Program Multiple Data (SPMD):</span>
<ul>
<p>
</p><li>SPMD is actually a "high level" programming model that can be
    built upon any combination of the previously mentioned parallel   
programming models.
<img src="./Introduction to Parallel Computing_files/spmd_model.gif" align="right" width="395" height="110" hspace="10" vspace="10">

<p>
</p></li><li>SINGLE PROGRAM: All tasks execute their copy of the same program
    simultaneously. This program can be threads, message passing,
    data parallel or hybrid.
<p>
</p></li><li>MULTIPLE DATA: All tasks may use different data
<p>
</p></li><li>SPMD programs usually have the necessary logic programmed into them to 
    allow different tasks to branch or conditionally execute only those
    parts of the program they are designed to execute. That is, tasks 
    do not necessarily have to execute the entire program - perhaps only a 
    portion of it.
<p>
</p></li><li>The SPMD model, using message passing or hybrid programming,
    is probably the most commonly used parallel programming model
    for multi-node clusters.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Multiple Program Multiple Data (MPMD):</span>
</p><ul>
<p>
</p><li>Like SPMD, MPMD is actually a "high level" programming model that can 
    be built upon any combination of the previously mentioned parallel   
    programming models.

<img src="./Introduction to Parallel Computing_files/mpmd_model.gif" align="right" width="395" height="110" hspace="10" vspace="10">

<p>
</p></li><li>MULTIPLE PROGRAM: Tasks may execute different programs
    simultaneously. The programs can be threads, message passing,
    data parallel or hybrid.
<p>
</p></li><li>MULTIPLE DATA: All tasks may use different data
<p>
</p></li><li>MPMD applications are not as common as SPMD applications, but may
    be better suited for certain types of problems, particularly those
    that lend themselves better to functional decomposition 
    than domain decomposition (discussed later under <a href="https://computing.llnl.gov/tutorials/parallel_comp/#DesignPartitioning">
    Partioning</a>).
</li></ul>

<!--========================================================================-->
<p>
<a name="Designing"> <br><br> </a>
<a name="DesignAutomatic"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Automatic vs. Manual Parallelization</h2>

<ul>
<p>
</p><li>Designing and developing parallel programs has characteristically been a
    very manual process.  The programmer is typically responsible for 
    both identifying and actually implementing parallelism. 
<p>
</p></li><li>Very often, manually developing parallel codes is a time consuming,
    complex, error-prone and <i><b>iterative</b></i> process.
<p>
</p></li><li>For a number of years now, various tools have been available to assist
    the programmer with converting serial programs into parallel programs.
    The most common type of tool used to automatically parallelize a serial
    program is a parallelizing compiler or pre-processor.
<p>
</p></li><li>A parallelizing compiler generally works in two different ways:
<p>
<b>Fully Automatic</b>
</p><ul>
<li>The compiler analyzes the source code and identifies opportunities for 
    parallelism.  
</li><li>The analysis includes identifying inhibitors to parallelism and possibly a cost 
    weighting on whether or not the parallelism would actually
    improve performance.
</li><li>Loops (do, for) are the most frequent target for
    automatic parallelization.
</li></ul>
<p>
<b>Programmer Directed</b>
</p><ul>
<li>Using "compiler directives" or possibly compiler flags,
    the programmer explicitly tells the compiler how to
    parallelize the code.
</li><li>May be able to be used in conjunction with some degree of 
    automatic parallelization also.
</li></ul>
<p>
</p></li><li>The most common compiler generated parallelization is done using
    on-node shared memory and threads (such as OpenMP).
<p>
</p></li><li>If you are beginning with an existing serial code and have time
    or budget constraints, then automatic parallelization may be 
    the answer.  However, there are several important caveats that
    apply to automatic parallelization:
    <ul>
    <li>Wrong results may be produced
    </li><li>Performance may actually degrade
    </li><li>Much less flexible than manual parallelization
    </li><li>Limited to a subset (mostly loops) of code
    </li><li>May actually not parallelize code if the compiler analysis suggests there
        are inhibitors or the code is too complex
    </li></ul><p>
</p></li><li>The remainder of this section applies to the manual method of 
    developing parallel codes.
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignUnderstand"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Understand the Problem and the Program</h2>

<ul>
<p>
</p><li>Undoubtedly, the first step in developing parallel software is to
    first understand the problem that you wish to solve in parallel.
    If you are starting with a serial program, this necessitates 
    understanding the existing code also. 
<p>
</p></li><li>Before spending time in an attempt to develop a parallel solution
    for a problem, determine whether or not the problem is one that can
    actually be parallelized.  
<ul>
<p>
</p><li>Example of an easy to parallelize problem: 
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr><td><b>
    Calculate the potential energy for each of several thousand 
    independent conformations of a molecule.
    When done, find the minimum energy conformation.
</b></td></tr></tbody></table>
</p><p>
    This problem is able to be solved in parallel. Each of the 
    molecular conformations is independently determinable.
    The calculation of the minimum energy conformation is also a
    parallelizable problem. 
</p><p>
</p></li><li>Example of a problem with little-to-no parallelism: 
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr><td><b>
    Calculation of the Fibonacci series (0,1,1,2,3,5,8,13,21,...) by use of 
    the formula: 
</b><p><b>F(n) = F(n-1) + F(n-2)
</b><br>
</p></td></tr></tbody></table>
</p><p> The calculation of the F(n) value uses those of both F(n-1) and F(n-2), which 
    must be computed first.  
</p></li></ul> 
</li></ul>
<p>

<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td>
<ul>
<li>Identify the program's 
    <font style="background-color: yellow"><i><b>hotspots</b></i></font>:
    <ul>
    <li>Know where most of the real work is being done.  
        The majority of scientific and technical programs usually 
        accomplish most of their work in a few places. 
    </li><li>Profilers and performance analysis tools can help here
    </li><li>Focus on parallelizing the hotspots and ignore those sections
        of the program that account for little CPU usage. 
    </li></ul>
<p>
</p></li><li>Identify <font style="background-color: yellow"><i><b>bottlenecks</b></i></font>
    in the program:
    <ul>
    <li>Are there areas that are disproportionately slow, or cause 
        parallelizable work to halt or be deferred?
        For example, I/O is usually something that slows a program down.
    </li><li>May be possible to restructure the program or use a different
        algorithm to reduce or eliminate unnecessary slow areas
    </li></ul>
<p>
</p></li><li>Identify inhibitors to parallelism.  One common class of inhibitor
    is <i>data dependence</i>, as demonstrated by the Fibonacci sequence
    above.  
<p>
</p></li><li>Investigate other algorithms if possible.  This may be the single most
    important consideration when designing a parallel application.
<p>
</p></li><li>Take advantage of optimized third party parallel software and highly
    optimized math libraries available from leading vendors (IBM's ESSL, 
    Intel's MKL, AMD's AMCL, etc.).
</li></ul></td>
<td><img src="./Introduction to Parallel Computing_files/hotspotBottleneck2.jpg" hspace="20"></td>
</tr></tbody></table>


<!--========================================================================-->

<a name="DesignPartitioning"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Partitioning</h2>

<ul>
<p>
</p><li>One of the first steps in designing a parallel program is to break the
    problem into discrete "chunks" of work that can be distributed to
    multiple tasks.  This is known as decomposition or partitioning.
<p>
</p></li><li>There are two basic ways to partition computational work among parallel 
    tasks: <b><i>domain decomposition</i></b> and 
    <b><i>functional decomposition</i></b>. 
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Domain Decomposition:</span>
    </p><ul>
    <p>
    </p><li>In this type of partitioning, the data associated with a problem
        is decomposed.  Each parallel task then works on a portion of
        the data.
    <p><img src="./Introduction to Parallel Computing_files/domain_decomp.gif" width="388" height="216">
    </p><p>
<a name="distributions"> </a>
    </p></li><li>There are different ways to partition data:
    <p><img src="./Introduction to Parallel Computing_files/distributions.gif" width="502" height="386">
    </p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Functional Decomposition:</span>
    </p><ul>
    <p>
    </p><li>In this approach, the focus is on the computation that is to be      
        performed rather than on the data manipulated by the computation.
        The problem is decomposed according to the work that must be done.
        Each task then performs a portion of the overall work.
    <p><img src="./Introduction to Parallel Computing_files/functional_decomp.gif" width="587" height="353">
    </p><p>
    </p></li><li>Functional decomposition lends itself well to problems that can be
        split into different tasks.  For example:
        <dl>
        <p>
        </p><dt><b>Ecosystem Modeling</b>
        <br>Each program calculates the population
            of a given group, where each group's growth depends on that of its
            neighbors. As time progresses, each process calculates
            its current state, then exchanges information with the neighbor  
            populations. All tasks then progress to calculate the state at the
            next time step.
        <p>
        <img src="./Introduction to Parallel Computing_files/functional_ex1.gif" width="567" height="221">
        </p><p>
        </p></dt><dt><b>Signal Processing</b>
        <br>An audio signal data set is passed
            through four distinct computational filters. Each filter is a 
            separate process. The first segment of data must pass through the
            first filter before progressing to the second. When it does, the
            second segment of data passes through the first filter. By the time
            the fourth segment of data is in the first filter, all four
            tasks are busy.
        <p>
        <img src="./Introduction to Parallel Computing_files/functional_ex2.gif" width="703" height="272">
        </p><p>
        </p></dt><dt><b>Climate Modeling</b>
        <br>Each model component can be thought of as a separate task.
            Arrows represent exchanges of data between components during
            computation: the atmosphere model generates wind velocity data 
            that are used by the ocean model, the ocean model generates sea
            surface temperature data that are used by the atmosphere model, 
            and so on.
        <p>
        <table border="0" cellpadding="0" cellspacing="0">
        <tbody><tr valign="top">
        <td><img src="./Introduction to Parallel Computing_files/functional_ex3.gif" width="372" height="257"></td>
        <td><a href="./Introduction to Parallel Computing_files/climateModelling.png" target="_blank">
            <img src="./Introduction to Parallel Computing_files/climateModelling.png" height="257" hspace="50"></a></td>
        </tr></tbody></table>
        </p></dt></dl>
<p>
</p></li><li>Combining these two types of problem decomposition is common and natural.
<p>
</p></li></ul>

<!--========================================================================-->

<a name="DesignCommunications"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
<h2>Communications</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Who Needs Communications?</span>
<ul>
<p>The need for communications between tasks depends upon your problem:
</p><p>
<table border="0" cellspacing="0" cellpadding="0" width="90%">
<tbody><tr valign="top">
<td><b>You DON'T need communications:</b>
<ul>
<li>Some types of problems can be decomposed and executed in parallel
    with virtually no need for tasks to share data. These types of problems are 
    often called <b><i>embarrassingly parallel</i></b> - little or no communications
    are required. 
</li><li>For example, imagine an
    image processing operation where every pixel in a black and white image
    needs to have its color reversed.   The image data can easily be 
    distributed to multiple tasks that then act independently of each other
    to do their portion of the work.  
</li></ul></td>
<td><b>You DO need communications:</b>
<ul>
<li>Most parallel applications are not quite so simple, and do require 
    tasks to share data with each other.  
</li><li>For example, a 2-D heat diffusion problem
    requires a task to know the temperatures calculated by the tasks that 
    have neighboring data.  Changes to neighboring data has a direct effect 
    on that task's data.
</li></ul></td>
</tr><tr valign="top">
<td align="center"><br><img src="./Introduction to Parallel Computing_files/black2white.gif"></td>
<td align="center"><br><img src="./Introduction to Parallel Computing_files/heat_partitioned.gif"></td>
</tr></tbody></table>
</p></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Factors to Consider:</span>
</p><ul>
<p>
There are a number of important factors to consider when designing your
program's inter-task communications:
</p><p>
<img src="./Introduction to Parallel Computing_files/commOverhead.jpg" align="right" hspace="20" width="250">
</p><li><b>Communication overhead</b>
    <ul>
    <li>Inter-task communication virtually always implies overhead.
    </li><li>Machine cycles and resources that could be used for computation
        are instead used to package and transmit data.
    </li><li>Communications frequently require some type of synchronization
        between tasks, which can result in tasks spending time "waiting"
        instead of doing work.
    </li><li>Competing communication traffic can saturate the available network
        bandwidth, further aggravating performance problems.
    </li></ul>
<p>
</p></li><li><b>Latency vs. Bandwidth</b>
    <ul>
    <li><b><i>latency</i></b> is the time it takes to send a minimal (0 byte)
        message from point A to point B.  Commonly expressed as microseconds.
    </li><li><b><i>bandwidth</i></b> is the amount of data that can be communicated
        per unit of time.  Commonly expressed as megabytes/sec or gigabytes/sec.
    </li><li>Sending many small messages can cause latency to dominate communication
        overheads.  Often it is more efficient to package small messages into a
        larger message, thus increasing the effective communications bandwidth.
    </li></ul>
<p>
</p></li><li><b>Visibility of communications</b>
    <ul>
    <li>With the Message Passing Model, communications are explicit and
        generally quite visible and under the control of the programmer.  
    </li><li>With the Data Parallel Model, communications often occur
        transparently to the programmer, particularly on distributed 
        memory architectures.  The programmer may not even be able to
        know exactly how inter-task communications are being accomplished.
    </li></ul>
<p>
</p></li><li><b>Synchronous vs. asynchronous communications</b>
    <ul>
    <li>Synchronous communications require some type of "handshaking"
        between tasks that are sharing data.  This can be explicitly
        structured in code by the programmer, or it may happen at a
        lower level unknown to the programmer.
    </li><li>Synchronous communications are often referred to as
        <b><i>blocking</i></b> communications since other work must
        wait until the communications have completed.
    </li><li>Asynchronous communications allow tasks to transfer data independently
        from one another. For example, task 1 can prepare and send a
        message to task 2, and then immediately begin doing other work.
        When task 2 actually receives the data doesn't matter. 
    </li><li>Asynchronous communications are often referred to as
        <b><i>non-blocking</i></b> communications since other work can
        be done while the communications are taking place.
    </li><li>Interleaving computation with communication is the single greatest
        benefit for using asynchronous communications.
    </li></ul>
<p>
</p></li><li><b>Scope of communications</b>
    <ul>
    <li>Knowing which tasks must communicate with each other is critical during
        the design stage of a parallel code. Both of the two scopings 
        described below can be implemented synchronously or asynchronously.
    </li><li><b><i>Point-to-point</i></b> - involves two tasks with one task
        acting as the sender/producer of data, and the other acting as 
        the receiver/consumer.
    </li><li><b><i>Collective</i></b> - involves data sharing between more than
        two tasks, which are often specified as being members in a common 
        group, or collective. Some common variations (there are more):
    <p>
    <img src="./Introduction to Parallel Computing_files/collective_comm.gif"> 
    </p></li></ul>
<p>
</p></li><li><b>Efficiency of communications</b>
    <ul>
    <li>Oftentimes, the programmer has choices that can affect communications
        performance.  Only a few are mentioned here.        
    </li><li>Which implementation for a given model should be used?  Using
        the Message Passing Model as an
        example, one MPI implementation may be faster on a given
        hardware platform than another.
    </li><li>What type of communication operations should be used?  As
        mentioned previously, asynchronous communication operations
        can improve overall program performance.
    </li><li>Network fabric - some platforms may offer more than one network
        for communications.  Which one is best?
    </li></ul>
<p>
</p><p>
</p></li><li><b>Overhead and Complexity</b>
<p>    
    <img src="./Introduction to Parallel Computing_files/helloWorldParallelCallgraph.gif" width="914" height="497">
</p><p>
</p></li><li>Finally, realize that this is only a partial list of things to consider!!!
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignSynchronization"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Synchronization</h2>

<img src="./Introduction to Parallel Computing_files/sychronization2.jpg" width="300" align="right" hspace="20">
<ul>
<li>Managing the sequence of work and the tasks performing it is a critical
    design consideration for most parallel programs.
<p>
</p></li><li>Can be a significant factor in program performance (or lack of it)
<p>
</p></li><li>Often requires "serialization" of segments of the program.
</li></ul>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Types of Synchronization:</span>
<ul>
<p>
</p><li><b>Barrier</b>
    <ul>
    <li>Usually implies that all tasks are involved
    </li><li>Each task performs its work until it reaches the barrier.  It then
        stops, or "blocks".        
    </li><li>When the last task reaches the barrier, all tasks are synchronized.
    </li><li>What happens from here varies.  Often, a serial section of work must
        be done.  In other cases, the tasks are automatically released to
        continue their work.  
    </li></ul>
<p>
</p></li><li><b>Lock / semaphore</b>
    <ul>
    <li>Can involve any number of tasks
    </li><li>Typically used to serialize (protect) access to global data
        or a section of code. Only one task at a time may use (own) the 
        lock / semaphore / flag.
    </li><li>The first task to acquire the lock "sets" it.  This task can then
        safely (serially) access the protected data or code.
    </li><li>Other tasks can attempt to acquire the lock but must wait until the
        task that owns the lock releases it.
    </li><li>Can be blocking or non-blocking
    </li></ul>
<p>
</p></li><li><b>Synchronous communication operations</b>
    <ul>
    <li>Involves only those tasks executing a communication operation
    </li><li>When a task performs a communication operation, some form of
        coordination is required with the other task(s) participating in
        the communication.  For example, before a task can perform a
        send operation, it must first receive an acknowledgment from the
        receiving task that it is OK to send.  
    </li><li>Discussed previously in the Communications section.
    </li></ul>
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignDependencies"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Data Dependencies</h2>

<img src="./Introduction to Parallel Computing_files/dependencies1.jpg" align="right" hspace="20">
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Definition:</span>
<ul>
<p>
</p><li>A <b><i>dependence</i></b> exists between program statements when
    the order of statement execution affects the results of the program. 
<p>
</p></li><li>A <b><i>data dependence</i></b> results from multiple use of the same
    location(s) in storage by different tasks.
<p>
</p></li><li>Dependencies are important to parallel programming because they are one
    of the primary inhibitors to parallelism.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Examples:</span>
</p><p>
</p><ul>
<li><b>Loop carried data dependence</b>
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top">
<td><pre><b>
DO J = MYSTART,MYEND
   A(J) = A(J-1) * 2.0
END DO
</b></pre></td>
</tr></tbody></table>
</p><p>
    The value of A(J-1) must be computed before the value of A(J),
    therefore A(J) exhibits a data dependency on A(J-1).
    Parallelism is inhibited.
</p><p> If Task 2 has A(J) and task 1 has A(J-1), 
    computing the correct value of A(J) necessitates: 
    </p><ul type="circle">
    <li>Distributed memory architecture - task 2 must obtain the value 
        of A(J-1) from task 1 after task 1 finishes its computation
    </li><li>Shared memory architecture - task 2 must read A(J-1) after 
        task 1 updates it 
    </li></ul>
<p>
</p></li><li><b>Loop independent data dependence</b>
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr valign="top">
<td><pre><b>
task 1        task 2
------        ------

X = 2         X = 4
  .             .
  .             .
Y = X**2      Y = X**3
</b></pre></td>
</tr></tbody></table>
</p><p>
    As with the previous example, parallelism is inhibited. 
    The value of Y is dependent on: 
    </p><ul type="circle">
    <li>Distributed memory architecture - if or when the value of X is
        communicated between the tasks. 
    </li><li>Shared memory architecture - which task last stores the value of X.
    </li></ul>
<p>
</p></li><li>Although all data dependencies are important to identify when designing
    parallel programs, loop carried dependencies are particularly important
    since loops are possibly the most common target of parallelization efforts.

</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">How to Handle Data Dependencies:</span>
</p><ul>
<p>
</p><li>Distributed memory architectures - communicate required data at  
    synchronization points.
<p>
</p></li><li>Shared memory architectures -synchronize read/write operations between
    tasks. 
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignLoadBalance"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Load Balancing</h2>

<ul>
<p>
</p><li>Load balancing refers to the practice of distributing approximately equal
    amounts of work among tasks
    so that <b><i>all</i></b> tasks are kept busy <b><i>all</i></b> of the time.  
    It can be considered a minimization of task idle time.
<p>
</p></li><li>Load balancing is important to parallel programs for performance
    reasons.  For example, if all tasks are subject to a barrier
    synchronization point, the slowest task will determine the overall
    performance.
<p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><img src="./Introduction to Parallel Computing_files/load_bal1.gif" width="403" height="188"></td>
<td><img src="./Introduction to Parallel Computing_files/loadImbalance2.jpg" height="188" hspace="50"></td>
</tr></tbody></table>
</p><p>
</p></li></ul>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">How to Achieve Load Balance:</span>
<ul>
<p>
</p><li><b>Equally partition the work each task receives</b>
    <ul>
    <li>For array/matrix operations where each task performs similar
        work, evenly distribute the data set among the tasks.
    </li><li>For loop iterations where the work done in each iteration
        is similar, evenly distribute the iterations across the tasks.
    </li><li>If a heterogeneous mix of machines with varying performance
        characteristics are being used, be sure to use some type of performance
        analysis tool to detect any load imbalances.  Adjust work accordingly.
    </li></ul><p>
</p></li><li><b>Use dynamic work assignment</b>
    <ul>
    <li>Certain classes of problems result in load imbalances even if data
        is evenly distributed among tasks:
<p>
<table border="0" cellspacing="0" cellpadding="5" width="90%">
<tbody><tr valign="top">
<td width="33%"><img src="./Introduction to Parallel Computing_files/sparseMatrix.gif" height="250"></td>
<td width="33%"><img src="./Introduction to Parallel Computing_files/adaptiveGrid.jpg" height="250"></td>
<td width="33%"><img src="./Introduction to Parallel Computing_files/n-body.jpg" height="230" vspace="10"></td>
</tr><tr valign="top">
<td>Sparse arrays - some tasks will have actual data to work on while others have mostly "zeros".</td>
<td>Adaptive grid methods - some tasks may need to refine their mesh while others don't.</td>
<td><i>N</i>-body simulations - particles may migrate across task domains requiring more work for some tasks.
</td>
</tr></tbody></table>
</p><p>
    </p></li><li>When the amount of work each task will perform is intentionally
        variable, or is unable to be predicted, it may be helpful to use
        a <b><i>scheduler-task pool</i></b> approach.  As each task finishes
        its work, it receives a new piece from the work queue.
<p>
<img src="./Introduction to Parallel Computing_files/schedulerTaskPool.gif">
</p><p>
    </p></li><li>Ultimately, it may become necessary to design an algorithm which detects and 
        handles load imbalances as they occur dynamically within the code.
    </li></ul>
</li></ul>


<!--========================================================================-->
<p>
<a name="DesignGranularity"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>Granularity</h2>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Computation / Communication Ratio:</span>
<ul>
<p>
</p><li>In parallel computing, granularity is a qualitative measure of the ratio
    of computation to communication. 
<p>
</p></li><li>Periods of computation are typically separated from periods of 
    communication by synchronization events.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Fine-grain Parallelism:</span>
</p><p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td> <ul> 
     <p>
     </p><li>Relatively small amounts of computational work are done between 
         communication events 
     <p>
     </p></li><li>Low computation to communication ratio  
     <p>
     </p></li><li>Facilitates load balancing 
     <p>
     </p></li><li>Implies high communication overhead and less opportunity for
         performance enhancement
     <p>
     </p></li><li>If granularity is too fine it is possible that the overhead
         required for communications and synchronization between tasks
         takes longer than the computation. 
     </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Coarse-grain Parallelism:</span>
     </p><ul>
     <p>
     </p><li>Relatively large amounts of computational work are done between  
         communication/synchronization events 
     <p>
     </p></li><li>High computation to communication ratio
     <p>
     </p></li><li>Implies more opportunity for performance increase
     <p>
     </p></li><li>Harder to load balance efficiently
     </li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Which is Best?</span>
</p><ul>
<p>
</p><li>The most efficient granularity is dependent on the algorithm and the 
    hardware environment in which it runs.
<p>
</p></li><li>In most cases the overhead associated with communications and 
    synchronization is high relative to execution speed
    so it is advantageous to have coarse granularity.
<p>
</p></li><li>Fine-grain parallelism can help reduce overheads due to load imbalance.
</li></ul></td>
<td><img src="./Introduction to Parallel Computing_files/granularity2.gif" align="right" hspace="20">
<img src="./Introduction to Parallel Computing_files/granularity3.gif" align="right" hspace="20" vspace="30"></td>
</tr></tbody></table>

<!--========================================================================-->

<a name="DesignIO"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
</p><h2>I/O</h2>

<img src="./Introduction to Parallel Computing_files/memoryAccessTimes.gif" width="555" height="258" align="right" hspace="25">
<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">The Bad News:</span>
<p>
</p><ul>
<li>I/O operations are generally regarded as inhibitors to parallelism. 
<p>
</p></li><li>I/O operations require orders of magnitude more time than memory operations.
<p>
</p></li><li>Parallel I/O systems may be immature or not available for all platforms.
<p>
</p></li><li>In an environment where all tasks see the same file space, write
    operations can result in file overwriting.
<p>
</p></li><li>Read operations can be affected by the file server's ability to handle
    multiple read requests at the same time.
<p>
</p></li><li>I/O that must be conducted over the network (NFS, non-local) can cause
    severe bottlenecks and even crash file servers.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">The Good News:</span>
</p><ul>
<p>
</p><li>Parallel file systems are available.  For example:
    <ul>
    <li>GPFS: General Parallel File System (IBM). Now called IBM Spectrum Scale.
    </li><li>Lustre: for Linux clusters (Intel)
    </li><li>HDFS: Hadoop Distributed File System (Apache)
    </li><li>PanFS: Panasas ActiveScale File System for Linux clusters (Panasas,
        Inc.)
    </li><li>And more - see        
<a href="http://en.wikipedia.org/wiki/List_of_file_systems#Distributed_parallel_file_systems" target="_blank">http://en.wikipedia.org/wiki/List_of_file_systems#Distributed_parallel_file_systems</a>
    </li></ul>
<p>
</p></li><li>The parallel I/O programming interface specification for MPI has been
    available since 1996 as part of MPI-2. Vendor and "free" implementations 
    are now commonly available.
<p>
</p></li><li>A few pointers:
    <ul>
    <p>
    </p><li>Rule #1: Reduce overall I/O as much as possible
    <p>
    </p></li><li>If you have access to a parallel file system, use it.
    <p>
    </p></li><li>Writing large chunks of data rather than small chunks is usually
        significantly more efficient.
    <p>
    </p></li><li>Fewer, larger files performs better than many small files.
    <p>
    </p></li><li>Confine I/O to specific serial portions of the job, and then use
        parallel communications to distribute data to parallel tasks.
        For example, Task 1 could read an input file and then communicate
        required data to other tasks.  Likewise, Task 1 could perform
        write operation after receiving required data from all other tasks.
    <p>
    </p></li><li>Aggregate I/O operations across tasks - rather than having many tasks
        perform I/O, have a subset of tasks perform it.
     </li></ul>
</li></ul>

<!--========================================================================-->

<a name="DesignDebug"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
<h2>Debugging</h2>

<ul>
<p>
</p><li>Debugging parallel codes can be incredibly difficult, particularly as codes
    scale upwards.
<p>
</p></li><li>The good news is that there are some excellent debuggers available to assist:
    <ul>
    <li>Threaded - pthreads and OpenMP
    </li><li>MPI
    </li><li>GPU / accelerator
    </li><li>Hybrid
    </li></ul>
<p>
</p></li><li>Livermore Computing users have access to several parallel debugging tools
    installed on LC's clusters:
    <ul>
    <li>TotalView from RogueWave Software
    </li><li>DDT from Allinea
    </li><li>Inspector from Intel
    </li><li>Stack Trace Analysis Tool (STAT) - locally developed
    </li></ul>
<p>
</p></li><li>All of these tools have a learning curve associated with them - some more than
    others.
<p>
</p></li><li>For details and getting started information, see:
    <ul>
    <li>LC's web pages at <a href="https://hpc.llnl.gov/software/development-environment-software" target="_blank">
https://hpc.llnl.gov/software/development-environment-software</a>
    </li><li>TotalView tutorial:  <a href="https://computing.llnl.gov/tutorials/totalview/" target="_blank">https://computing.llnl.gov/tutorials/totalview/</a>
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/debug1.gif" width="1000" height="509">
</p></li></ul>

<!--========================================================================-->

<a name="DesignPerformance"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Designing Parallel Programs</span></td>
</tr></tbody></table>
<h2>Performance Analysis and Tuning</h2>

<ul>
<p>
</p><li>As with debugging, analyzing and tuning parallel program performance
    can be much more challenging than for serial programs.
<p>
</p></li><li>Fortunately, there are a number of excellent tools for parallel program
    performance analysis and tuning.
<p>
</p></li><li>Livermore Computing users have access to several such tools, most of which
    are available on all production clusters.
<p>
</p></li><li>Some starting points for tools installed on LC systems: 
    <ul>
    <li>LC's web pages at <a href="https://hpc.llnl.gov/software/development-environment-software" target="_blank">
https://hpc.llnl.gov/software/development-environment-software</a>
    </li><li>TAU:
        <a href="http://www.cs.uoregon.edu/research/tau/docs.php" target="_blank">http://www.cs.uoregon.edu/research/tau/docs.php</a>
    </li><li>HPCToolkit:
        <a href="http://hpctoolkit.org/documentation.html" target="_blank">http://hpctoolkit.org/documentation.html</a>
    </li><li>Open|Speedshop:
        <a href="http://www.openspeedshop.org/" target="_blank">http://www.openspeedshop.org/</a>
    </li><li>Vampir / Vampirtrace:
        <a href="http://vampir.eu/" target="_blank">http://vampir.eu/</a>
    </li><li>Valgrind:
        <a href="http://valgrind.org/" target="_blank">http://valgrind.org/</a>
    </li><li>PAPI:
        <a href="http://icl.cs.utk.edu/papi/" target="_blank">http://icl.cs.utk.edu/papi/</a>
    </li><li>mpitrace
        <a href="https://computing.llnl.gov/tutorials/bgq/index.html#mpitrace" target="_blank">
        https://computing.llnl.gov/tutorials/bgq/index.html#mpitrace</a>
    </li><li>mpiP:
        <a href="http://mpip.sourceforge.net/" target="_blank">http://mpip.sourceforge.net/</a>
    </li><li>memP:
        <a href="http://memp.sourceforge.net/" target="_blank">http://memp.sourceforge.net/</a>
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/perfAnalysis.jpg" width="981" height="627">
</p></li></ul>

<!---------------------------- Removed 5/7/14 -------------------------------------
    <LI>A dated, but potentially useful LC whitepaper on the subject of "High Performance Tools and Technologies" describes a large number of tools, and a number of performance related topics applicable to code developers. Find it at:
<A HREF=../performance_tools/HighPerformanceToolsTechnologiesLC.pdf TARGET=toolspaper>computing.llnl.gov/tutorials/performance_tools/HighPerformanceToolsTechnologiesLC.pdf</A>.
    <LI><A HREF=../performance_tools TARGET=W2>Performance 
    Analysis Tools Tutorial</A>
------------------------------ Removed 5/7/14 ------------------------------------->


<!--========================================================================-->

<a name="Examples"> <br><br> </a><a name="ExamplesArray"> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Examples</span></td>
</tr></tbody></table>
<h2>Array Processing</h2>

<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td>
<ul>
<p>
</p><li>This example demonstrates calculations on 2-dimensional array elements; a 
    function is evaluated on each array element.
<p>
</p></li><li>The computation on each array element is independent from other array elements.
<p>
</p></li><li>The problem is computationally intensive.
<p>
</p></li><li>The serial program calculates one element at a time in sequential order.
<p>
</p></li><li>Serial code could be of the form:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
do j = 1,n
  do i = 1,n
    a(i,j) = fcn(i,j)
  end do
end do

</b></pre>
</td></tr></tbody></table>
</p><p>
</p></li><li>Questions to ask:
<ul>
<li>Is this problem able to be parallelized?
</li><li>How would the problem be partitioned?
</li><li>Are communications needed?
</li><li>Are there any data dependencies?
</li><li>Are there synchronization needs?
</li><li>Will load balancing be a concern?
</li></ul>
</li></ul>
</td>
<td>
<img src="./Introduction to Parallel Computing_files/array_proc1.gif" width="297" height="369" hspace="20">
</td>
</tr></tbody></table>


<p></p><hr><p>
</p><h2>Array Processing <br>Parallel Solution 1</h2>

<ul>
<p>
<img src="./Introduction to Parallel Computing_files/array_proc2.gif" width="297" height="247" hspace="20" align="right">
</p><li>The calculation of elements is independent of one another - leads to an 
    embarrassingly parallel solution.
<p>
</p></li><li>Arrays elements are evenly distributed so that each process owns a portion of 
    the array (subarray).
<ul>
<p>
</p><li>Distribution scheme is chosen for efficient memory access; e.g. unit stride 
    (stride of 1) through the subarrays.  Unit stride maximizes cache/memory usage.
<p>
</p></li><li>Since it is desirable to have unit stride through the subarrays, the
    choice of a distribution scheme depends on the programming language.
    See the <a href="https://computing.llnl.gov/tutorials/parallel_comp/#distributions">Block - Cyclic Distributions Diagram</a>
    for the options.
</li></ul>
<p>
</p></li><li>Independent calculation of array elements ensures there is no
    need for communication or synchronization between tasks.
<p>
</p></li><li>Since the amount of work is evenly distributed across processes, there should
    not be load balance concerns.
<p>
</p></li><li>After the array is distributed, each task executes the portion of the loop 
    corresponding to the data it owns. <br>For example, both Fortran (column-major)
    and C (row-major) block distributions are shown:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<td width="50%"><pre><b>
do j = <font color="red">mystart, myend</font>
  do i = 1, n
    a(i,j) = fcn(i,j)
  end do
end do

</b></pre></td>
<td width="50%"><pre><b>
for i (i = <font color="red">mystart</font>; i &lt; <font color="red">myend</font>; i++) {
  for j (j = 0; j &lt; n; j++) {
    a(i,j) = fcn(i,j);
    }
  }

</b></pre></td>
</tr></tbody></table>
</p><p>
</p></li><li>Notice that only the outer loop variables are different from the serial 
    solution.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">One Possible Solution:</span>
</p><ul>
<p>
</p><li>Implement as a Single Program Multiple Data (SPMD) model - every task executes
    the same program.
<p>
</p></li><li>Master process initializes array, sends info to worker processes and receives
    results.
<p>
</p></li><li>Worker process receives info, performs its share of computation and sends 
    results to master.
<p>
</p></li><li>Using the Fortran storage scheme, perform block distribution of the array.
<p>
</p></li><li>Pseudo code solution:
    <b><font color="red">red</font></b> highlights changes for 
    parallelism.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out if I am MASTER or WORKER
   
if I am MASTER
   
  initialize the array
  send each WORKER info on part of array it owns
  send each WORKER its portion of initial array
   
  receive from each WORKER results 
   
else if I am WORKER
  receive from MASTER info on part of array I own
  receive from MASTER my portion of initial array
</font>
  # calculate my portion of array
  do j = <font color="red">my first column,my last column </font>
    do i = 1,n
      a(i,j) = fcn(i,j)
    end do
  end do
<font color="red">
  send MASTER results 

endif
</font></b></pre>
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Example Programs:</span>
</p><ul>
<li>MPI Program in C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_array.c" onclick="popUp(&#39;../mpi/samples/C/mpi_array.c&#39;)"></b></font>
<p>
</p></li><li>MPI Program in Fortran: &nbsp;
<font size="-1"><b><input type="button" value="mpi_array.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_array.f&#39;)"></b></font>
</li></ul>

<p></p><hr><p>
</p><h2>Array Processing <br>Parallel Solution 2: Pool of Tasks</h2>

<ul>
<p>
</p><li>The previous array solution demonstrated static load balancing: 
    <ul>
    <li>Each task has a fixed amount of work to do 
    </li><li>May be significant idle time for faster or more lightly loaded 
        processors - slowest tasks determines overall performance.
    </li></ul>
<p>
</p></li><li>Static load balancing is not usually a major concern if all tasks
    are performing the same amount of work on identical machines. 
<p>
</p></li><li>If you have a load balance problem (some tasks work faster than
    others), you may benefit by using a "pool of tasks"
    scheme.
</li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Pool of Tasks Scheme:</span>
</p><p>
</p><ul>
<p>
</p><li>Two processes are employed
<p>
Master Process: 
    </p><ul type="circle">
    <li>Holds pool of tasks for worker processes to do
    </li><li>Sends worker a task when requested
    </li><li>Collects results from workers
    </li></ul>
<p>
Worker Process: repeatedly does the following
    </p><ul type="circle">
    <li>Gets task from master process
    </li><li>Performs computation 
    </li><li>Sends results to master
    </li></ul>
<p>
</p></li><li>Worker processes do not know before runtime which portion of array
    they will handle or how many tasks they will perform.
<p>
</p></li><li>Dynamic load balancing occurs at run time: the faster tasks will 
    get more work to do.     
<p>
</p></li><li>Pseudo code solution:
    <font color="red"><b>red</b></font> highlights changes for 
    parallelism.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out if I am MASTER or WORKER

if I am MASTER

  do until no more jobs
    if request send to WORKER next job
    else receive results from WORKER
  end do

else if I am WORKER

  do until no more jobs
    request job from MASTER
    receive from MASTER next job
</font>
    calculate array element: a(i,j) = fcn(i,j)
<font color="red">
    send results to MASTER
  end do

endif
</font>
</b></pre>
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Discussion:</span>
</p><ul>
<p>
</p><li>In the above pool of tasks example, each task calculated an individual
    array element as a job.  The computation to communication ratio is 
    finely granular.
<p>
</p></li><li>Finely granular solutions incur more communication overhead in order
    to reduce task idle time.
<p>
</p></li><li>A more optimal solution might be to distribute more work with each job.
    The "right" amount of work is problem dependent.
</li></ul>


<!--========================================================================-->

<a name="ExamplesPI"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Examples</span></td>
</tr></tbody></table>
<h2>PI Calculation</h2> 

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td>
<ul>
<li>The value of PI can be calculated in various ways. Consider the Monte Carlo  
    method of approximating PI: 
<ul> 
<li>Inscribe a circle with radius <b>r</b> in a square with side length of <b>2<i>r</i></b> 
</li><li>The area of the circle is <b>Πr<sup>2</sup></b> and the area of the square is <b>4r<sup>2</sup></b> 
</li><li>The ratio of the area of the circle to the area of the square is: <br><b>Πr<sup>2</sup> / 4r<sup>2</sup> = Π / 4</b> 
</li><li>If you randomly generate <b>N</b> points inside the square, approximately <br><b>N * Π / 4</b> of those points (<b>M</b>) should fall inside the circle. </li><li><b>Π</b> is then approximated as: 
<br><b>N * Π / 4 = M 
<br>Π / 4 = M / N 
<br>Π = 4 * M / N</b> 
</li><li>Note that increasing the number of points generated improves the approximation.
</li></ul>
<p>
</p></li><li>Serial pseudo code for this procedure:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
npoints = 10000
circle_count = 0

do j = 1,npoints
  generate 2 random numbers between 0 and 1
  xcoordinate = random1
  ycoordinate = random2
  if (xcoordinate, ycoordinate) inside circle
  then circle_count = circle_count + 1
end do

PI = 4.0*circle_count/npoints

</b></pre>
</td></tr></tbody></table>


</p><p>
</p></li><li>The problem is computationally intensive -  most of the time is spent executing 
    the loop
<p>
</p></li><li>Questions to ask:
<ul>
<li>Is this problem able to be parallelized?
</li><li>How would the problem be partitioned?
</li><li>Are communications needed?
</li><li>Are there any data dependencies?
</li><li>Are there synchronization needs?
</li><li>Will load balancing be a concern?
</li></ul>
</li></ul>
</td>
<td>
<img src="./Introduction to Parallel Computing_files/pi1.gif" width="400" height="517" hspace="20">
</td></tr></tbody></table>


<p></p><hr><p>
</p><h2>PI Calculation<br>Parallel Solution</h2>

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td>
<ul>
<p>
</p><li>Another problem that's easy to parallelize:
<ul>
<li>All point calculations are independent; no data dependencies
</li><li>Work can be evenly divided; no load balance concerns
</li><li>No need for communication or synchronization between tasks
</li></ul>
<p>
</p></li><li>Parallel strategy: 
<ul>
<li>Divide the loop into equal portions that can be executed by the pool of tasks
</li><li>Each task independently performs its work
</li><li>A SPMD model is used
</li><li>One task acts as the master to collect results and compute the value of PI
</li></ul>
<p>
</p></li><li>Pseudo code solution:
    <font color="red"><b>red</b></font> highlights changes for 
    parallelism.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
npoints = 10000
circle_count = 0
<font color="red">
p = number of tasks
num = npoints/p

find out if I am MASTER or WORKER </font>

do j = 1,<font color="red">num </font>
  generate 2 random numbers between 0 and 1
  xcoordinate = random1
  ycoordinate = random2
  if (xcoordinate, ycoordinate) inside circle
  then circle_count = circle_count + 1
end do
<font color="red">
if I am MASTER

  receive from WORKERS their circle_counts
  compute PI (use MASTER and WORKER calculations)

else if I am WORKER

  send to MASTER circle_count

endif
</font>
</b></pre>
</td></tr></tbody></table>
</p></li></ul>
</td>
<td>
<img src="./Introduction to Parallel Computing_files/pi2.gif" width="400" height="475" hspace="20">
</td></tr></tbody></table>

<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Example Programs:</span>
</p><ul>
<li>MPI Program in C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_pi_reduce.c" onclick="popUp(&#39;../mpi/samples/C/mpi_pi_reduce.c&#39;)"></b></font>
<p>
</p></li><li>MPI Program in Fortran: &nbsp;
<font size="-1"><b><input type="button" value="mpi_pi_reduce.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_pi_reduce.f&#39;)"></b></font>
</li></ul>

<!--========================================================================-->

<a name="ExamplesHeat"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Examples</span></td>
</tr></tbody></table>
<h2>Simple Heat Equation</h2>

<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
<p>
</p><li>Most problems in parallel computing require communication among the tasks.
    A number of common problems require communication with "neighbor" tasks.
<p>
</p></li><li>The 2-D heat equation describes the temperature change over time,
    given initial temperature distribution and boundary conditions.
<p>
</p></li><li>A finite differencing scheme is employed to solve the heat equation numerically 
    on a square region.
<ul>
<li>The elements of a 2-dimensional array represent the temperature at
    points on the square.
</li><li>The initial temperature is zero on the boundaries and high in the middle.
</li><li>The boundary temperature is held at zero.
</li><li>A time stepping algorithm is used.
</li></ul>
<p>
</p></li><li>The calculation of an element is <b><i>dependent</i></b> upon neighbor element
    values:
<p>
<img src="./Introduction to Parallel Computing_files/heat_equation2.gif" width="276" height="114">
</p><p>
</p></li><li>A serial program would contain code like:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>
do iy = 2, ny - 1
  do ix = 2, nx - 1
    u2(ix, iy) =  u1(ix, iy)  +
        cx * (u1(ix+1,iy) + u1(ix-1,iy) - 2.*u1(ix,iy)) +
        cy * (u1(ix,iy+1) + u1(ix,iy-1) - 2.*u1(ix,iy))
  end do
end do
</b></pre>
</td></tr></tbody></table>
</p><p>
</p></li><li>Questions to ask:
<ul>
<li>Is this problem able to be parallelized?
</li><li>How would the problem be partitioned?
</li><li>Are communications needed?
</li><li>Are there any data dependencies?
</li><li>Are there synchronization needs?
</li><li>Will load balancing be a concern?
</li></ul>
</li></ul></td>

<td><img src="./Introduction to Parallel Computing_files/heat_initial.gif" width="300" height="301" hspace="20">

<img src="./Introduction to Parallel Computing_files/heat_equation.gif" width="261" height="258" border="0" hspace="20" vspace="20" alt="Heat equation">
</td></tr></tbody></table>

<p></p><hr><p>
</p><h2>Simple Heat Equation<br>Parallel Solution</h2>
<p>

<img src="./Introduction to Parallel Computing_files/heat_partitioned.gif" width="300" height="301" align="right" hspace="20">
</p><ul>
<p>
</p><li>This problem is more challenging, since there data dependencies, which require
    communications and synchronization.
<p>
</p></li><li>The entire array is partitioned and distributed as subarrays to all
    tasks. Each task owns an equal portion of the total array.
<p>
</p></li><li>Because the amount of work is equal, load balancing should not be a concern
<p>
</p></li><li>Determine data dependencies:
    <ul>
    <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/images/heat_interior.gif" target="W6">interior 
        elements</a> belonging to a task are independent of other tasks
    </li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/images/heat_edge.gif" target="W7">border
         elements</a> are dependent upon
         a neighbor task's data, necessitating communication.
    </li></ul>
<p>
</p></li><li>Implement as an SPMD model:
<ul>
<li>Master process sends initial info to workers, and then waits
    to collect results from all workers
</li><li>Worker processes calculate solution within specified number of time steps,
    communicating as necessary with neighbor processes
</li></ul>
<p>
</p></li><li>Pseudo code solution:
    <font color="red"><b>red</b></font> highlights changes for parallelism.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out if I am MASTER or WORKER

if I am MASTER
  initialize array
  send each WORKER starting info and subarray
  receive results from each WORKER

else if I am WORKER
  receive from MASTER starting info and subarray
</font>
  # Perform time steps
  do t = 1, nsteps
    update time <font color="red">
    send neighbors my border info
    receive from neighbors their border info </font>
    update my portion of solution array
    
  end do
  <font color="red">
  send MASTER results
      
endif
</font>
</b></pre>
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Example Programs:</span>
</p><ul>
<li>MPI Program in C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_heat2D.c" onclick="popUp(&#39;../mpi/samples/C/mpi_heat2D.c&#39;)"></b></font>
<p>
</p></li><li>MPI Program in Fortran: &nbsp;
<font size="-1"><b><input type="button" value="mpi_heat2D.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_heat2D.f&#39;)"></b></font>
</li></ul>

<!-----------------------------------------------------------------------

<P><HR><P> 
<H2>Simple Heat Equation<BR>
Parallel Solution 2: Overlapping Communication and Computation</H2>

<UL>
<P>
<LI>In the previous solution, it was assumed that blocking communications
    were used by the worker tasks.  Blocking communications wait for the 
    communication process to complete before continuing to the next 
    program instruction.
<P>
<LI>In the previous solution, neighbor tasks communicated border
    data, then each process updated its portion of the array.
<P>
<LI>Computing times can often be reduced by using non-blocking
    communication.  Non-blocking communications allow work to be performed 
    while communication is in progress.
<P>
<LI>Each task could update the interior of its part of the solution
    array while the communication of border data is occurring, and
    update its border after communication has completed.
<P>
<LI>Pseudo code for the second solution:
    <FONT COLOR=red><B>red</B></FONT COLOR> highlights changes for 
    non-blocking communications.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0>
<TR><TD><PRE><B>
find out if I am MASTER or WORKER
 
if I am MASTER
  initialize array
  send each WORKER starting info and subarray
    
  do until all WORKERS converge
    gather from all WORKERS convergence data
    broadcast to all WORKERS convergence signal
  end do
 
  receive results from each WORKER
 
else if I am WORKER
  receive from MASTER starting info and subarray
 
  do until solution converged
    update time
    <FONT COLOR=red>
    non-blocking send neighbors my border info
    non-blocking receive neighbors border info

    update interior of my portion of solution array
    wait for non-blocking communication complete
    update border of my portion of solution array
    </FONT>
    determine if my solution has converged
      send MASTER convergence data
      receive from MASTER convergence signal
  end do
  
  send MASTER results
       
endif

</B></PRE>
</TD></TR></TABLE>
</UL>
------------------------------------------------------------------------>

<!--========================================================================-->

<a name="ExamplesWave"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Parallel Examples</span></td>
</tr></tbody></table>
<h2>1-D Wave Equation</h2>

<ul>
<p>
</p><li>In this example, the amplitude along a uniform, vibrating string is 
    calculated after a specified amount of time has elapsed.
<p>
</p></li><li>The calculation involves:
    <ul>
    <li>the amplitude on the y axis
    </li><li>i as the position index along the x axis
    </li><li>node points imposed along the string
    </li><li>update of the amplitude at discrete time steps. 
    </li></ul>
<p>
<img src="./Introduction to Parallel Computing_files/wave3.gif">
</p><p>
</p></li><li>The equation to be solved is the one-dimensional wave equation: 
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<td><pre><span style="white-space: nowrap"><b>
    A(i,t+1) = (2.0 * A(i,t)) - A(i,t-1) 
        + (c * (A(i-1,t) - (2.0 * A(i,t)) + A(i+1,t))) 
</b></span></pre></td>
</tr></tbody></table>
</p><p>
where c is a constant
</p><p>
</p></li><li>Note that amplitude will depend on previous timesteps (t, t-1) and  
    neighboring points (i-1, i+1).  
<p>
</p></li><li>Questions to ask:
<ul>
<li>Is this problem able to be parallelized?
</li><li>How would the problem be partitioned?
</li><li>Are communications needed?
</li><li>Are there any data dependencies?
</li><li>Are there synchronization needs?
</li><li>Will load balancing be a concern?
</li></ul>
</li></ul>

<p></p><hr><p> 
</p><h2>1-D Wave Equation<br>Parallel Solution</h2>

<ul>
<p>
</p><li>This is another example of a problem involving data dependencies. A parallel 
    solution will involve communications and synchronization.
<p>
</p></li><li>The entire amplitude array is partitioned and distributed as subarrays to all 
    tasks. Each task owns an equal portion of the total array.
<p>
</p></li><li>Load balancing: all points require equal work, so the points should 
    be divided equally 
<p>
</p></li><li>A block decomposition would have the work partitioned into the number 
    of tasks as chunks, allowing each task to own mostly contiguous data points.
<p>
</p></li><li>Communication need only occur on data borders.  The larger the block size
    the less the communication. 
<p>
<img src="./Introduction to Parallel Computing_files/wave4.gif">

</p><p>
</p></li><li>Implement as an SPMD model:
<ul>
<li>Master process sends initial info to workers, and then waits
    to collect results from all workers
</li><li>Worker processes calculate solution within specified number of time steps,
    communicating as necessary with neighbor processes
</li></ul>
<p>
</p></li><li>Pseudo code solution:
    <font color="red"><b>red</b></font> highlights changes for parallelism.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b><font color="red">
find out number of tasks and task identities

#Identify left and right neighbors
left_neighbor = mytaskid - 1
right_neighbor = mytaskid +1
if mytaskid = first then left_neigbor = last
if mytaskid = last then right_neighbor = first

find out if I am MASTER or WORKER
if I am MASTER
  initialize array
  send each WORKER starting info and subarray
else if I am WORKER`
  receive starting info and subarray from MASTER
endif
</font>
#Perform time steps <font color="red">
#In this example the master participates in calculations</font>
do t = 1, nsteps <font color="red">
  send left endpoint to left neighbor
  receive left endpoint from right neighbor
  send right endpoint to right neighbor
  receive right endpoint from left neighbor
</font>
  #Update points along line
  do i = 1, npoints
    newval(i) = (2.0 * values(i)) - oldval(i) 
    + (sqtau * (values(i-1) - (2.0 * values(i)) + values(i+1))) 
  end do

end do
<font color="red">
#Collect results and write to file
if I am MASTER
  receive results from each WORKER
  write results to file
else if I am WORKER
  send results to MASTER
endif </font>

</b></pre></td></tr></tbody></table>
</p><p>
</p></li></ul>
<p>

<img src="./Introduction to Parallel Computing_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Example Programs:</span>
</p><ul>
<li>MPI Program in C: &nbsp;
<font size="-1"><b><input type="button" value="mpi_wave.c" onclick="popUp(&#39;../mpi/samples/C/mpi_wave.c&#39;)"></b></font>
<p>
</p></li><li>MPI Program in Fortran: &nbsp;
<font size="-1"><b><input type="button" value="mpi_wave.f" onclick="popUp(&#39;../mpi/samples/Fortran/mpi_wave.f&#39;)"></b></font>
</li></ul>


<br><br>
<p></p><hr><p>

<b>This completes the tutorial.</b>
</p><p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><a href="https://computing.llnl.gov/tutorials/evaluation/index.html" target="evalForm">
    <img src="./Introduction to Parallel Computing_files/evaluationForm.gif"></a> &nbsp; &nbsp; &nbsp;</td>
<td>Please complete the online evaluation form. </td>
</tr>
</tbody></table>
</p><p>
<b>Where would you like to go now?</b>
</p><ul>
<li><a href="https://computing.llnl.gov/tutorials/agenda/index.html">Agenda</a>
</li><li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#top">Back to the top</a>
</li></ul>

<!--========================================================================-->

<a name="References"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">References and More Information</span></td>
</tr></tbody></table>

<ul>
<li>Author: <a href="mailto:blaiseb@llnl.gov">Blaise Barney</a>, Livermore
    Computing.
<p>
</p></li><li>A search on the WWW for "parallel programming" or "parallel computing"
    will yield a wide variety of information.
<p>
</p></li><li>Recommended reading:
    <ul>
    <li>"Designing and Building Parallel Programs".  Ian Foster.
    <br><a href="http://www.mcs.anl.gov/~itf/dbpp/" target="_blank">
        http://www.mcs.anl.gov/~itf/dbpp/</a>
    </li><li>"Introduction to Parallel Computing". Ananth Grama, Anshul Gupta,
        George Karypis, Vipin Kumar.
    <br><a href="http://www-users.cs.umn.edu/~karypis/parbook/" target="_blank">
        http://www-users.cs.umn.edu/~karypis/parbook/</a>
    </li><li>"Overview of Recent Supercomputers". A.J. van der Steen, Jack Dongarra.
    <br><a href="https://computing.llnl.gov/tutorials/parallel_comp/OverviewRecentSupercomputers.2008.pdf" target="_blank">
        OverviewRecentSupercomputers.2008.pdf</a>
    </li></ul>
<p>
</p></li><li>Photos/Graphics have been
    created by the author, created by other LLNL employees, 
    obtained from non-copyrighted, government or public domain (such as
    http://commons.wikimedia.org/) sources,
    or used with the permission of authors from other presentations and
    web pages.
<p>
</p></li><li>History: These materials have evolved from the following
    sources, some of which are no longer maintained or available:
    <ul>
    <li>Tutorials developed for the Maui High Performance Computing 
        Center's "SP Parallel Programming Workshop".
    </li><li>Tutorials developed by the Cornell University Center for Advanced 
        Computing (CAC), now available as Cornell Virtual Workshops at:
        <a href="https://cvw.cac.cornell.edu/topics" target="_blank">https://cvw.cac.cornell.edu/topics</a>.
    </li></ul>
</li></ul>

<!-------------------------------------------------------------------------->

<script language="JavaScript">PrintFooter("UCRL-MI-133316")</script><p></p><hr><span class="footer">https://computing.llnl.gov/tutorials/parallel_comp/<br>Last Modified: 05/05/2018 01:54:23 <a href="mailto:blaiseb@llnl.gov">blaiseb@llnl.gov</a><br>UCRL-MI-133316<p>This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>




</p></span></font><div class="dd-sttbtn dd-sttbtn--visible" data-stt-pos="br"></div></body></html>